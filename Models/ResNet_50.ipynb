{"cells":[{"cell_type":"markdown","metadata":{"id":"W2cYkLHWItWw"},"source":["# Importing Dependecies\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4389,"status":"ok","timestamp":1683882883467,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"9rrjdaImGf4M"},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D,GlobalAveragePooling2D\n","from keras import regularizers, optimizers\n","import numpy as np\n","from keras.layers import Add\n","from keras.layers import Input\n","from keras.models import Model\n","from keras.layers import Flatten\n","from keras.utils import plot_model\n","from keras.preprocessing import image\n","import os\n","import re\n","from sklearn.model_selection import train_test_split\n","from keras.applications.imagenet_utils import preprocess_input\n","from sklearn.utils import shuffle\n","from keras import backend as K\n","from keras.callbacks import ReduceLROnPlateau, CSVLogger,EarlyStopping,ModelCheckpoint\n","import keras\n","from keras.preprocessing.image import ImageDataGenerator\n","import keras.utils as image"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20575,"status":"ok","timestamp":1683882788795,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"yYxfkOzEIsN0","outputId":"ebadbbfb-8b08-4b20-8b4e-862176f6c042"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"PTlPzPa6P0-t"},"source":["# Define Paths"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":359,"status":"ok","timestamp":1683882888760,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"3YPGLHfkGmtc","outputId":"18da6466-cc77-4b47-cce0-55e90be1423b"},"outputs":[{"output_type":"stream","name":"stdout","text":["PWD /content/drive/MyDrive/Malaria_Cell_Dataset\n"]}],"source":["PATH = \"/content/drive/MyDrive/Malaria_Cell_Dataset\"\n","METRICS_PATH=\"/content/drive/MyDrive/Metrics/Malaria_Cell_Metrics/\"\n","MODEL_NAME=\"ResNet\"\n","print(\"PWD\", PATH)"]},{"cell_type":"markdown","metadata":{"id":"vj6JkdnQKH-6"},"source":["# Define Metrics"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1476,"status":"ok","timestamp":1683882893332,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"ZskICu_8Gopr"},"outputs":[],"source":["def sensitivity(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    return true_positives / (possible_positives + K.epsilon())"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683882893332,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"3NUnI6O4Goni"},"outputs":[],"source":["def specificity(y_true, y_pred):\n","    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n","    false_positives = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n","    return true_negatives / (true_negatives+false_positives + K.epsilon())"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683882893333,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"lwzrX00JGolR"},"outputs":[],"source":["def f1(y_true, y_pred):\n","    def recall(y_true, y_pred):\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","        recall = true_positives / (possible_positives + K.epsilon())\n","        return recall\n","    def precision(y_true, y_pred):\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + K.epsilon())\n","        return precision\n","    precision = precision(y_true, y_pred)\n","    recall = recall(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))"]},{"cell_type":"markdown","metadata":{"id":"SX_hZ-ODNAHz"},"source":["# Read Images from Drive"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1683882895862,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"hgGgQB1BG4Od"},"outputs":[],"source":["def sorted_alphanumeric(data):\n","    convert = lambda text: int(text) if text.isdigit() else text.lower()\n","    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n","    return sorted(data, key=alphanum_key)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683882897959,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"lDGRsXoiN9Wf","outputId":"19847f18-baf1-43ae-9867-2765aba48825"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Parasitized', 'Uninfected']\n"]}],"source":["data_path = PATH\n","data_dir_list = sorted_alphanumeric(os.listdir(data_path))\n","print(data_dir_list)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":414053,"status":"ok","timestamp":1683883314206,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"cQMNxx5yG4HY","outputId":"d0e51009-547d-479b-f111-d373f726f3c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded the images of dataset-Parasitized\n","\n","Loaded the images of dataset-Uninfected\n","\n"]}],"source":["img_data_list = []\n","\n","for dataset in data_dir_list:\n","    img_list = sorted_alphanumeric(os.listdir(data_path + '/' + dataset))\n","    print('Loaded the images of dataset-' + '{}\\n'.format(dataset))\n","    for img in img_list:\n","        # print(img)\n","        img_path = data_path + '/' + dataset + '/' + img\n","        img = image.load_img(img_path, target_size=(32,32))\n","        x = image.img_to_array(img)\n","        x = np.expand_dims(x, axis=0)\n","        x = preprocess_input(x)\n","        #     x = x/255\n","        # print('Input image shape:', x.shape)\n","        img_data_list.append(x)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1683883314208,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"5DnkHKm5G4FS","outputId":"8de1e122-433a-438b-d2d0-e9066d3df1f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["(27558, 1, 32, 32, 3)\n","(1, 27558, 32, 32, 3)\n","(27558, 32, 32, 3)\n"]}],"source":["img_data = np.array(img_data_list)\n","# img_data = img_data.astype('float32')\n","print(img_data.shape)\n","img_data = np.rollaxis(img_data, 1, 0)\n","print(img_data.shape)\n","img_data = img_data[0]\n","print(img_data.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"ny8pdS-NNEhL"},"source":["# Train & Test Data"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1683883314208,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"xEymJW76JQTv","outputId":"41637a81-d203-49f0-b165-1b4eac0eb507"},"outputs":[{"output_type":"stream","name":"stdout","text":["sample 27558\n"]}],"source":["num_classes = 2\n","num_of_samples = img_data.shape[0]\n","print(\"sample\", num_of_samples)\n","labels = np.ones((num_of_samples,), dtype='int64')\n","labels[0:13780] = 1\n","labels[13780:] = 0\n","names = ['Parasitized', 'Uninfected']"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1683883314209,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"T2i6ejV-HEl4"},"outputs":[],"source":["Y = np_utils.to_categorical(labels, num_classes)\n","x, y = shuffle(img_data, Y, random_state=2)\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n"]},{"cell_type":"markdown","metadata":{"id":"yetl9cqpNLCp"},"source":["# Model Architecture"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"g7BpPGd-HXzC","executionInfo":{"status":"ok","timestamp":1683883314209,"user_tz":-330,"elapsed":8,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"}}},"outputs":[],"source":["def initial_conv(Input, filters, stride=1, kernel_size=7):\n","    x = Conv2D(filters, kernel_size=(kernel_size, kernel_size), strides=(stride, stride), padding=\"same\")(Input)\n","\n","    x = BatchNormalization()(x)\n","\n","    x = Activation('relu')(x)\n","    return x\n","\n","\n","def expand_conv_basic_block(Input, filters, stride=1, dropout=0.0):\n","    Init = Input\n","\n","    # First conv which is used to downsample the image\n","    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(Input)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # Optional Dropout layer\n","    if (dropout > 0.0):\n","        x = Dropout(dropout)(x)\n","\n","    x = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","\n","    # Projection shortcut to make skip connection(Paper terminology)\n","    skip_conv = Conv2D(filters, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n","    skip = BatchNormalization()(skip_conv)\n","\n","    # Skip connection\n","    x = Add()([x, skip])\n","    return x\n","\n","\n","def normal_conv_basic_block(Input, filters, stride=1, dropout=0.0):\n","    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(Input)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # Optional Dropout layer\n","    if (dropout > 0.0):\n","        x = Dropout(dropout)(x)\n","\n","    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","\n","    # Identity skip connection\n","    x = Add()([x, Input])\n","\n","    return x\n","\n","\n","def expand_conv_bottleneck_block(Input, filters, stride=1, dropout=0.0):\n","    # Contracting 1*1 conv\n","    x = Conv2D(filters, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # if(dropout > 0.0):\n","    #   x = Dropout(dropout)(x)\n","\n","    # Depth preserving 3*3 conv\n","    x = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # if(Dropout > 0.0):\n","    #   x = Dropout(dropout)(x)\n","\n","    # Expanding 1*1 Conv\n","    x = Conv2D(filters * 4, kernel_size=(1, 1), strides=(1, 1), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","\n","    # Projection shortcut\n","    skip_conv = Conv2D(filters * 4, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n","    skip = BatchNormalization()(skip_conv)\n","\n","    # Skip connection\n","    x = Add()([x, skip])\n","\n","    return x\n","\n","def normal_conv_bottleneck_block(Input, filters, stride=1, dropout=0.0):\n","    # Contracting 1*1 conv\n","    x = Conv2D(filters, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # if(dropout > 0.0):\n","    #   x = Dropout(dropout)(x)\n","\n","    # Depth preserving 3*3 Conv\n","    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # if(Dropout > 0.0):\n","    #    x = Dropout(dropout)(x)\n","\n","    # Expanding 1*1 Conv\n","    x = Conv2D(filters * 4, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","\n","    # Identity skip connection\n","    x = Add()([x, Input])\n","\n","    return x\n","\n","\n","def build_basic_resnet(h, w, no_of_outputs, r1, r2, r3, r4, first_conv_stride=2, first_max_pool=True,\n","                       first_conv_kernel_size=7):\n","    # Creating input tensor\n","    inputs = Input(shape=(h, w, 3), name=\"image_input\")\n","\n","    # Inital Conv block\n","    x = initial_conv(inputs, 64, first_conv_stride, first_conv_kernel_size)\n","\n","    # Optional Max pooling layer\n","    if (first_max_pool):\n","        x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","    # Expanding block1 with projection shortcut\n","    x = expand_conv_basic_block(x, 64, 1)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv1\n","    for i in range(r1 - 1):\n","        x = normal_conv_basic_block(x, 64)\n","        x = Activation('relu')(x)\n","\n","    # Expanding block2 with projection shortcut\n","    x = expand_conv_basic_block(x, 128, 2)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv2\n","    for i in range(r2 - 1):\n","        x = normal_conv_basic_block(x, 128)\n","        x = Activation('relu')(x)\n","\n","    # Expanding block3 with projection shortcut\n","    x = expand_conv_basic_block(x, 256, 2)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv3\n","    for i in range(r3 - 1):\n","        x = normal_conv_basic_block(x, 256)\n","        x = Activation('relu')(x)\n","\n","    # Expanding block4 with projection shortcut\n","    x = expand_conv_basic_block(x, 512, 2)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv3\n","    for i in range(r4 - 1):\n","        x = normal_conv_basic_block(x, 512)\n","        x = Activation('relu')(x)\n","\n","    shape = K.int_shape(x)\n","\n","    # Average pooling layer\n","    x = AveragePooling2D(pool_size=(shape[1], shape[2]),\n","                         strides=(1, 1))(x)\n","    # x = GlobalAveragePooling2D()(x)\n","    x = Flatten()(x)\n","\n","    # Classifier Block\n","    x = Dense(no_of_outputs, activation='softmax')(x)\n","\n","    model = Model(inputs=inputs, outputs=x)\n","    return model\n","\n","\n","def build_bottleneck_resnet(h, w, no_of_outputs, r1, r2, r3, r4, first_conv_stride=2, first_max_pool=True,\n","                            first_conv_kernel_size=7):\n","    # Creating input tensor\n","    inputs = Input(shape=(h, w, 3), name=\"image_input\")\n","\n","    # Inital Conv block\n","    x = initial_conv(inputs, 64, first_conv_stride, first_conv_kernel_size)\n","\n","    # Optional Max pooling layer\n","    if (first_max_pool):\n","        x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","    # Expanding block1 with projection shortcut\n","    x = expand_conv_bottleneck_block(x, 64, 1)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv1\n","    for i in range(r1 - 1):\n","        x = normal_conv_bottleneck_block(x, 64)\n","        x = Activation('relu')(x)\n","\n","    # Expanding block2 with projection shortcut\n","    x = expand_conv_bottleneck_block(x, 128, 2)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv2\n","    for i in range(r2 - 1):\n","        x = normal_conv_bottleneck_block(x, 128)\n","        x = Activation('relu')(x)\n","\n","    # Expanding block3 with projection shortcut\n","    x = expand_conv_bottleneck_block(x, 256, 2)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv3\n","    for i in range(r3 - 1):\n","        x = normal_conv_bottleneck_block(x, 256)\n","        x = Activation('relu')(x)\n","\n","    # Expanding block4 with projection shortcut\n","    x = expand_conv_bottleneck_block(x, 512, 2)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv4\n","    for i in range(r4 - 1):\n","        x = normal_conv_bottleneck_block(x, 512)\n","        x = Activation('relu')(x)\n","\n","    shape = K.int_shape(x)\n","\n","    # Average pooling layer\n","    x = AveragePooling2D(pool_size=(shape[1], shape[2]),\n","                         strides=(1, 1))(x)\n","    # x = GlobalAveragePooling2D()(x)\n","\n","    # Classifier Block\n","    x = Flatten()(x)\n","    x = Dense(no_of_outputs, activation='softmax')(x)\n","\n","    model = Model(inputs=inputs, outputs=x)\n","    return model\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7057,"status":"ok","timestamp":1683883321259,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"8ZfeGbU4J3aB","outputId":"8cbe88b5-5016-4a7b-e921-82c67aac4100"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," image_input (InputLayer)       [(None, 32, 32, 3)]  0           []                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 16, 16, 64)   9472        ['image_input[0][0]']            \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 16, 16, 64)  256         ['conv2d[0][0]']                 \n"," alization)                                                                                       \n","                                                                                                  \n"," activation (Activation)        (None, 16, 16, 64)   0           ['batch_normalization[0][0]']    \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 8, 8, 64)     0           ['activation[0][0]']             \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 8, 8, 64)     4160        ['max_pooling2d[0][0]']          \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_1[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_1 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_1[0][0]']  \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 8, 8, 64)     36928       ['activation_1[0][0]']           \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_2[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_2 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_2[0][0]']  \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 8, 8, 256)    16640       ['activation_2[0][0]']           \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 8, 8, 256)    16640       ['max_pooling2d[0][0]']          \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_3[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_4[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," add (Add)                      (None, 8, 8, 256)    0           ['batch_normalization_3[0][0]',  \n","                                                                  'batch_normalization_4[0][0]']  \n","                                                                                                  \n"," activation_3 (Activation)      (None, 8, 8, 256)    0           ['add[0][0]']                    \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 8, 8, 64)     16448       ['activation_3[0][0]']           \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_5[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_4 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 8, 8, 64)     36928       ['activation_4[0][0]']           \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_6[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_5 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 8, 8, 256)    16640       ['activation_5[0][0]']           \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_7[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," add_1 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_7[0][0]',  \n","                                                                  'activation_3[0][0]']           \n","                                                                                                  \n"," activation_6 (Activation)      (None, 8, 8, 256)    0           ['add_1[0][0]']                  \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 8, 8, 64)     16448       ['activation_6[0][0]']           \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_8[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_7 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_8[0][0]']  \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 8, 8, 64)     36928       ['activation_7[0][0]']           \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_9[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_8 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 8, 8, 256)    16640       ['activation_8[0][0]']           \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_10[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_2 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_10[0][0]', \n","                                                                  'activation_6[0][0]']           \n","                                                                                                  \n"," activation_9 (Activation)      (None, 8, 8, 256)    0           ['add_2[0][0]']                  \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 4, 4, 128)    32896       ['activation_9[0][0]']           \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 4, 4, 128)   512         ['conv2d_11[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_10 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_11[0][0]'] \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 4, 4, 128)    147584      ['activation_10[0][0]']          \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 4, 4, 128)   512         ['conv2d_12[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_11 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 4, 4, 512)    66048       ['activation_11[0][0]']          \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 4, 4, 512)    131584      ['activation_9[0][0]']           \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_13[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_14[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_3 (Add)                    (None, 4, 4, 512)    0           ['batch_normalization_13[0][0]', \n","                                                                  'batch_normalization_14[0][0]'] \n","                                                                                                  \n"," activation_12 (Activation)     (None, 4, 4, 512)    0           ['add_3[0][0]']                  \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 4, 4, 128)    65664       ['activation_12[0][0]']          \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 4, 4, 128)   512         ['conv2d_15[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_13 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_15[0][0]'] \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 4, 4, 128)    147584      ['activation_13[0][0]']          \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 4, 4, 128)   512         ['conv2d_16[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_14 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_16[0][0]'] \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 4, 4, 512)    66048       ['activation_14[0][0]']          \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_17[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_4 (Add)                    (None, 4, 4, 512)    0           ['batch_normalization_17[0][0]', \n","                                                                  'activation_12[0][0]']          \n","                                                                                                  \n"," activation_15 (Activation)     (None, 4, 4, 512)    0           ['add_4[0][0]']                  \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 4, 4, 128)    65664       ['activation_15[0][0]']          \n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 4, 4, 128)   512         ['conv2d_18[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_16 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_18[0][0]'] \n","                                                                                                  \n"," conv2d_19 (Conv2D)             (None, 4, 4, 128)    147584      ['activation_16[0][0]']          \n","                                                                                                  \n"," batch_normalization_19 (BatchN  (None, 4, 4, 128)   512         ['conv2d_19[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_17 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_19[0][0]'] \n","                                                                                                  \n"," conv2d_20 (Conv2D)             (None, 4, 4, 512)    66048       ['activation_17[0][0]']          \n","                                                                                                  \n"," batch_normalization_20 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_20[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_5 (Add)                    (None, 4, 4, 512)    0           ['batch_normalization_20[0][0]', \n","                                                                  'activation_15[0][0]']          \n","                                                                                                  \n"," activation_18 (Activation)     (None, 4, 4, 512)    0           ['add_5[0][0]']                  \n","                                                                                                  \n"," conv2d_21 (Conv2D)             (None, 4, 4, 128)    65664       ['activation_18[0][0]']          \n","                                                                                                  \n"," batch_normalization_21 (BatchN  (None, 4, 4, 128)   512         ['conv2d_21[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_19 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_21[0][0]'] \n","                                                                                                  \n"," conv2d_22 (Conv2D)             (None, 4, 4, 128)    147584      ['activation_19[0][0]']          \n","                                                                                                  \n"," batch_normalization_22 (BatchN  (None, 4, 4, 128)   512         ['conv2d_22[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_20 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_22[0][0]'] \n","                                                                                                  \n"," conv2d_23 (Conv2D)             (None, 4, 4, 512)    66048       ['activation_20[0][0]']          \n","                                                                                                  \n"," batch_normalization_23 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_23[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_6 (Add)                    (None, 4, 4, 512)    0           ['batch_normalization_23[0][0]', \n","                                                                  'activation_18[0][0]']          \n","                                                                                                  \n"," activation_21 (Activation)     (None, 4, 4, 512)    0           ['add_6[0][0]']                  \n","                                                                                                  \n"," conv2d_24 (Conv2D)             (None, 2, 2, 256)    131328      ['activation_21[0][0]']          \n","                                                                                                  \n"," batch_normalization_24 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_24[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_22 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_24[0][0]'] \n","                                                                                                  \n"," conv2d_25 (Conv2D)             (None, 2, 2, 256)    590080      ['activation_22[0][0]']          \n","                                                                                                  \n"," batch_normalization_25 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_25[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_23 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_25[0][0]'] \n","                                                                                                  \n"," conv2d_26 (Conv2D)             (None, 2, 2, 1024)   263168      ['activation_23[0][0]']          \n","                                                                                                  \n"," conv2d_27 (Conv2D)             (None, 2, 2, 1024)   525312      ['activation_21[0][0]']          \n","                                                                                                  \n"," batch_normalization_26 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_26[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_27 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_27[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_7 (Add)                    (None, 2, 2, 1024)   0           ['batch_normalization_26[0][0]', \n","                                                                  'batch_normalization_27[0][0]'] \n","                                                                                                  \n"," activation_24 (Activation)     (None, 2, 2, 1024)   0           ['add_7[0][0]']                  \n","                                                                                                  \n"," conv2d_28 (Conv2D)             (None, 2, 2, 256)    262400      ['activation_24[0][0]']          \n","                                                                                                  \n"," batch_normalization_28 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_28[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_25 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_28[0][0]'] \n","                                                                                                  \n"," conv2d_29 (Conv2D)             (None, 2, 2, 256)    590080      ['activation_25[0][0]']          \n","                                                                                                  \n"," batch_normalization_29 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_29[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_26 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_29[0][0]'] \n","                                                                                                  \n"," conv2d_30 (Conv2D)             (None, 2, 2, 1024)   263168      ['activation_26[0][0]']          \n","                                                                                                  \n"," batch_normalization_30 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_30[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_8 (Add)                    (None, 2, 2, 1024)   0           ['batch_normalization_30[0][0]', \n","                                                                  'activation_24[0][0]']          \n","                                                                                                  \n"," activation_27 (Activation)     (None, 2, 2, 1024)   0           ['add_8[0][0]']                  \n","                                                                                                  \n"," conv2d_31 (Conv2D)             (None, 2, 2, 256)    262400      ['activation_27[0][0]']          \n","                                                                                                  \n"," batch_normalization_31 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_31[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_28 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_31[0][0]'] \n","                                                                                                  \n"," conv2d_32 (Conv2D)             (None, 2, 2, 256)    590080      ['activation_28[0][0]']          \n","                                                                                                  \n"," batch_normalization_32 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_32[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_29 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_32[0][0]'] \n","                                                                                                  \n"," conv2d_33 (Conv2D)             (None, 2, 2, 1024)   263168      ['activation_29[0][0]']          \n","                                                                                                  \n"," batch_normalization_33 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_33[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_9 (Add)                    (None, 2, 2, 1024)   0           ['batch_normalization_33[0][0]', \n","                                                                  'activation_27[0][0]']          \n","                                                                                                  \n"," activation_30 (Activation)     (None, 2, 2, 1024)   0           ['add_9[0][0]']                  \n","                                                                                                  \n"," conv2d_34 (Conv2D)             (None, 2, 2, 256)    262400      ['activation_30[0][0]']          \n","                                                                                                  \n"," batch_normalization_34 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_34[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_31 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_34[0][0]'] \n","                                                                                                  \n"," conv2d_35 (Conv2D)             (None, 2, 2, 256)    590080      ['activation_31[0][0]']          \n","                                                                                                  \n"," batch_normalization_35 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_35[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_32 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_35[0][0]'] \n","                                                                                                  \n"," conv2d_36 (Conv2D)             (None, 2, 2, 1024)   263168      ['activation_32[0][0]']          \n","                                                                                                  \n"," batch_normalization_36 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_36[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_10 (Add)                   (None, 2, 2, 1024)   0           ['batch_normalization_36[0][0]', \n","                                                                  'activation_30[0][0]']          \n","                                                                                                  \n"," activation_33 (Activation)     (None, 2, 2, 1024)   0           ['add_10[0][0]']                 \n","                                                                                                  \n"," conv2d_37 (Conv2D)             (None, 2, 2, 256)    262400      ['activation_33[0][0]']          \n","                                                                                                  \n"," batch_normalization_37 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_37[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_34 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_37[0][0]'] \n","                                                                                                  \n"," conv2d_38 (Conv2D)             (None, 2, 2, 256)    590080      ['activation_34[0][0]']          \n","                                                                                                  \n"," batch_normalization_38 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_38[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_35 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_38[0][0]'] \n","                                                                                                  \n"," conv2d_39 (Conv2D)             (None, 2, 2, 1024)   263168      ['activation_35[0][0]']          \n","                                                                                                  \n"," batch_normalization_39 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_39[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_11 (Add)                   (None, 2, 2, 1024)   0           ['batch_normalization_39[0][0]', \n","                                                                  'activation_33[0][0]']          \n","                                                                                                  \n"," activation_36 (Activation)     (None, 2, 2, 1024)   0           ['add_11[0][0]']                 \n","                                                                                                  \n"," conv2d_40 (Conv2D)             (None, 2, 2, 256)    262400      ['activation_36[0][0]']          \n","                                                                                                  \n"," batch_normalization_40 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_40[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_37 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_40[0][0]'] \n","                                                                                                  \n"," conv2d_41 (Conv2D)             (None, 2, 2, 256)    590080      ['activation_37[0][0]']          \n","                                                                                                  \n"," batch_normalization_41 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_41[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_38 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_41[0][0]'] \n","                                                                                                  \n"," conv2d_42 (Conv2D)             (None, 2, 2, 1024)   263168      ['activation_38[0][0]']          \n","                                                                                                  \n"," batch_normalization_42 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_42[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_12 (Add)                   (None, 2, 2, 1024)   0           ['batch_normalization_42[0][0]', \n","                                                                  'activation_36[0][0]']          \n","                                                                                                  \n"," activation_39 (Activation)     (None, 2, 2, 1024)   0           ['add_12[0][0]']                 \n","                                                                                                  \n"," conv2d_43 (Conv2D)             (None, 1, 1, 512)    524800      ['activation_39[0][0]']          \n","                                                                                                  \n"," batch_normalization_43 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_43[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_40 (Activation)     (None, 1, 1, 512)    0           ['batch_normalization_43[0][0]'] \n","                                                                                                  \n"," conv2d_44 (Conv2D)             (None, 1, 1, 512)    2359808     ['activation_40[0][0]']          \n","                                                                                                  \n"," batch_normalization_44 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_44[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_41 (Activation)     (None, 1, 1, 512)    0           ['batch_normalization_44[0][0]'] \n","                                                                                                  \n"," conv2d_45 (Conv2D)             (None, 1, 1, 2048)   1050624     ['activation_41[0][0]']          \n","                                                                                                  \n"," conv2d_46 (Conv2D)             (None, 1, 1, 2048)   2099200     ['activation_39[0][0]']          \n","                                                                                                  \n"," batch_normalization_45 (BatchN  (None, 1, 1, 2048)  8192        ['conv2d_45[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_46 (BatchN  (None, 1, 1, 2048)  8192        ['conv2d_46[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_13 (Add)                   (None, 1, 1, 2048)   0           ['batch_normalization_45[0][0]', \n","                                                                  'batch_normalization_46[0][0]'] \n","                                                                                                  \n"," activation_42 (Activation)     (None, 1, 1, 2048)   0           ['add_13[0][0]']                 \n","                                                                                                  \n"," conv2d_47 (Conv2D)             (None, 1, 1, 512)    1049088     ['activation_42[0][0]']          \n","                                                                                                  \n"," batch_normalization_47 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_47[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_43 (Activation)     (None, 1, 1, 512)    0           ['batch_normalization_47[0][0]'] \n","                                                                                                  \n"," conv2d_48 (Conv2D)             (None, 1, 1, 512)    2359808     ['activation_43[0][0]']          \n","                                                                                                  \n"," batch_normalization_48 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_48[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_44 (Activation)     (None, 1, 1, 512)    0           ['batch_normalization_48[0][0]'] \n","                                                                                                  \n"," conv2d_49 (Conv2D)             (None, 1, 1, 2048)   1050624     ['activation_44[0][0]']          \n","                                                                                                  \n"," batch_normalization_49 (BatchN  (None, 1, 1, 2048)  8192        ['conv2d_49[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_14 (Add)                   (None, 1, 1, 2048)   0           ['batch_normalization_49[0][0]', \n","                                                                  'activation_42[0][0]']          \n","                                                                                                  \n"," activation_45 (Activation)     (None, 1, 1, 2048)   0           ['add_14[0][0]']                 \n","                                                                                                  \n"," conv2d_50 (Conv2D)             (None, 1, 1, 512)    1049088     ['activation_45[0][0]']          \n","                                                                                                  \n"," batch_normalization_50 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_50[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_46 (Activation)     (None, 1, 1, 512)    0           ['batch_normalization_50[0][0]'] \n","                                                                                                  \n"," conv2d_51 (Conv2D)             (None, 1, 1, 512)    2359808     ['activation_46[0][0]']          \n","                                                                                                  \n"," batch_normalization_51 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_51[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_47 (Activation)     (None, 1, 1, 512)    0           ['batch_normalization_51[0][0]'] \n","                                                                                                  \n"," conv2d_52 (Conv2D)             (None, 1, 1, 2048)   1050624     ['activation_47[0][0]']          \n","                                                                                                  \n"," batch_normalization_52 (BatchN  (None, 1, 1, 2048)  8192        ['conv2d_52[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_15 (Add)                   (None, 1, 1, 2048)   0           ['batch_normalization_52[0][0]', \n","                                                                  'activation_45[0][0]']          \n","                                                                                                  \n"," activation_48 (Activation)     (None, 1, 1, 2048)   0           ['add_15[0][0]']                 \n","                                                                                                  \n"," average_pooling2d (AveragePool  (None, 1, 1, 2048)  0           ['activation_48[0][0]']          \n"," ing2D)                                                                                           \n","                                                                                                  \n"," flatten (Flatten)              (None, 2048)         0           ['average_pooling2d[0][0]']      \n","                                                                                                  \n"," dense (Dense)                  (None, 2)            4098        ['flatten[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 23,591,810\n","Trainable params: 23,538,690\n","Non-trainable params: 53,120\n","__________________________________________________________________________________________________\n"]}],"source":["model = build_bottleneck_resnet(32,32,2,3,4,6,3,2,True,7)\n","model.summary()\n","plot_model(model,\"ResNet50.png\",show_shapes=True)\n","model.compile(loss='binary_crossentropy',\n","        optimizer=\"Adam\",\n","        metrics=['accuracy',f1,sensitivity,specificity])\n"]},{"cell_type":"code","source":["METRICS_PATH+MODEL_NAME+\".csv\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"baZJgX6_PrSq","executionInfo":{"status":"ok","timestamp":1683883354288,"user_tz":-330,"elapsed":608,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"}},"outputId":"1e5353cf-f16b-41d3-a860-69dc23f3badb"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Metrics/Malaria_Cell_Metrics/ResNet.csv'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"G_GW2KEVHXvh","executionInfo":{"status":"ok","timestamp":1683883444246,"user_tz":-330,"elapsed":2,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"}}},"outputs":[],"source":["lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)\n","csv_logger = CSVLogger(METRICS_PATH+MODEL_NAME+\".csv\")\n","\n","model_chekpoint = ModelCheckpoint(\"ResNet50_F1score_DA_aug.hdf5\",monitor = 'val_loss',verbose = 1,save_best_only=True)\n"]},{"cell_type":"markdown","metadata":{"id":"vavqPMAvNWQQ"},"source":["# Model Training"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"DMTEO8kRHEq3","executionInfo":{"status":"ok","timestamp":1683883447391,"user_tz":-330,"elapsed":2,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"}}},"outputs":[],"source":["\n","batch_size = 16\n","data_augmentation = True\n","epochs = 100"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9m2mVaSaHxVN","executionInfo":{"status":"ok","timestamp":1683890977929,"user_tz":-330,"elapsed":7527453,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"}},"outputId":"a68a664b-a5d9-4a82-f531-02bf415e9e76"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------Using Data augmentation------------\n","Epoch 1/100\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-18-2c13fe4e579f>:14: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n"]},{"output_type":"stream","name":"stdout","text":["1377/1377 [==============================] - ETA: 0s - loss: 0.7254 - accuracy: 0.6474 - f1: 0.6474 - sensitivity: 0.6474 - specificity: 0.6474\n","Epoch 1: val_loss improved from inf to 1.21388, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 124s 55ms/step - loss: 0.7254 - accuracy: 0.6474 - f1: 0.6474 - sensitivity: 0.6474 - specificity: 0.6474 - val_loss: 1.2139 - val_accuracy: 0.5539 - val_f1: 0.5531 - val_sensitivity: 0.5531 - val_specificity: 0.5531 - lr: 0.0010\n","Epoch 2/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.6833 - accuracy: 0.6598 - f1: 0.6598 - sensitivity: 0.6598 - specificity: 0.6598\n","Epoch 2: val_loss did not improve from 1.21388\n","1377/1377 [==============================] - 76s 55ms/step - loss: 0.6833 - accuracy: 0.6598 - f1: 0.6598 - sensitivity: 0.6598 - specificity: 0.6598 - val_loss: 1.3865 - val_accuracy: 0.6116 - val_f1: 0.6111 - val_sensitivity: 0.6111 - val_specificity: 0.6111 - lr: 0.0010\n","Epoch 3/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.6596 - accuracy: 0.6723 - f1: 0.6722 - sensitivity: 0.6722 - specificity: 0.6722\n","Epoch 3: val_loss improved from 1.21388 to 1.18588, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 75s 55ms/step - loss: 0.6597 - accuracy: 0.6720 - f1: 0.6720 - sensitivity: 0.6720 - specificity: 0.6720 - val_loss: 1.1859 - val_accuracy: 0.6381 - val_f1: 0.6380 - val_sensitivity: 0.6380 - val_specificity: 0.6380 - lr: 0.0010\n","Epoch 4/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.4894 - accuracy: 0.7808 - f1: 0.7808 - sensitivity: 0.7808 - specificity: 0.7808\n","Epoch 4: val_loss improved from 1.18588 to 0.21618, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 70s 51ms/step - loss: 0.4894 - accuracy: 0.7808 - f1: 0.7808 - sensitivity: 0.7808 - specificity: 0.7808 - val_loss: 0.2162 - val_accuracy: 0.9332 - val_f1: 0.9335 - val_sensitivity: 0.9335 - val_specificity: 0.9335 - lr: 0.0010\n","Epoch 5/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.2096 - accuracy: 0.9315 - f1: 0.9315 - sensitivity: 0.9315 - specificity: 0.9315\n","Epoch 5: val_loss improved from 0.21618 to 0.20270, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 70s 51ms/step - loss: 0.2098 - accuracy: 0.9314 - f1: 0.9314 - sensitivity: 0.9314 - specificity: 0.9314 - val_loss: 0.2027 - val_accuracy: 0.9503 - val_f1: 0.9505 - val_sensitivity: 0.9505 - val_specificity: 0.9505 - lr: 0.0010\n","Epoch 6/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.2032 - accuracy: 0.9374 - f1: 0.9374 - sensitivity: 0.9374 - specificity: 0.9374\n","Epoch 6: val_loss did not improve from 0.20270\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.2032 - accuracy: 0.9374 - f1: 0.9374 - sensitivity: 0.9374 - specificity: 0.9374 - val_loss: 0.2123 - val_accuracy: 0.9274 - val_f1: 0.9277 - val_sensitivity: 0.9277 - val_specificity: 0.9277 - lr: 0.0010\n","Epoch 7/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1856 - accuracy: 0.9440 - f1: 0.9440 - sensitivity: 0.9440 - specificity: 0.9440\n","Epoch 7: val_loss improved from 0.20270 to 0.17426, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1856 - accuracy: 0.9440 - f1: 0.9440 - sensitivity: 0.9440 - specificity: 0.9440 - val_loss: 0.1743 - val_accuracy: 0.9492 - val_f1: 0.9494 - val_sensitivity: 0.9494 - val_specificity: 0.9494 - lr: 0.0010\n","Epoch 8/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1810 - accuracy: 0.9462 - f1: 0.9462 - sensitivity: 0.9462 - specificity: 0.9462\n","Epoch 8: val_loss did not improve from 0.17426\n","1377/1377 [==============================] - 67s 48ms/step - loss: 0.1810 - accuracy: 0.9462 - f1: 0.9462 - sensitivity: 0.9462 - specificity: 0.9462 - val_loss: 0.1860 - val_accuracy: 0.9488 - val_f1: 0.9485 - val_sensitivity: 0.9485 - val_specificity: 0.9485 - lr: 0.0010\n","Epoch 9/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.9471 - f1: 0.9471 - sensitivity: 0.9471 - specificity: 0.9471\n","Epoch 9: val_loss improved from 0.17426 to 0.16041, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1716 - accuracy: 0.9471 - f1: 0.9471 - sensitivity: 0.9471 - specificity: 0.9471 - val_loss: 0.1604 - val_accuracy: 0.9512 - val_f1: 0.9509 - val_sensitivity: 0.9509 - val_specificity: 0.9509 - lr: 0.0010\n","Epoch 10/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9483 - f1: 0.9483 - sensitivity: 0.9483 - specificity: 0.9483\n","Epoch 10: val_loss did not improve from 0.16041\n","1377/1377 [==============================] - 65s 47ms/step - loss: 0.1689 - accuracy: 0.9483 - f1: 0.9483 - sensitivity: 0.9483 - specificity: 0.9483 - val_loss: 0.1884 - val_accuracy: 0.9525 - val_f1: 0.9521 - val_sensitivity: 0.9521 - val_specificity: 0.9521 - lr: 0.0010\n","Epoch 11/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1733 - accuracy: 0.9476 - f1: 0.9476 - sensitivity: 0.9476 - specificity: 0.9476\n","Epoch 11: val_loss did not improve from 0.16041\n","1377/1377 [==============================] - 66s 48ms/step - loss: 0.1733 - accuracy: 0.9476 - f1: 0.9476 - sensitivity: 0.9476 - specificity: 0.9476 - val_loss: 0.2872 - val_accuracy: 0.9218 - val_f1: 0.9216 - val_sensitivity: 0.9216 - val_specificity: 0.9216 - lr: 0.0010\n","Epoch 12/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1455 - accuracy: 0.9538 - f1: 0.9538 - sensitivity: 0.9538 - specificity: 0.9538\n","Epoch 12: val_loss improved from 0.16041 to 0.14554, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 66s 48ms/step - loss: 0.1455 - accuracy: 0.9538 - f1: 0.9538 - sensitivity: 0.9538 - specificity: 0.9538 - val_loss: 0.1455 - val_accuracy: 0.9575 - val_f1: 0.9572 - val_sensitivity: 0.9572 - val_specificity: 0.9572 - lr: 3.1623e-04\n","Epoch 13/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.9568 - f1: 0.9568 - sensitivity: 0.9568 - specificity: 0.9568\n","Epoch 13: val_loss improved from 0.14554 to 0.13941, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1367 - accuracy: 0.9569 - f1: 0.9569 - sensitivity: 0.9569 - specificity: 0.9569 - val_loss: 0.1394 - val_accuracy: 0.9568 - val_f1: 0.9565 - val_sensitivity: 0.9565 - val_specificity: 0.9565 - lr: 3.1623e-04\n","Epoch 14/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.9573 - f1: 0.9573 - sensitivity: 0.9573 - specificity: 0.9573\n","Epoch 14: val_loss improved from 0.13941 to 0.13801, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 67s 49ms/step - loss: 0.1353 - accuracy: 0.9573 - f1: 0.9573 - sensitivity: 0.9573 - specificity: 0.9573 - val_loss: 0.1380 - val_accuracy: 0.9561 - val_f1: 0.9563 - val_sensitivity: 0.9563 - val_specificity: 0.9563 - lr: 3.1623e-04\n","Epoch 15/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9578 - f1: 0.9578 - sensitivity: 0.9578 - specificity: 0.9578\n","Epoch 15: val_loss improved from 0.13801 to 0.12519, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 71s 52ms/step - loss: 0.1317 - accuracy: 0.9578 - f1: 0.9578 - sensitivity: 0.9578 - specificity: 0.9578 - val_loss: 0.1252 - val_accuracy: 0.9604 - val_f1: 0.9606 - val_sensitivity: 0.9606 - val_specificity: 0.9606 - lr: 3.1623e-04\n","Epoch 16/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.9587 - f1: 0.9587 - sensitivity: 0.9587 - specificity: 0.9587\n","Epoch 16: val_loss did not improve from 0.12519\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1321 - accuracy: 0.9587 - f1: 0.9587 - sensitivity: 0.9587 - specificity: 0.9587 - val_loss: 0.1313 - val_accuracy: 0.9572 - val_f1: 0.9568 - val_sensitivity: 0.9568 - val_specificity: 0.9568 - lr: 3.1623e-04\n","Epoch 17/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9590 - f1: 0.9590 - sensitivity: 0.9590 - specificity: 0.9590\n","Epoch 17: val_loss did not improve from 0.12519\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1306 - accuracy: 0.9590 - f1: 0.9590 - sensitivity: 0.9590 - specificity: 0.9590 - val_loss: 0.1386 - val_accuracy: 0.9579 - val_f1: 0.9581 - val_sensitivity: 0.9581 - val_specificity: 0.9581 - lr: 3.1623e-04\n","Epoch 18/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1229 - accuracy: 0.9601 - f1: 0.9601 - sensitivity: 0.9601 - specificity: 0.9601\n","Epoch 18: val_loss improved from 0.12519 to 0.11941, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 71s 51ms/step - loss: 0.1229 - accuracy: 0.9601 - f1: 0.9601 - sensitivity: 0.9601 - specificity: 0.9601 - val_loss: 0.1194 - val_accuracy: 0.9594 - val_f1: 0.9590 - val_sensitivity: 0.9590 - val_specificity: 0.9590 - lr: 1.0000e-04\n","Epoch 19/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1186 - accuracy: 0.9606 - f1: 0.9606 - sensitivity: 0.9606 - specificity: 0.9606\n","Epoch 19: val_loss did not improve from 0.11941\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1186 - accuracy: 0.9606 - f1: 0.9606 - sensitivity: 0.9606 - specificity: 0.9606 - val_loss: 0.1275 - val_accuracy: 0.9592 - val_f1: 0.9588 - val_sensitivity: 0.9588 - val_specificity: 0.9588 - lr: 1.0000e-04\n","Epoch 20/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9620 - f1: 0.9620 - sensitivity: 0.9620 - specificity: 0.9620\n","Epoch 20: val_loss improved from 0.11941 to 0.11710, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1180 - accuracy: 0.9620 - f1: 0.9620 - sensitivity: 0.9620 - specificity: 0.9620 - val_loss: 0.1171 - val_accuracy: 0.9595 - val_f1: 0.9597 - val_sensitivity: 0.9597 - val_specificity: 0.9597 - lr: 1.0000e-04\n","Epoch 21/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9628 - f1: 0.9628 - sensitivity: 0.9628 - specificity: 0.9628\n","Epoch 21: val_loss did not improve from 0.11710\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1160 - accuracy: 0.9628 - f1: 0.9628 - sensitivity: 0.9628 - specificity: 0.9628 - val_loss: 0.1172 - val_accuracy: 0.9601 - val_f1: 0.9603 - val_sensitivity: 0.9603 - val_specificity: 0.9603 - lr: 1.0000e-04\n","Epoch 22/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9619 - f1: 0.9619 - sensitivity: 0.9619 - specificity: 0.9619\n","Epoch 22: val_loss did not improve from 0.11710\n","1377/1377 [==============================] - 68s 50ms/step - loss: 0.1170 - accuracy: 0.9619 - f1: 0.9619 - sensitivity: 0.9619 - specificity: 0.9619 - val_loss: 0.1193 - val_accuracy: 0.9614 - val_f1: 0.9615 - val_sensitivity: 0.9615 - val_specificity: 0.9615 - lr: 1.0000e-04\n","Epoch 23/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9617 - f1: 0.9617 - sensitivity: 0.9617 - specificity: 0.9617\n","Epoch 23: val_loss improved from 0.11710 to 0.11589, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 70s 51ms/step - loss: 0.1175 - accuracy: 0.9617 - f1: 0.9617 - sensitivity: 0.9617 - specificity: 0.9617 - val_loss: 0.1159 - val_accuracy: 0.9615 - val_f1: 0.9617 - val_sensitivity: 0.9617 - val_specificity: 0.9617 - lr: 3.1623e-05\n","Epoch 24/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1137 - accuracy: 0.9619 - f1: 0.9619 - sensitivity: 0.9619 - specificity: 0.9619\n","Epoch 24: val_loss improved from 0.11589 to 0.11533, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 70s 51ms/step - loss: 0.1138 - accuracy: 0.9619 - f1: 0.9619 - sensitivity: 0.9619 - specificity: 0.9619 - val_loss: 0.1153 - val_accuracy: 0.9624 - val_f1: 0.9626 - val_sensitivity: 0.9626 - val_specificity: 0.9626 - lr: 3.1623e-05\n","Epoch 25/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9633 - f1: 0.9633 - sensitivity: 0.9633 - specificity: 0.9633\n","Epoch 25: val_loss improved from 0.11533 to 0.11397, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1108 - accuracy: 0.9633 - f1: 0.9633 - sensitivity: 0.9633 - specificity: 0.9633 - val_loss: 0.1140 - val_accuracy: 0.9617 - val_f1: 0.9619 - val_sensitivity: 0.9619 - val_specificity: 0.9619 - lr: 3.1623e-05\n","Epoch 26/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.9635 - f1: 0.9635 - sensitivity: 0.9635 - specificity: 0.9635\n","Epoch 26: val_loss did not improve from 0.11397\n","1377/1377 [==============================] - 72s 52ms/step - loss: 0.1118 - accuracy: 0.9635 - f1: 0.9635 - sensitivity: 0.9635 - specificity: 0.9635 - val_loss: 0.1140 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 3.1623e-05\n","Epoch 27/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1113 - accuracy: 0.9643 - f1: 0.9643 - sensitivity: 0.9643 - specificity: 0.9643\n","Epoch 27: val_loss improved from 0.11397 to 0.11280, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 76s 55ms/step - loss: 0.1113 - accuracy: 0.9643 - f1: 0.9643 - sensitivity: 0.9643 - specificity: 0.9643 - val_loss: 0.1128 - val_accuracy: 0.9637 - val_f1: 0.9639 - val_sensitivity: 0.9639 - val_specificity: 0.9639 - lr: 3.1623e-05\n","Epoch 28/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9631 - f1: 0.9631 - sensitivity: 0.9631 - specificity: 0.9631\n","Epoch 28: val_loss improved from 0.11280 to 0.11182, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 77s 56ms/step - loss: 0.1115 - accuracy: 0.9631 - f1: 0.9631 - sensitivity: 0.9631 - specificity: 0.9631 - val_loss: 0.1118 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 3.1623e-05\n","Epoch 29/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9650 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651\n","Epoch 29: val_loss did not improve from 0.11182\n","1377/1377 [==============================] - 73s 53ms/step - loss: 0.1081 - accuracy: 0.9650 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651 - val_loss: 0.1125 - val_accuracy: 0.9626 - val_f1: 0.9628 - val_sensitivity: 0.9628 - val_specificity: 0.9628 - lr: 3.1623e-05\n","Epoch 30/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1087 - accuracy: 0.9635 - f1: 0.9635 - sensitivity: 0.9635 - specificity: 0.9635\n","Epoch 30: val_loss improved from 0.11182 to 0.11019, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1086 - accuracy: 0.9635 - f1: 0.9636 - sensitivity: 0.9636 - specificity: 0.9636 - val_loss: 0.1102 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 3.1623e-05\n","Epoch 31/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9635 - f1: 0.9636 - sensitivity: 0.9636 - specificity: 0.9636\n","Epoch 31: val_loss did not improve from 0.11019\n","1377/1377 [==============================] - 67s 49ms/step - loss: 0.1082 - accuracy: 0.9635 - f1: 0.9636 - sensitivity: 0.9636 - specificity: 0.9636 - val_loss: 0.1105 - val_accuracy: 0.9614 - val_f1: 0.9615 - val_sensitivity: 0.9615 - val_specificity: 0.9615 - lr: 3.1623e-05\n","Epoch 32/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1109 - accuracy: 0.9628 - f1: 0.9628 - sensitivity: 0.9628 - specificity: 0.9628\n","Epoch 32: val_loss did not improve from 0.11019\n","1377/1377 [==============================] - 68s 50ms/step - loss: 0.1109 - accuracy: 0.9628 - f1: 0.9628 - sensitivity: 0.9628 - specificity: 0.9628 - val_loss: 0.1109 - val_accuracy: 0.9614 - val_f1: 0.9615 - val_sensitivity: 0.9615 - val_specificity: 0.9615 - lr: 3.1623e-05\n","Epoch 33/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9647 - f1: 0.9647 - sensitivity: 0.9647 - specificity: 0.9647\n","Epoch 33: val_loss did not improve from 0.11019\n","1377/1377 [==============================] - 67s 49ms/step - loss: 0.1050 - accuracy: 0.9647 - f1: 0.9647 - sensitivity: 0.9647 - specificity: 0.9647 - val_loss: 0.1109 - val_accuracy: 0.9612 - val_f1: 0.9613 - val_sensitivity: 0.9613 - val_specificity: 0.9613 - lr: 1.0000e-05\n","Epoch 34/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9641 - f1: 0.9641 - sensitivity: 0.9641 - specificity: 0.9641\n","Epoch 34: val_loss improved from 0.11019 to 0.10850, saving model to ResNet50_F1score_DA_aug.hdf5\n","1377/1377 [==============================] - 71s 51ms/step - loss: 0.1086 - accuracy: 0.9641 - f1: 0.9641 - sensitivity: 0.9641 - specificity: 0.9641 - val_loss: 0.1085 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 1.0000e-05\n","Epoch 35/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9638 - f1: 0.9638 - sensitivity: 0.9638 - specificity: 0.9638\n","Epoch 35: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 50ms/step - loss: 0.1070 - accuracy: 0.9638 - f1: 0.9638 - sensitivity: 0.9638 - specificity: 0.9638 - val_loss: 0.1090 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 1.0000e-05\n","Epoch 36/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1064 - accuracy: 0.9646 - f1: 0.9646 - sensitivity: 0.9646 - specificity: 0.9646\n","Epoch 36: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 67s 49ms/step - loss: 0.1064 - accuracy: 0.9646 - f1: 0.9646 - sensitivity: 0.9646 - specificity: 0.9646 - val_loss: 0.1095 - val_accuracy: 0.9624 - val_f1: 0.9626 - val_sensitivity: 0.9626 - val_specificity: 0.9626 - lr: 1.0000e-05\n","Epoch 37/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9637 - f1: 0.9637 - sensitivity: 0.9637 - specificity: 0.9637\n","Epoch 37: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 66s 48ms/step - loss: 0.1057 - accuracy: 0.9637 - f1: 0.9637 - sensitivity: 0.9637 - specificity: 0.9637 - val_loss: 0.1094 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 3.1623e-06\n","Epoch 38/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1056 - accuracy: 0.9640 - f1: 0.9640 - sensitivity: 0.9640 - specificity: 0.9640\n","Epoch 38: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 66s 48ms/step - loss: 0.1056 - accuracy: 0.9640 - f1: 0.9640 - sensitivity: 0.9640 - specificity: 0.9640 - val_loss: 0.1088 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 3.1623e-06\n","Epoch 39/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1024 - accuracy: 0.9654 - f1: 0.9654 - sensitivity: 0.9654 - specificity: 0.9654\n","Epoch 39: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1027 - accuracy: 0.9653 - f1: 0.9653 - sensitivity: 0.9653 - specificity: 0.9653 - val_loss: 0.1091 - val_accuracy: 0.9626 - val_f1: 0.9628 - val_sensitivity: 0.9628 - val_specificity: 0.9628 - lr: 1.0000e-06\n","Epoch 40/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1069 - accuracy: 0.9646 - f1: 0.9646 - sensitivity: 0.9646 - specificity: 0.9646\n","Epoch 40: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1069 - accuracy: 0.9646 - f1: 0.9646 - sensitivity: 0.9646 - specificity: 0.9646 - val_loss: 0.1095 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 1.0000e-06\n","Epoch 41/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9653 - f1: 0.9653 - sensitivity: 0.9653 - specificity: 0.9653\n","Epoch 41: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 50ms/step - loss: 0.1049 - accuracy: 0.9653 - f1: 0.9653 - sensitivity: 0.9653 - specificity: 0.9653 - val_loss: 0.1089 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 5.0000e-07\n","Epoch 42/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1049 - accuracy: 0.9649 - f1: 0.9649 - sensitivity: 0.9649 - specificity: 0.9649\n","Epoch 42: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1049 - accuracy: 0.9650 - f1: 0.9650 - sensitivity: 0.9650 - specificity: 0.9650 - val_loss: 0.1093 - val_accuracy: 0.9626 - val_f1: 0.9628 - val_sensitivity: 0.9628 - val_specificity: 0.9628 - lr: 5.0000e-07\n","Epoch 43/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9649 - f1: 0.9649 - sensitivity: 0.9649 - specificity: 0.9649\n","Epoch 43: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 71s 52ms/step - loss: 0.1047 - accuracy: 0.9649 - f1: 0.9649 - sensitivity: 0.9649 - specificity: 0.9649 - val_loss: 0.1091 - val_accuracy: 0.9626 - val_f1: 0.9628 - val_sensitivity: 0.9628 - val_specificity: 0.9628 - lr: 5.0000e-07\n","Epoch 44/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9642 - f1: 0.9642 - sensitivity: 0.9642 - specificity: 0.9642\n","Epoch 44: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1046 - accuracy: 0.9642 - f1: 0.9642 - sensitivity: 0.9642 - specificity: 0.9642 - val_loss: 0.1091 - val_accuracy: 0.9626 - val_f1: 0.9628 - val_sensitivity: 0.9628 - val_specificity: 0.9628 - lr: 5.0000e-07\n","Epoch 45/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9634 - f1: 0.9634 - sensitivity: 0.9634 - specificity: 0.9634\n","Epoch 45: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 71s 52ms/step - loss: 0.1069 - accuracy: 0.9634 - f1: 0.9634 - sensitivity: 0.9634 - specificity: 0.9634 - val_loss: 0.1089 - val_accuracy: 0.9617 - val_f1: 0.9619 - val_sensitivity: 0.9619 - val_specificity: 0.9619 - lr: 5.0000e-07\n","Epoch 46/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1041 - accuracy: 0.9654 - f1: 0.9654 - sensitivity: 0.9654 - specificity: 0.9654\n","Epoch 46: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1042 - accuracy: 0.9653 - f1: 0.9653 - sensitivity: 0.9653 - specificity: 0.9653 - val_loss: 0.1090 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 5.0000e-07\n","Epoch 47/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1047 - accuracy: 0.9651 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651\n","Epoch 47: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1047 - accuracy: 0.9651 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651 - val_loss: 0.1092 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 5.0000e-07\n","Epoch 48/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1066 - accuracy: 0.9648 - f1: 0.9648 - sensitivity: 0.9648 - specificity: 0.9648\n","Epoch 48: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 50ms/step - loss: 0.1066 - accuracy: 0.9648 - f1: 0.9648 - sensitivity: 0.9648 - specificity: 0.9648 - val_loss: 0.1099 - val_accuracy: 0.9624 - val_f1: 0.9626 - val_sensitivity: 0.9626 - val_specificity: 0.9626 - lr: 5.0000e-07\n","Epoch 49/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9651 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651\n","Epoch 49: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1046 - accuracy: 0.9650 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651 - val_loss: 0.1094 - val_accuracy: 0.9624 - val_f1: 0.9626 - val_sensitivity: 0.9626 - val_specificity: 0.9626 - lr: 5.0000e-07\n","Epoch 50/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.9645 - f1: 0.9645 - sensitivity: 0.9645 - specificity: 0.9645\n","Epoch 50: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1055 - accuracy: 0.9645 - f1: 0.9645 - sensitivity: 0.9645 - specificity: 0.9645 - val_loss: 0.1094 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 5.0000e-07\n","Epoch 51/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9641 - f1: 0.9641 - sensitivity: 0.9641 - specificity: 0.9641\n","Epoch 51: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 67s 49ms/step - loss: 0.1057 - accuracy: 0.9641 - f1: 0.9641 - sensitivity: 0.9641 - specificity: 0.9641 - val_loss: 0.1094 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 5.0000e-07\n","Epoch 52/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1052 - accuracy: 0.9644 - f1: 0.9644 - sensitivity: 0.9644 - specificity: 0.9644\n","Epoch 52: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1052 - accuracy: 0.9644 - f1: 0.9644 - sensitivity: 0.9644 - specificity: 0.9644 - val_loss: 0.1089 - val_accuracy: 0.9626 - val_f1: 0.9628 - val_sensitivity: 0.9628 - val_specificity: 0.9628 - lr: 5.0000e-07\n","Epoch 53/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1051 - accuracy: 0.9645 - f1: 0.9645 - sensitivity: 0.9645 - specificity: 0.9645\n","Epoch 53: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1051 - accuracy: 0.9645 - f1: 0.9645 - sensitivity: 0.9645 - specificity: 0.9645 - val_loss: 0.1094 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 5.0000e-07\n","Epoch 54/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1058 - accuracy: 0.9646 - f1: 0.9646 - sensitivity: 0.9646 - specificity: 0.9646\n","Epoch 54: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1059 - accuracy: 0.9646 - f1: 0.9646 - sensitivity: 0.9646 - specificity: 0.9646 - val_loss: 0.1093 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 5.0000e-07\n","Epoch 55/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1048 - accuracy: 0.9652 - f1: 0.9653 - sensitivity: 0.9653 - specificity: 0.9653\n","Epoch 55: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 50ms/step - loss: 0.1048 - accuracy: 0.9652 - f1: 0.9652 - sensitivity: 0.9652 - specificity: 0.9652 - val_loss: 0.1096 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 5.0000e-07\n","Epoch 56/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1064 - accuracy: 0.9642 - f1: 0.9642 - sensitivity: 0.9642 - specificity: 0.9642\n","Epoch 56: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 50ms/step - loss: 0.1064 - accuracy: 0.9642 - f1: 0.9642 - sensitivity: 0.9642 - specificity: 0.9642 - val_loss: 0.1090 - val_accuracy: 0.9626 - val_f1: 0.9628 - val_sensitivity: 0.9628 - val_specificity: 0.9628 - lr: 5.0000e-07\n","Epoch 57/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1053 - accuracy: 0.9649 - f1: 0.9649 - sensitivity: 0.9649 - specificity: 0.9649\n","Epoch 57: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1052 - accuracy: 0.9649 - f1: 0.9649 - sensitivity: 0.9649 - specificity: 0.9649 - val_loss: 0.1098 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 5.0000e-07\n","Epoch 58/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1038 - accuracy: 0.9656 - f1: 0.9656 - sensitivity: 0.9656 - specificity: 0.9656\n","Epoch 58: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 50ms/step - loss: 0.1038 - accuracy: 0.9655 - f1: 0.9655 - sensitivity: 0.9655 - specificity: 0.9655 - val_loss: 0.1090 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 5.0000e-07\n","Epoch 59/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1036 - accuracy: 0.9648 - f1: 0.9648 - sensitivity: 0.9648 - specificity: 0.9648\n","Epoch 59: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 50ms/step - loss: 0.1035 - accuracy: 0.9648 - f1: 0.9648 - sensitivity: 0.9648 - specificity: 0.9648 - val_loss: 0.1093 - val_accuracy: 0.9626 - val_f1: 0.9628 - val_sensitivity: 0.9628 - val_specificity: 0.9628 - lr: 5.0000e-07\n","Epoch 60/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9659 - f1: 0.9659 - sensitivity: 0.9659 - specificity: 0.9659\n","Epoch 60: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1037 - accuracy: 0.9659 - f1: 0.9659 - sensitivity: 0.9659 - specificity: 0.9659 - val_loss: 0.1094 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 5.0000e-07\n","Epoch 61/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9648 - f1: 0.9648 - sensitivity: 0.9648 - specificity: 0.9648\n","Epoch 61: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1059 - accuracy: 0.9648 - f1: 0.9648 - sensitivity: 0.9648 - specificity: 0.9648 - val_loss: 0.1099 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 5.0000e-07\n","Epoch 62/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9652 - f1: 0.9652 - sensitivity: 0.9652 - specificity: 0.9652\n","Epoch 62: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1046 - accuracy: 0.9652 - f1: 0.9652 - sensitivity: 0.9652 - specificity: 0.9652 - val_loss: 0.1092 - val_accuracy: 0.9624 - val_f1: 0.9626 - val_sensitivity: 0.9626 - val_specificity: 0.9626 - lr: 5.0000e-07\n","Epoch 63/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1056 - accuracy: 0.9649 - f1: 0.9649 - sensitivity: 0.9649 - specificity: 0.9649\n","Epoch 63: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1056 - accuracy: 0.9649 - f1: 0.9649 - sensitivity: 0.9649 - specificity: 0.9649 - val_loss: 0.1093 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 5.0000e-07\n","Epoch 64/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9642 - f1: 0.9642 - sensitivity: 0.9642 - specificity: 0.9642\n","Epoch 64: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1057 - accuracy: 0.9642 - f1: 0.9642 - sensitivity: 0.9642 - specificity: 0.9642 - val_loss: 0.1093 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 65/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9639 - f1: 0.9639 - sensitivity: 0.9639 - specificity: 0.9639\n","Epoch 65: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 70s 51ms/step - loss: 0.1065 - accuracy: 0.9639 - f1: 0.9639 - sensitivity: 0.9639 - specificity: 0.9639 - val_loss: 0.1093 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 5.0000e-07\n","Epoch 66/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9650 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651\n","Epoch 66: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1049 - accuracy: 0.9650 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651 - val_loss: 0.1091 - val_accuracy: 0.9626 - val_f1: 0.9628 - val_sensitivity: 0.9628 - val_specificity: 0.9628 - lr: 5.0000e-07\n","Epoch 67/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9645 - f1: 0.9645 - sensitivity: 0.9645 - specificity: 0.9645\n","Epoch 67: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1059 - accuracy: 0.9645 - f1: 0.9645 - sensitivity: 0.9645 - specificity: 0.9645 - val_loss: 0.1094 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 5.0000e-07\n","Epoch 68/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1069 - accuracy: 0.9640 - f1: 0.9640 - sensitivity: 0.9640 - specificity: 0.9640\n","Epoch 68: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1069 - accuracy: 0.9640 - f1: 0.9640 - sensitivity: 0.9640 - specificity: 0.9640 - val_loss: 0.1096 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 69/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9645 - f1: 0.9646 - sensitivity: 0.9646 - specificity: 0.9646\n","Epoch 69: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1058 - accuracy: 0.9645 - f1: 0.9646 - sensitivity: 0.9646 - specificity: 0.9646 - val_loss: 0.1089 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 5.0000e-07\n","Epoch 70/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1067 - accuracy: 0.9647 - f1: 0.9647 - sensitivity: 0.9647 - specificity: 0.9647\n","Epoch 70: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 50ms/step - loss: 0.1067 - accuracy: 0.9647 - f1: 0.9647 - sensitivity: 0.9647 - specificity: 0.9647 - val_loss: 0.1095 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 71/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1060 - accuracy: 0.9648 - f1: 0.9648 - sensitivity: 0.9648 - specificity: 0.9648\n","Epoch 71: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1060 - accuracy: 0.9648 - f1: 0.9648 - sensitivity: 0.9648 - specificity: 0.9648 - val_loss: 0.1090 - val_accuracy: 0.9617 - val_f1: 0.9619 - val_sensitivity: 0.9619 - val_specificity: 0.9619 - lr: 5.0000e-07\n","Epoch 72/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1049 - accuracy: 0.9638 - f1: 0.9638 - sensitivity: 0.9638 - specificity: 0.9638\n","Epoch 72: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1048 - accuracy: 0.9638 - f1: 0.9638 - sensitivity: 0.9638 - specificity: 0.9638 - val_loss: 0.1093 - val_accuracy: 0.9617 - val_f1: 0.9619 - val_sensitivity: 0.9619 - val_specificity: 0.9619 - lr: 5.0000e-07\n","Epoch 73/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9632 - f1: 0.9632 - sensitivity: 0.9632 - specificity: 0.9632\n","Epoch 73: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1054 - accuracy: 0.9632 - f1: 0.9632 - sensitivity: 0.9632 - specificity: 0.9632 - val_loss: 0.1096 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 5.0000e-07\n","Epoch 74/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9654 - f1: 0.9654 - sensitivity: 0.9654 - specificity: 0.9654\n","Epoch 74: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 50ms/step - loss: 0.1058 - accuracy: 0.9654 - f1: 0.9654 - sensitivity: 0.9654 - specificity: 0.9654 - val_loss: 0.1092 - val_accuracy: 0.9617 - val_f1: 0.9619 - val_sensitivity: 0.9619 - val_specificity: 0.9619 - lr: 5.0000e-07\n","Epoch 75/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1050 - accuracy: 0.9653 - f1: 0.9653 - sensitivity: 0.9653 - specificity: 0.9653\n","Epoch 75: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 67s 48ms/step - loss: 0.1050 - accuracy: 0.9653 - f1: 0.9653 - sensitivity: 0.9653 - specificity: 0.9653 - val_loss: 0.1093 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 5.0000e-07\n","Epoch 76/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9651 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651\n","Epoch 76: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1044 - accuracy: 0.9651 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651 - val_loss: 0.1091 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 5.0000e-07\n","Epoch 77/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9639 - f1: 0.9639 - sensitivity: 0.9639 - specificity: 0.9639\n","Epoch 77: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 66s 48ms/step - loss: 0.1042 - accuracy: 0.9639 - f1: 0.9639 - sensitivity: 0.9639 - specificity: 0.9639 - val_loss: 0.1092 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 78/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1019 - accuracy: 0.9656 - f1: 0.9656 - sensitivity: 0.9656 - specificity: 0.9656\n","Epoch 78: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 65s 47ms/step - loss: 0.1019 - accuracy: 0.9656 - f1: 0.9656 - sensitivity: 0.9656 - specificity: 0.9656 - val_loss: 0.1089 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 79/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1032 - accuracy: 0.9649 - f1: 0.9649 - sensitivity: 0.9649 - specificity: 0.9649\n","Epoch 79: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 67s 49ms/step - loss: 0.1031 - accuracy: 0.9650 - f1: 0.9650 - sensitivity: 0.9650 - specificity: 0.9650 - val_loss: 0.1092 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 5.0000e-07\n","Epoch 80/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9650 - f1: 0.9650 - sensitivity: 0.9650 - specificity: 0.9650\n","Epoch 80: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 70s 51ms/step - loss: 0.1049 - accuracy: 0.9650 - f1: 0.9650 - sensitivity: 0.9650 - specificity: 0.9650 - val_loss: 0.1095 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 5.0000e-07\n","Epoch 81/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9654 - f1: 0.9654 - sensitivity: 0.9654 - specificity: 0.9654\n","Epoch 81: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 71s 51ms/step - loss: 0.1026 - accuracy: 0.9654 - f1: 0.9654 - sensitivity: 0.9654 - specificity: 0.9654 - val_loss: 0.1090 - val_accuracy: 0.9623 - val_f1: 0.9624 - val_sensitivity: 0.9624 - val_specificity: 0.9624 - lr: 5.0000e-07\n","Epoch 82/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1060 - accuracy: 0.9655 - f1: 0.9656 - sensitivity: 0.9656 - specificity: 0.9656\n","Epoch 82: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 71s 51ms/step - loss: 0.1060 - accuracy: 0.9655 - f1: 0.9656 - sensitivity: 0.9656 - specificity: 0.9656 - val_loss: 0.1092 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 83/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9640 - f1: 0.9641 - sensitivity: 0.9641 - specificity: 0.9641\n","Epoch 83: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1049 - accuracy: 0.9640 - f1: 0.9641 - sensitivity: 0.9641 - specificity: 0.9641 - val_loss: 0.1094 - val_accuracy: 0.9617 - val_f1: 0.9619 - val_sensitivity: 0.9619 - val_specificity: 0.9619 - lr: 5.0000e-07\n","Epoch 84/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1041 - accuracy: 0.9651 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651\n","Epoch 84: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1041 - accuracy: 0.9651 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651 - val_loss: 0.1089 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 85/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9646 - f1: 0.9646 - sensitivity: 0.9646 - specificity: 0.9646\n","Epoch 85: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1044 - accuracy: 0.9645 - f1: 0.9645 - sensitivity: 0.9645 - specificity: 0.9645 - val_loss: 0.1091 - val_accuracy: 0.9615 - val_f1: 0.9617 - val_sensitivity: 0.9617 - val_specificity: 0.9617 - lr: 5.0000e-07\n","Epoch 86/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1052 - accuracy: 0.9648 - f1: 0.9648 - sensitivity: 0.9648 - specificity: 0.9648\n","Epoch 86: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1052 - accuracy: 0.9648 - f1: 0.9648 - sensitivity: 0.9648 - specificity: 0.9648 - val_loss: 0.1094 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 87/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9659 - f1: 0.9659 - sensitivity: 0.9659 - specificity: 0.9659\n","Epoch 87: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1030 - accuracy: 0.9659 - f1: 0.9659 - sensitivity: 0.9659 - specificity: 0.9659 - val_loss: 0.1091 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 88/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1067 - accuracy: 0.9650 - f1: 0.9650 - sensitivity: 0.9650 - specificity: 0.9650\n","Epoch 88: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 66s 48ms/step - loss: 0.1067 - accuracy: 0.9650 - f1: 0.9650 - sensitivity: 0.9650 - specificity: 0.9650 - val_loss: 0.1092 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 89/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9660 - f1: 0.9660 - sensitivity: 0.9660 - specificity: 0.9660\n","Epoch 89: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1037 - accuracy: 0.9660 - f1: 0.9660 - sensitivity: 0.9660 - specificity: 0.9660 - val_loss: 0.1090 - val_accuracy: 0.9614 - val_f1: 0.9615 - val_sensitivity: 0.9615 - val_specificity: 0.9615 - lr: 5.0000e-07\n","Epoch 90/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1063 - accuracy: 0.9632 - f1: 0.9632 - sensitivity: 0.9632 - specificity: 0.9632\n","Epoch 90: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 75s 54ms/step - loss: 0.1063 - accuracy: 0.9632 - f1: 0.9632 - sensitivity: 0.9632 - specificity: 0.9632 - val_loss: 0.1087 - val_accuracy: 0.9617 - val_f1: 0.9619 - val_sensitivity: 0.9619 - val_specificity: 0.9619 - lr: 5.0000e-07\n","Epoch 91/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1041 - accuracy: 0.9647 - f1: 0.9647 - sensitivity: 0.9647 - specificity: 0.9647\n","Epoch 91: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 76s 55ms/step - loss: 0.1041 - accuracy: 0.9647 - f1: 0.9647 - sensitivity: 0.9647 - specificity: 0.9647 - val_loss: 0.1089 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 92/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9655 - f1: 0.9655 - sensitivity: 0.9655 - specificity: 0.9655\n","Epoch 92: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 74s 54ms/step - loss: 0.1046 - accuracy: 0.9655 - f1: 0.9655 - sensitivity: 0.9655 - specificity: 0.9655 - val_loss: 0.1088 - val_accuracy: 0.9617 - val_f1: 0.9619 - val_sensitivity: 0.9619 - val_specificity: 0.9619 - lr: 5.0000e-07\n","Epoch 93/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.9641 - f1: 0.9641 - sensitivity: 0.9641 - specificity: 0.9641\n","Epoch 93: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 75s 55ms/step - loss: 0.1048 - accuracy: 0.9641 - f1: 0.9641 - sensitivity: 0.9641 - specificity: 0.9641 - val_loss: 0.1088 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 94/100\n","1376/1377 [============================>.] - ETA: 0s - loss: 0.1047 - accuracy: 0.9640 - f1: 0.9640 - sensitivity: 0.9640 - specificity: 0.9640\n","Epoch 94: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1046 - accuracy: 0.9640 - f1: 0.9640 - sensitivity: 0.9640 - specificity: 0.9640 - val_loss: 0.1090 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 95/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9651 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651\n","Epoch 95: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 49ms/step - loss: 0.1051 - accuracy: 0.9651 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651 - val_loss: 0.1089 - val_accuracy: 0.9615 - val_f1: 0.9617 - val_sensitivity: 0.9617 - val_specificity: 0.9617 - lr: 5.0000e-07\n","Epoch 96/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1053 - accuracy: 0.9645 - f1: 0.9645 - sensitivity: 0.9645 - specificity: 0.9645\n","Epoch 96: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 68s 50ms/step - loss: 0.1053 - accuracy: 0.9645 - f1: 0.9645 - sensitivity: 0.9645 - specificity: 0.9645 - val_loss: 0.1090 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 5.0000e-07\n","Epoch 97/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9641 - f1: 0.9641 - sensitivity: 0.9641 - specificity: 0.9641\n","Epoch 97: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 71s 51ms/step - loss: 0.1054 - accuracy: 0.9641 - f1: 0.9641 - sensitivity: 0.9641 - specificity: 0.9641 - val_loss: 0.1096 - val_accuracy: 0.9619 - val_f1: 0.9621 - val_sensitivity: 0.9621 - val_specificity: 0.9621 - lr: 5.0000e-07\n","Epoch 98/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9650 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651\n","Epoch 98: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 71s 52ms/step - loss: 0.1018 - accuracy: 0.9650 - f1: 0.9651 - sensitivity: 0.9651 - specificity: 0.9651 - val_loss: 0.1092 - val_accuracy: 0.9624 - val_f1: 0.9626 - val_sensitivity: 0.9626 - val_specificity: 0.9626 - lr: 5.0000e-07\n","Epoch 99/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9644 - f1: 0.9644 - sensitivity: 0.9644 - specificity: 0.9644\n","Epoch 99: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 69s 50ms/step - loss: 0.1059 - accuracy: 0.9644 - f1: 0.9644 - sensitivity: 0.9644 - specificity: 0.9644 - val_loss: 0.1089 - val_accuracy: 0.9621 - val_f1: 0.9622 - val_sensitivity: 0.9622 - val_specificity: 0.9622 - lr: 5.0000e-07\n","Epoch 100/100\n","1377/1377 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9645 - f1: 0.9645 - sensitivity: 0.9645 - specificity: 0.9645\n","Epoch 100: val_loss did not improve from 0.10850\n","1377/1377 [==============================] - 74s 54ms/step - loss: 0.1037 - accuracy: 0.9645 - f1: 0.9645 - sensitivity: 0.9645 - specificity: 0.9645 - val_loss: 0.1090 - val_accuracy: 0.9617 - val_f1: 0.9619 - val_sensitivity: 0.9619 - val_specificity: 0.9619 - lr: 5.0000e-07\n"]}],"source":["import time\n","start = time.time()\n","\n","if data_augmentation:\n","    print(\"-------------Using Data augmentation------------\")\n","    # This will do preprocessing and realtime data augmentation:\n","    datagen = ImageDataGenerator(\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        horizontal_flip=True,\n","        vertical_flip=True)  # randomly flip images\n","\n","    datagen.fit(x_train)\n","    history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n","                        steps_per_epoch=x_train.shape[0] // batch_size,\n","                        epochs=epochs, verbose=1, validation_data=(x_test, y_test),\n","                        callbacks=[lr_reducer,  csv_logger, model_chekpoint])\n","\n","else:\n","    print(\"-----Not Using Data augmentation---------------\")\n","    history = model.fit(x_train, y_train,\n","              batch_size=batch_size * 4,\n","              epochs=epochs,\n","              validation_data=(x_test, y_test),\n","              shuffle=True, callbacks=[lr_reducer,  csv_logger, model_chekpoint])\n"]},{"cell_type":"markdown","metadata":{"id":"AcNrHJTYNZVl"},"source":["# Results"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2622,"status":"ok","timestamp":1683890986321,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"knp-ULKtHxS3","outputId":"ddc2d2f4-f8c2-4c50-bc41-ec12f430b80c"},"outputs":[{"output_type":"stream","name":"stdout","text":["------------Training time is seconds:%s 7534.214286804199\n","173/173 [==============================] - 2s 12ms/step - loss: 0.1090 - accuracy: 0.9617 - f1: 0.9619 - sensitivity: 0.9619 - specificity: 0.9619\n","dict_keys(['loss', 'accuracy', 'f1', 'sensitivity', 'specificity', 'val_loss', 'val_accuracy', 'val_f1', 'val_sensitivity', 'val_specificity', 'lr'])\n","dict_values([[0.7253891229629517, 0.6833365559577942, 0.659715473651886, 0.4893648326396942, 0.20984157919883728, 0.20319215953350067, 0.18558508157730103, 0.1810041218996048, 0.17159464955329895, 0.16893556714057922, 0.17329156398773193, 0.14545395970344543, 0.1367153525352478, 0.1352968066930771, 0.13170476257801056, 0.13205905258655548, 0.1305571347475052, 0.12289437651634216, 0.11856039613485336, 0.11796209961175919, 0.11599606275558472, 0.11704840511083603, 0.1175173968076706, 0.11375691741704941, 0.11084169149398804, 0.1117527186870575, 0.11126287281513214, 0.11150683462619781, 0.10813106596469879, 0.10860505700111389, 0.10819799453020096, 0.11086031049489975, 0.10499279201030731, 0.10861910879611969, 0.10695326328277588, 0.10637203603982925, 0.10566546022891998, 0.10559061914682388, 0.10267522931098938, 0.10690221190452576, 0.10486437380313873, 0.1048903837800026, 0.1046796664595604, 0.10455475747585297, 0.10692167282104492, 0.10420820862054825, 0.10466105490922928, 0.10656934231519699, 0.10464078933000565, 0.1054961234331131, 0.10571876168251038, 0.10515151172876358, 0.1050868034362793, 0.10587936639785767, 0.10481913387775421, 0.10641355067491531, 0.1052306517958641, 0.10379170626401901, 0.10351096093654633, 0.10368742048740387, 0.1059330627322197, 0.10455343872308731, 0.10561978071928024, 0.1057329848408699, 0.10649099200963974, 0.10486819595098495, 0.10586969554424286, 0.10685759037733078, 0.10580885410308838, 0.10665630549192429, 0.10603363066911697, 0.10480033606290817, 0.10535874962806702, 0.1057688444852829, 0.10496421158313751, 0.10436250269412994, 0.10419470816850662, 0.10186629742383957, 0.10312338173389435, 0.10493353754281998, 0.1026240885257721, 0.1060473695397377, 0.10492347180843353, 0.1040508970618248, 0.10442359000444412, 0.10519962012767792, 0.10300192981958389, 0.10668251663446426, 0.10369130223989487, 0.10630229860544205, 0.10414516180753708, 0.10457396507263184, 0.10478188842535019, 0.10461556911468506, 0.10512393712997437, 0.10527253150939941, 0.10544176399707794, 0.10178278386592865, 0.10587277263402939, 0.10374557971954346], [0.6474353075027466, 0.6598275303840637, 0.6720381379127502, 0.780753493309021, 0.9314116835594177, 0.9373581409454346, 0.9440308809280396, 0.946209728717804, 0.9471175670623779, 0.9482523798942566, 0.9475715160369873, 0.953835666179657, 0.9568769931793213, 0.9572855234146118, 0.9578302502632141, 0.9587380886077881, 0.9590104222297668, 0.9601452350616455, 0.960553765296936, 0.9620063304901123, 0.9627780318260193, 0.9619155526161194, 0.9617339968681335, 0.9619155526161194, 0.9633227586746216, 0.9635043144226074, 0.9643213748931885, 0.9630957841873169, 0.9650476574897766, 0.9635497331619263, 0.9635497331619263, 0.9628234505653381, 0.9646845459938049, 0.9640944004058838, 0.9637766480445862, 0.9645937085151672, 0.9637312889099121, 0.9639582633972168, 0.9653199911117554, 0.9645937085151672, 0.9652746319770813, 0.9649568796157837, 0.9648661017417908, 0.9641851782798767, 0.9634135365486145, 0.9652746319770813, 0.9650930762290955, 0.9648206830024719, 0.9650476574897766, 0.9645029306411743, 0.9641398191452026, 0.9643667936325073, 0.9645029306411743, 0.9645937085151672, 0.9652292132377625, 0.9641851782798767, 0.9649114608764648, 0.9655469655990601, 0.9648206830024719, 0.9659101366996765, 0.9648206830024719, 0.9652292132377625, 0.9649114608764648, 0.9642305970191956, 0.963912844657898, 0.9650476574897766, 0.9645029306411743, 0.9640490412712097, 0.9645483493804932, 0.9646845459938049, 0.9648206830024719, 0.963822066783905, 0.9632319808006287, 0.9653654098510742, 0.9652746319770813, 0.9650930762290955, 0.963912844657898, 0.965637743473053, 0.9649568796157837, 0.9649568796157837, 0.9653654098510742, 0.9655469655990601, 0.9640490412712097, 0.9651384353637695, 0.9645483493804932, 0.9648206830024719, 0.9658647179603577, 0.9650022983551025, 0.9659554958343506, 0.9631865620613098, 0.9646845459938049, 0.9654561877250671, 0.9640944004058838, 0.9640490412712097, 0.9651384353637695, 0.9645029306411743, 0.9640944004058838, 0.9650476574897766, 0.9643667936325073, 0.9645029306411743], [0.6474413275718689, 0.6598389148712158, 0.6720094680786133, 0.7807604074478149, 0.9314179420471191, 0.937363862991333, 0.9440359473228455, 0.9462016820907593, 0.9471223950386047, 0.9482505917549133, 0.9475632309913635, 0.953839898109436, 0.9568809270858765, 0.9572893977165222, 0.9578340649604797, 0.9587418437004089, 0.9590141773223877, 0.9601423740386963, 0.9605573415756226, 0.9620097875595093, 0.9627814292907715, 0.9619124531745911, 0.9617374539375305, 0.9619190096855164, 0.963326096534729, 0.9635076522827148, 0.9643245935440063, 0.9630926847457886, 0.9650508165359497, 0.9635530114173889, 0.9635530114173889, 0.9628267884254456, 0.964687705039978, 0.9640976786613464, 0.9637799859046936, 0.9645969271659851, 0.9637345671653748, 0.9639550447463989, 0.9653231501579285, 0.9645969271659851, 0.9652777910232544, 0.9649600386619568, 0.9648692607879639, 0.9641884565353394, 0.9634103775024414, 0.9652712941169739, 0.9650896787643433, 0.9648239016532898, 0.9650508165359497, 0.9645061492919922, 0.9641364812850952, 0.9643635153770447, 0.9645061492919922, 0.9645969271659851, 0.9652323722839355, 0.9641884565353394, 0.9649081826210022, 0.9655436277389526, 0.9648239016532898, 0.9659067392349243, 0.9648239016532898, 0.9652259349822998, 0.9649146795272827, 0.9642338156700134, 0.9639096260070801, 0.9650508165359497, 0.9644997119903564, 0.9640458226203918, 0.964551568031311, 0.9646747708320618, 0.9648239016532898, 0.9638188481330872, 0.9632222652435303, 0.9653620719909668, 0.9652712941169739, 0.9650962352752686, 0.9639096260070801, 0.9656409025192261, 0.9649600386619568, 0.964953601360321, 0.9653685688972473, 0.9655501246452332, 0.9640522599220276, 0.9651351571083069, 0.9645450711250305, 0.9648239016532898, 0.9658613801002502, 0.9649989604949951, 0.9659585952758789, 0.963176965713501, 0.9646747708320618, 0.9654528498649597, 0.9640847444534302, 0.9640458226203918, 0.9651415944099426, 0.9645061492919922, 0.9640911817550659, 0.9650508165359497, 0.9643635153770447, 0.9645061492919922], [0.6474413871765137, 0.6598389148712158, 0.6720095276832581, 0.7807604670524597, 0.9314179420471191, 0.937363862991333, 0.9440359473228455, 0.9462016820907593, 0.9471223950386047, 0.9482505917549133, 0.9475632905960083, 0.953839898109436, 0.9568809270858765, 0.9572893977165222, 0.9578340649604797, 0.9587418437004089, 0.9590141773223877, 0.9601423740386963, 0.9605573415756226, 0.9620097875595093, 0.9627814292907715, 0.9619125723838806, 0.9617374539375305, 0.9619190096855164, 0.963326096534729, 0.9635076522827148, 0.9643245935440063, 0.9630926847457886, 0.9650508165359497, 0.9635530114173889, 0.9635530114173889, 0.9628267884254456, 0.964687705039978, 0.9640976786613464, 0.9637799859046936, 0.9645969271659851, 0.9637345671653748, 0.9639550447463989, 0.9653231501579285, 0.9645969271659851, 0.9652777910232544, 0.9649600386619568, 0.9648692607879639, 0.9641884565353394, 0.9634103775024414, 0.9652712941169739, 0.965089738368988, 0.9648239016532898, 0.9650508165359497, 0.9645061492919922, 0.9641366004943848, 0.9643635153770447, 0.9645061492919922, 0.9645969271659851, 0.9652323722839355, 0.9641884565353394, 0.9649081826210022, 0.9655436277389526, 0.9648239016532898, 0.9659067392349243, 0.9648239016532898, 0.9652259349822998, 0.9649146795272827, 0.9642338156700134, 0.9639096260070801, 0.9650508165359497, 0.9644997119903564, 0.9640458226203918, 0.964551568031311, 0.9646747708320618, 0.9648239016532898, 0.9638188481330872, 0.963222324848175, 0.9653620719909668, 0.9652712941169739, 0.9650962352752686, 0.9639096260070801, 0.9656409025192261, 0.9649600386619568, 0.964953601360321, 0.9653685688972473, 0.9655501246452332, 0.9640522599220276, 0.9651351571083069, 0.9645450711250305, 0.9648239016532898, 0.9658613801002502, 0.9649989604949951, 0.9659585952758789, 0.963176965713501, 0.9646747708320618, 0.9654528498649597, 0.9640847444534302, 0.9640458226203918, 0.9651415944099426, 0.9645061492919922, 0.9640911817550659, 0.9650508165359497, 0.9643635153770447, 0.9645061492919922], [0.6474413871765137, 0.6598389148712158, 0.6720095276832581, 0.7807604670524597, 0.9314179420471191, 0.937363862991333, 0.9440359473228455, 0.9462016820907593, 0.9471223950386047, 0.9482505917549133, 0.9475632905960083, 0.953839898109436, 0.9568809270858765, 0.9572893977165222, 0.9578340649604797, 0.9587418437004089, 0.9590141773223877, 0.9601423740386963, 0.9605573415756226, 0.9620097875595093, 0.9627814292907715, 0.9619125723838806, 0.9617374539375305, 0.9619190096855164, 0.963326096534729, 0.9635076522827148, 0.9643245935440063, 0.9630926847457886, 0.9650508165359497, 0.9635530114173889, 0.9635530114173889, 0.9628267884254456, 0.964687705039978, 0.9640976786613464, 0.9637799859046936, 0.9645969271659851, 0.9637345671653748, 0.9639550447463989, 0.9653231501579285, 0.9645969271659851, 0.9652777910232544, 0.9649600386619568, 0.9648692607879639, 0.9641884565353394, 0.9634103775024414, 0.9652712941169739, 0.965089738368988, 0.9648239016532898, 0.9650508165359497, 0.9645061492919922, 0.9641366004943848, 0.9643635153770447, 0.9645061492919922, 0.9645969271659851, 0.9652323722839355, 0.9641884565353394, 0.9649081826210022, 0.9655436277389526, 0.9648239016532898, 0.9659067392349243, 0.9648239016532898, 0.9652259349822998, 0.9649146795272827, 0.9642338156700134, 0.9639096260070801, 0.9650508165359497, 0.9644997119903564, 0.9640458226203918, 0.964551568031311, 0.9646747708320618, 0.9648239016532898, 0.9638188481330872, 0.963222324848175, 0.9653620719909668, 0.9652712941169739, 0.9650962352752686, 0.9639096260070801, 0.9656409025192261, 0.9649600386619568, 0.964953601360321, 0.9653685688972473, 0.9655501246452332, 0.9640522599220276, 0.9651351571083069, 0.9645450711250305, 0.9648239016532898, 0.9658613801002502, 0.9649989604949951, 0.9659585952758789, 0.963176965713501, 0.9646747708320618, 0.9654528498649597, 0.9640847444534302, 0.9640458226203918, 0.9651415944099426, 0.9645061492919922, 0.9640911817550659, 0.9650508165359497, 0.9643635153770447, 0.9645061492919922], [1.2138843536376953, 1.3865408897399902, 1.185877799987793, 0.21618366241455078, 0.20269693434238434, 0.21234722435474396, 0.17426180839538574, 0.1860332041978836, 0.16040946543216705, 0.18840184807777405, 0.2872231602668762, 0.14554210007190704, 0.1394100934267044, 0.13800962269306183, 0.1251922845840454, 0.13132689893245697, 0.13858799636363983, 0.11940619349479675, 0.12749722599983215, 0.11709831655025482, 0.11720839142799377, 0.11934541165828705, 0.11589443683624268, 0.11533007025718689, 0.1139710396528244, 0.11399012804031372, 0.1127963662147522, 0.11182110011577606, 0.11250562965869904, 0.11018603295087814, 0.11049927026033401, 0.11091295629739761, 0.11093663424253464, 0.10850460827350616, 0.10896845161914825, 0.10954068601131439, 0.10937944054603577, 0.10884295403957367, 0.10909157991409302, 0.10951928049325943, 0.10892029106616974, 0.10933846235275269, 0.10905898362398148, 0.10914218425750732, 0.10894013196229935, 0.10896413028240204, 0.10923542082309723, 0.10988527536392212, 0.10940729081630707, 0.1094408631324768, 0.10944706201553345, 0.10887963324785233, 0.10940155386924744, 0.10931476205587387, 0.10961029678583145, 0.10899052023887634, 0.10978174209594727, 0.10900530964136124, 0.10930107533931732, 0.10936853289604187, 0.10991031676530838, 0.10918604582548141, 0.109293133020401, 0.1093340739607811, 0.10925538837909698, 0.1091146394610405, 0.10938330739736557, 0.10955113917589188, 0.10887200385332108, 0.10948871821165085, 0.10904593765735626, 0.10930430144071579, 0.10958097130060196, 0.1092434972524643, 0.10926664620637894, 0.10911956429481506, 0.10924636572599411, 0.10885054618120193, 0.10920406132936478, 0.10946151614189148, 0.1090097725391388, 0.10918877273797989, 0.10944025218486786, 0.10894209891557693, 0.10909068584442139, 0.10940442234277725, 0.1090867668390274, 0.10915227234363556, 0.10896201431751251, 0.10869216173887253, 0.108904168009758, 0.10881076753139496, 0.1087697446346283, 0.1090482622385025, 0.10892970114946365, 0.10895922780036926, 0.10955316573381424, 0.10917164385318756, 0.10886827856302261, 0.10903620719909668], [0.5538824200630188, 0.6115747690200806, 0.6380624175071716, 0.9332365989685059, 0.9502902626991272, 0.927431046962738, 0.949201762676239, 0.9488388895988464, 0.9511973857879639, 0.9524673223495483, 0.9218069911003113, 0.9575471878051758, 0.9568215012550354, 0.956095814704895, 0.9604499340057373, 0.9571843147277832, 0.9579100012779236, 0.9593613743782043, 0.9591799974441528, 0.9595428109169006, 0.9600870609283447, 0.961357057094574, 0.9615384340286255, 0.9624455571174622, 0.9617198705673218, 0.9622641801834106, 0.9637155532836914, 0.9619013071060181, 0.9626269936561584, 0.9622641801834106, 0.961357057094574, 0.961357057094574, 0.9611756205558777, 0.9620827436447144, 0.9619013071060181, 0.9624455571174622, 0.9620827436447144, 0.9622641801834106, 0.9626269936561584, 0.9622641801834106, 0.9622641801834106, 0.9626269936561584, 0.9626269936561584, 0.9626269936561584, 0.9617198705673218, 0.9620827436447144, 0.9622641801834106, 0.9624455571174622, 0.9624455571174622, 0.9622641801834106, 0.9622641801834106, 0.9626269936561584, 0.9620827436447144, 0.9622641801834106, 0.9622641801834106, 0.9626269936561584, 0.9622641801834106, 0.9622641801834106, 0.9626269936561584, 0.9620827436447144, 0.9622641801834106, 0.9624455571174622, 0.9622641801834106, 0.9619013071060181, 0.9620827436447144, 0.9626269936561584, 0.9622641801834106, 0.9619013071060181, 0.9620827436447144, 0.9619013071060181, 0.9617198705673218, 0.9617198705673218, 0.9620827436447144, 0.9617198705673218, 0.9620827436447144, 0.9620827436447144, 0.9619013071060181, 0.9619013071060181, 0.9620827436447144, 0.9620827436447144, 0.9622641801834106, 0.9619013071060181, 0.9617198705673218, 0.9619013071060181, 0.9615384340286255, 0.9619013071060181, 0.9619013071060181, 0.9619013071060181, 0.961357057094574, 0.9617198705673218, 0.9619013071060181, 0.9617198705673218, 0.9619013071060181, 0.9619013071060181, 0.9615384340286255, 0.9620827436447144, 0.9619013071060181, 0.9624455571174622, 0.9620827436447144, 0.9617198705673218], [0.5531069040298462, 0.6110910177230835, 0.6380057334899902, 0.9335260391235352, 0.950505793094635, 0.9277456402778625, 0.9494219422340393, 0.9485188126564026, 0.9508670568466187, 0.9521315097808838, 0.9216039776802063, 0.9571893215179443, 0.9564667344093323, 0.9562861323356628, 0.9606214165687561, 0.9568280577659607, 0.9580925107002258, 0.9589956402778625, 0.9588150382041931, 0.9597182273864746, 0.9602600932121277, 0.9615245461463928, 0.961705207824707, 0.9626083970069885, 0.9618858098983765, 0.9624277353286743, 0.9638728499412537, 0.9620664715766907, 0.962788999080658, 0.9624277353286743, 0.9615245461463928, 0.9615245461463928, 0.9613439440727234, 0.9622471332550049, 0.9620664715766907, 0.9626083970069885, 0.9622471332550049, 0.9624277353286743, 0.962788999080658, 0.9624277353286743, 0.9624277353286743, 0.962788999080658, 0.962788999080658, 0.962788999080658, 0.9618858098983765, 0.9622471332550049, 0.9624277353286743, 0.9626083970069885, 0.9626083970069885, 0.9624277353286743, 0.9624277353286743, 0.962788999080658, 0.9622471332550049, 0.9624277353286743, 0.9624277353286743, 0.962788999080658, 0.9624277353286743, 0.9624277353286743, 0.962788999080658, 0.9622471332550049, 0.9624277353286743, 0.9626083970069885, 0.9624277353286743, 0.9620664715766907, 0.9622471332550049, 0.962788999080658, 0.9624277353286743, 0.9620664715766907, 0.9622471332550049, 0.9620664715766907, 0.9618858098983765, 0.9618858098983765, 0.9622471332550049, 0.9618858098983765, 0.9622471332550049, 0.9622471332550049, 0.9620664715766907, 0.9620664715766907, 0.9622471332550049, 0.9622471332550049, 0.9624277353286743, 0.9620664715766907, 0.9618858098983765, 0.9620664715766907, 0.961705207824707, 0.9620664715766907, 0.9620664715766907, 0.9620664715766907, 0.9615245461463928, 0.9618858098983765, 0.9620664715766907, 0.9618858098983765, 0.9620664715766907, 0.9620664715766907, 0.961705207824707, 0.9622471332550049, 0.9620664715766907, 0.9626083970069885, 0.9622471332550049, 0.9618858098983765], [0.553106963634491, 0.6110910177230835, 0.638005793094635, 0.9335260391235352, 0.950505793094635, 0.9277456402778625, 0.9494219422340393, 0.9485188126564026, 0.9508670568466187, 0.9521315097808838, 0.9216040372848511, 0.9571893215179443, 0.9564667344093323, 0.9562861323356628, 0.9606214165687561, 0.9568280577659607, 0.9580925107002258, 0.9589956402778625, 0.9588150382041931, 0.9597182273864746, 0.9602600932121277, 0.9615245461463928, 0.961705207824707, 0.9626083970069885, 0.9618858098983765, 0.9624277353286743, 0.9638728499412537, 0.9620664715766907, 0.962788999080658, 0.9624277353286743, 0.9615245461463928, 0.9615245461463928, 0.9613439440727234, 0.9622471332550049, 0.9620664715766907, 0.9626083970069885, 0.9622471332550049, 0.9624277353286743, 0.962788999080658, 0.9624277353286743, 0.9624277353286743, 0.962788999080658, 0.962788999080658, 0.962788999080658, 0.9618858098983765, 0.9622471332550049, 0.9624277353286743, 0.9626083970069885, 0.9626083970069885, 0.9624277353286743, 0.9624277353286743, 0.962788999080658, 0.9622471332550049, 0.9624277353286743, 0.9624277353286743, 0.962788999080658, 0.9624277353286743, 0.9624277353286743, 0.962788999080658, 0.9622471332550049, 0.9624277353286743, 0.9626083970069885, 0.9624277353286743, 0.9620664715766907, 0.9622471332550049, 0.962788999080658, 0.9624277353286743, 0.9620664715766907, 0.9622471332550049, 0.9620664715766907, 0.9618858098983765, 0.9618858098983765, 0.9622471332550049, 0.9618858098983765, 0.9622471332550049, 0.9622471332550049, 0.9620664715766907, 0.9620664715766907, 0.9622471332550049, 0.9622471332550049, 0.9624277353286743, 0.9620664715766907, 0.9618858098983765, 0.9620664715766907, 0.961705207824707, 0.9620664715766907, 0.9620664715766907, 0.9620664715766907, 0.9615245461463928, 0.9618858098983765, 0.9620664715766907, 0.9618858098983765, 0.9620664715766907, 0.9620664715766907, 0.961705207824707, 0.9622471332550049, 0.9620664715766907, 0.9626083970069885, 0.9622471332550049, 0.9618858098983765], [0.553106963634491, 0.6110910177230835, 0.638005793094635, 0.9335260391235352, 0.950505793094635, 0.9277456402778625, 0.9494219422340393, 0.9485188126564026, 0.9508670568466187, 0.9521315097808838, 0.9216040372848511, 0.9571893215179443, 0.9564667344093323, 0.9562861323356628, 0.9606214165687561, 0.9568280577659607, 0.9580925107002258, 0.9589956402778625, 0.9588150382041931, 0.9597182273864746, 0.9602600932121277, 0.9615245461463928, 0.961705207824707, 0.9626083970069885, 0.9618858098983765, 0.9624277353286743, 0.9638728499412537, 0.9620664715766907, 0.962788999080658, 0.9624277353286743, 0.9615245461463928, 0.9615245461463928, 0.9613439440727234, 0.9622471332550049, 0.9620664715766907, 0.9626083970069885, 0.9622471332550049, 0.9624277353286743, 0.962788999080658, 0.9624277353286743, 0.9624277353286743, 0.962788999080658, 0.962788999080658, 0.962788999080658, 0.9618858098983765, 0.9622471332550049, 0.9624277353286743, 0.9626083970069885, 0.9626083970069885, 0.9624277353286743, 0.9624277353286743, 0.962788999080658, 0.9622471332550049, 0.9624277353286743, 0.9624277353286743, 0.962788999080658, 0.9624277353286743, 0.9624277353286743, 0.962788999080658, 0.9622471332550049, 0.9624277353286743, 0.9626083970069885, 0.9624277353286743, 0.9620664715766907, 0.9622471332550049, 0.962788999080658, 0.9624277353286743, 0.9620664715766907, 0.9622471332550049, 0.9620664715766907, 0.9618858098983765, 0.9618858098983765, 0.9622471332550049, 0.9618858098983765, 0.9622471332550049, 0.9622471332550049, 0.9620664715766907, 0.9620664715766907, 0.9622471332550049, 0.9622471332550049, 0.9624277353286743, 0.9620664715766907, 0.9618858098983765, 0.9620664715766907, 0.961705207824707, 0.9620664715766907, 0.9620664715766907, 0.9620664715766907, 0.9615245461463928, 0.9618858098983765, 0.9620664715766907, 0.9618858098983765, 0.9620664715766907, 0.9620664715766907, 0.961705207824707, 0.9622471332550049, 0.9620664715766907, 0.9626083970069885, 0.9622471332550049, 0.9618858098983765], [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0003162278, 0.0003162278, 0.0003162278, 0.0003162278, 0.0003162278, 0.0003162278, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 3.1622778e-05, 3.1622778e-05, 3.1622778e-05, 3.1622778e-05, 3.1622778e-05, 3.1622778e-05, 3.1622778e-05, 3.1622778e-05, 3.1622778e-05, 3.1622778e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 3.1622778e-06, 3.1622778e-06, 1.0000001e-06, 1.0000001e-06, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07]])\n","[0.10903620719909668, 0.9617198705673218, 0.9618858098983765, 0.9618858098983765, 0.9618858098983765]\n","Test loss: 0.10903620719909668\n","Test accuracy: 0.9617198705673218\n","Test f1: 0.9618858098983765\n","Test sensitivity: 0.9618858098983765\n","Test specificity: 0.9618858098983765\n","Max Test accuracy 0.9637155532836914\n"]}],"source":["\n","print('------------Training time is seconds:%s',time.time()-start)\n","scores = model.evaluate(x_test, y_test, verbose=1)\n","\n","#history keys and values\n","print(history.history.keys())\n","print(history.history.values())\n","\n","#Metrics for testing\n","print(scores)\n","print('Test loss:', scores[0])\n","print('Test accuracy:', scores[1])\n","print('Test f1:',scores[2])\n","print('Test sensitivity:',scores[3])\n","print('Test specificity:',scores[4])\n","print(\"Max Test accuracy\", max(history.history['val_accuracy']))"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":1007,"status":"ok","timestamp":1683890992615,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"o8do8qKLOPWk","outputId":"40e2de88-b1ce-4c46-dd36-db581de19caa"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","# summarize history for accuracy\n","\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model Accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.savefig(METRICS_PATH+MODEL_NAME+\"_acc.png\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":1009,"status":"ok","timestamp":1683890996450,"user":{"displayName":"DL Project AIML","userId":"14237088217944868047"},"user_tz":-330},"id":"5DDu3Xc8OPT1","outputId":"9ad0a761-669e-421b-b063-fe739cfcaf43"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# summarize history for loss\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.savefig(METRICS_PATH+MODEL_NAME+\"_loss.png\")\n","plt.show()\n"]},{"cell_type":"markdown","source":["Without Augmentation"],"metadata":{"id":"F0C3Xs_sKt6g"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMVtt6FcZYTs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683350271472,"user_tz":-330,"elapsed":14,"user":{"displayName":"pythoder 179","userId":"04425462557897485101"}},"outputId":"63e9be2c-aa53-4d23-e4bd-541cd4cbf749"},"outputs":[{"output_type":"stream","name":"stdout","text":["PWD /content/drive/MyDrive/Malaria/Malaria_Cell_Dataset\n"]}],"source":["PATH = \"/content/drive/MyDrive/Malaria/Malaria_Cell_Dataset\"\n","METRICS_PATH=\"/content/drive/MyDrive/Malaria/Malaria_Cell_Metrics/\"\n","MODEL_NAME=\"ResNet_wag\"\n","print(\"PWD\", PATH)"]},{"cell_type":"code","source":["def initial_conv(Input, filters, stride=1, kernel_size=7):\n","    x = Conv2D(filters, kernel_size=(kernel_size, kernel_size), strides=(stride, stride), padding=\"same\")(Input)\n","\n","    x = BatchNormalization()(x)\n","\n","    x = Activation('relu')(x)\n","    return x\n","\n","\n","def expand_conv_basic_block(Input, filters, stride=1, dropout=0.0):\n","    Init = Input\n","\n","    # First conv which is used to downsample the image\n","    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(Input)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # Optional Dropout layer\n","    if (dropout > 0.0):\n","        x = Dropout(dropout)(x)\n","\n","    x = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","\n","    # Projection shortcut to make skip connection(Paper terminology)\n","    skip_conv = Conv2D(filters, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n","    skip = BatchNormalization()(skip_conv)\n","\n","    # Skip connection\n","    x = Add()([x, skip])\n","    return x\n","\n","\n","def normal_conv_basic_block(Input, filters, stride=1, dropout=0.0):\n","    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(Input)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # Optional Dropout layer\n","    if (dropout > 0.0):\n","        x = Dropout(dropout)(x)\n","\n","    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","\n","    # Identity skip connection\n","    x = Add()([x, Input])\n","\n","    return x\n","\n","\n","def expand_conv_bottleneck_block(Input, filters, stride=1, dropout=0.0):\n","    # Contracting 1*1 conv\n","    x = Conv2D(filters, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # if(dropout > 0.0):\n","    #   x = Dropout(dropout)(x)\n","\n","    # Depth preserving 3*3 conv\n","    x = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # if(Dropout > 0.0):\n","    #   x = Dropout(dropout)(x)\n","\n","    # Expanding 1*1 Conv\n","    x = Conv2D(filters * 4, kernel_size=(1, 1), strides=(1, 1), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","\n","    # Projection shortcut\n","    skip_conv = Conv2D(filters * 4, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n","    skip = BatchNormalization()(skip_conv)\n","\n","    # Skip connection\n","    x = Add()([x, skip])\n","\n","    return x\n","\n","def normal_conv_bottleneck_block(Input, filters, stride=1, dropout=0.0):\n","    # Contracting 1*1 conv\n","    x = Conv2D(filters, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # if(dropout > 0.0):\n","    #   x = Dropout(dropout)(x)\n","\n","    # Depth preserving 3*3 Conv\n","    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # if(Dropout > 0.0):\n","    #    x = Dropout(dropout)(x)\n","\n","    # Expanding 1*1 Conv\n","    x = Conv2D(filters * 4, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","\n","    # Identity skip connection\n","    x = Add()([x, Input])\n","\n","    return x\n","\n","\n","def build_basic_resnet(h, w, no_of_outputs, r1, r2, r3, r4, first_conv_stride=2, first_max_pool=True,\n","                       first_conv_kernel_size=7):\n","    # Creating input tensor\n","    inputs = Input(shape=(h, w, 3), name=\"image_input\")\n","\n","    # Inital Conv block\n","    x = initial_conv(inputs, 64, first_conv_stride, first_conv_kernel_size)\n","\n","    # Optional Max pooling layer\n","    if (first_max_pool):\n","        x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","    # Expanding block1 with projection shortcut\n","    x = expand_conv_basic_block(x, 64, 1)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv1\n","    for i in range(r1 - 1):\n","        x = normal_conv_basic_block(x, 64)\n","        x = Activation('relu')(x)\n","\n","    # Expanding block2 with projection shortcut\n","    x = expand_conv_basic_block(x, 128, 2)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv2\n","    for i in range(r2 - 1):\n","        x = normal_conv_basic_block(x, 128)\n","        x = Activation('relu')(x)\n","\n","    # Expanding block3 with projection shortcut\n","    x = expand_conv_basic_block(x, 256, 2)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv3\n","    for i in range(r3 - 1):\n","        x = normal_conv_basic_block(x, 256)\n","        x = Activation('relu')(x)\n","\n","    # Expanding block4 with projection shortcut\n","    x = expand_conv_basic_block(x, 512, 2)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv3\n","    for i in range(r4 - 1):\n","        x = normal_conv_basic_block(x, 512)\n","        x = Activation('relu')(x)\n","\n","    shape = K.int_shape(x)\n","\n","    # Average pooling layer\n","    x = AveragePooling2D(pool_size=(shape[1], shape[2]),\n","                         strides=(1, 1))(x)\n","    # x = GlobalAveragePooling2D()(x)\n","    x = Flatten()(x)\n","\n","    # Classifier Block\n","    x = Dense(no_of_outputs, activation='softmax')(x)\n","\n","    model = Model(inputs=inputs, outputs=x)\n","    return model\n","\n","\n","def build_bottleneck_resnet(h, w, no_of_outputs, r1, r2, r3, r4, first_conv_stride=2, first_max_pool=True,\n","                            first_conv_kernel_size=7):\n","    # Creating input tensor\n","    inputs = Input(shape=(h, w, 3), name=\"image_input\")\n","\n","    # Inital Conv block\n","    x = initial_conv(inputs, 64, first_conv_stride, first_conv_kernel_size)\n","\n","    # Optional Max pooling layer\n","    if (first_max_pool):\n","        x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","    # Expanding block1 with projection shortcut\n","    x = expand_conv_bottleneck_block(x, 64, 1)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv1\n","    for i in range(r1 - 1):\n","        x = normal_conv_bottleneck_block(x, 64)\n","        x = Activation('relu')(x)\n","\n","    # Expanding block2 with projection shortcut\n","    x = expand_conv_bottleneck_block(x, 128, 2)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv2\n","    for i in range(r2 - 1):\n","        x = normal_conv_bottleneck_block(x, 128)\n","        x = Activation('relu')(x)\n","\n","    # Expanding block3 with projection shortcut\n","    x = expand_conv_bottleneck_block(x, 256, 2)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv3\n","    for i in range(r3 - 1):\n","        x = normal_conv_bottleneck_block(x, 256)\n","        x = Activation('relu')(x)\n","\n","    # Expanding block4 with projection shortcut\n","    x = expand_conv_bottleneck_block(x, 512, 2)\n","    x = Activation('relu')(x)\n","\n","    # Repeating block of Conv4\n","    for i in range(r4 - 1):\n","        x = normal_conv_bottleneck_block(x, 512)\n","        x = Activation('relu')(x)\n","\n","    shape = K.int_shape(x)\n","\n","    # Average pooling layer\n","    x = AveragePooling2D(pool_size=(shape[1], shape[2]),\n","                         strides=(1, 1))(x)\n","    # x = GlobalAveragePooling2D()(x)\n","\n","    # Classifier Block\n","    x = Flatten()(x)\n","    x = Dense(no_of_outputs, activation='softmax')(x)\n","\n","    model = Model(inputs=inputs, outputs=x)\n","    return model\n"],"metadata":{"id":"5O0lIpOiFHxz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = build_bottleneck_resnet(32,32,2,3,4,6,3,2,True,7)\n","model.summary()\n","plot_model(model,\"ResNet50_wag.png\",show_shapes=True)\n","model.compile(loss='binary_crossentropy',\n","        optimizer=\"Adam\",\n","        metrics=['accuracy',f1,sensitivity,specificity])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjnD_jhlFTKm","executionInfo":{"status":"ok","timestamp":1683350278796,"user_tz":-330,"elapsed":7331,"user":{"displayName":"pythoder 179","userId":"04425462557897485101"}},"outputId":"b10c1790-7a5d-4694-8505-50ffc52e6b65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," image_input (InputLayer)       [(None, 32, 32, 3)]  0           []                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 16, 16, 64)   9472        ['image_input[0][0]']            \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 16, 16, 64)  256         ['conv2d[0][0]']                 \n"," alization)                                                                                       \n","                                                                                                  \n"," activation (Activation)        (None, 16, 16, 64)   0           ['batch_normalization[0][0]']    \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 8, 8, 64)     0           ['activation[0][0]']             \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 8, 8, 64)     4160        ['max_pooling2d[0][0]']          \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_1[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_1 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_1[0][0]']  \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 8, 8, 64)     36928       ['activation_1[0][0]']           \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_2[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_2 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_2[0][0]']  \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 8, 8, 256)    16640       ['activation_2[0][0]']           \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 8, 8, 256)    16640       ['max_pooling2d[0][0]']          \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_3[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_4[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," add (Add)                      (None, 8, 8, 256)    0           ['batch_normalization_3[0][0]',  \n","                                                                  'batch_normalization_4[0][0]']  \n","                                                                                                  \n"," activation_3 (Activation)      (None, 8, 8, 256)    0           ['add[0][0]']                    \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 8, 8, 64)     16448       ['activation_3[0][0]']           \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_5[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_4 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 8, 8, 64)     36928       ['activation_4[0][0]']           \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_6[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_5 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 8, 8, 256)    16640       ['activation_5[0][0]']           \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_7[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," add_1 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_7[0][0]',  \n","                                                                  'activation_3[0][0]']           \n","                                                                                                  \n"," activation_6 (Activation)      (None, 8, 8, 256)    0           ['add_1[0][0]']                  \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 8, 8, 64)     16448       ['activation_6[0][0]']           \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_8[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_7 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_8[0][0]']  \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 8, 8, 64)     36928       ['activation_7[0][0]']           \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_9[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_8 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 8, 8, 256)    16640       ['activation_8[0][0]']           \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_10[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_2 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_10[0][0]', \n","                                                                  'activation_6[0][0]']           \n","                                                                                                  \n"," activation_9 (Activation)      (None, 8, 8, 256)    0           ['add_2[0][0]']                  \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 4, 4, 128)    32896       ['activation_9[0][0]']           \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 4, 4, 128)   512         ['conv2d_11[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_10 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_11[0][0]'] \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 4, 4, 128)    147584      ['activation_10[0][0]']          \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 4, 4, 128)   512         ['conv2d_12[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_11 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 4, 4, 512)    66048       ['activation_11[0][0]']          \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 4, 4, 512)    131584      ['activation_9[0][0]']           \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_13[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_14[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_3 (Add)                    (None, 4, 4, 512)    0           ['batch_normalization_13[0][0]', \n","                                                                  'batch_normalization_14[0][0]'] \n","                                                                                                  \n"," activation_12 (Activation)     (None, 4, 4, 512)    0           ['add_3[0][0]']                  \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 4, 4, 128)    65664       ['activation_12[0][0]']          \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 4, 4, 128)   512         ['conv2d_15[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_13 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_15[0][0]'] \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 4, 4, 128)    147584      ['activation_13[0][0]']          \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 4, 4, 128)   512         ['conv2d_16[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_14 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_16[0][0]'] \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 4, 4, 512)    66048       ['activation_14[0][0]']          \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_17[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_4 (Add)                    (None, 4, 4, 512)    0           ['batch_normalization_17[0][0]', \n","                                                                  'activation_12[0][0]']          \n","                                                                                                  \n"," activation_15 (Activation)     (None, 4, 4, 512)    0           ['add_4[0][0]']                  \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 4, 4, 128)    65664       ['activation_15[0][0]']          \n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 4, 4, 128)   512         ['conv2d_18[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_16 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_18[0][0]'] \n","                                                                                                  \n"," conv2d_19 (Conv2D)             (None, 4, 4, 128)    147584      ['activation_16[0][0]']          \n","                                                                                                  \n"," batch_normalization_19 (BatchN  (None, 4, 4, 128)   512         ['conv2d_19[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_17 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_19[0][0]'] \n","                                                                                                  \n"," conv2d_20 (Conv2D)             (None, 4, 4, 512)    66048       ['activation_17[0][0]']          \n","                                                                                                  \n"," batch_normalization_20 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_20[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_5 (Add)                    (None, 4, 4, 512)    0           ['batch_normalization_20[0][0]', \n","                                                                  'activation_15[0][0]']          \n","                                                                                                  \n"," activation_18 (Activation)     (None, 4, 4, 512)    0           ['add_5[0][0]']                  \n","                                                                                                  \n"," conv2d_21 (Conv2D)             (None, 4, 4, 128)    65664       ['activation_18[0][0]']          \n","                                                                                                  \n"," batch_normalization_21 (BatchN  (None, 4, 4, 128)   512         ['conv2d_21[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_19 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_21[0][0]'] \n","                                                                                                  \n"," conv2d_22 (Conv2D)             (None, 4, 4, 128)    147584      ['activation_19[0][0]']          \n","                                                                                                  \n"," batch_normalization_22 (BatchN  (None, 4, 4, 128)   512         ['conv2d_22[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_20 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_22[0][0]'] \n","                                                                                                  \n"," conv2d_23 (Conv2D)             (None, 4, 4, 512)    66048       ['activation_20[0][0]']          \n","                                                                                                  \n"," batch_normalization_23 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_23[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_6 (Add)                    (None, 4, 4, 512)    0           ['batch_normalization_23[0][0]', \n","                                                                  'activation_18[0][0]']          \n","                                                                                                  \n"," activation_21 (Activation)     (None, 4, 4, 512)    0           ['add_6[0][0]']                  \n","                                                                                                  \n"," conv2d_24 (Conv2D)             (None, 2, 2, 256)    131328      ['activation_21[0][0]']          \n","                                                                                                  \n"," batch_normalization_24 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_24[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_22 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_24[0][0]'] \n","                                                                                                  \n"," conv2d_25 (Conv2D)             (None, 2, 2, 256)    590080      ['activation_22[0][0]']          \n","                                                                                                  \n"," batch_normalization_25 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_25[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_23 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_25[0][0]'] \n","                                                                                                  \n"," conv2d_26 (Conv2D)             (None, 2, 2, 1024)   263168      ['activation_23[0][0]']          \n","                                                                                                  \n"," conv2d_27 (Conv2D)             (None, 2, 2, 1024)   525312      ['activation_21[0][0]']          \n","                                                                                                  \n"," batch_normalization_26 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_26[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_27 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_27[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_7 (Add)                    (None, 2, 2, 1024)   0           ['batch_normalization_26[0][0]', \n","                                                                  'batch_normalization_27[0][0]'] \n","                                                                                                  \n"," activation_24 (Activation)     (None, 2, 2, 1024)   0           ['add_7[0][0]']                  \n","                                                                                                  \n"," conv2d_28 (Conv2D)             (None, 2, 2, 256)    262400      ['activation_24[0][0]']          \n","                                                                                                  \n"," batch_normalization_28 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_28[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_25 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_28[0][0]'] \n","                                                                                                  \n"," conv2d_29 (Conv2D)             (None, 2, 2, 256)    590080      ['activation_25[0][0]']          \n","                                                                                                  \n"," batch_normalization_29 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_29[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_26 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_29[0][0]'] \n","                                                                                                  \n"," conv2d_30 (Conv2D)             (None, 2, 2, 1024)   263168      ['activation_26[0][0]']          \n","                                                                                                  \n"," batch_normalization_30 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_30[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_8 (Add)                    (None, 2, 2, 1024)   0           ['batch_normalization_30[0][0]', \n","                                                                  'activation_24[0][0]']          \n","                                                                                                  \n"," activation_27 (Activation)     (None, 2, 2, 1024)   0           ['add_8[0][0]']                  \n","                                                                                                  \n"," conv2d_31 (Conv2D)             (None, 2, 2, 256)    262400      ['activation_27[0][0]']          \n","                                                                                                  \n"," batch_normalization_31 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_31[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_28 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_31[0][0]'] \n","                                                                                                  \n"," conv2d_32 (Conv2D)             (None, 2, 2, 256)    590080      ['activation_28[0][0]']          \n","                                                                                                  \n"," batch_normalization_32 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_32[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_29 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_32[0][0]'] \n","                                                                                                  \n"," conv2d_33 (Conv2D)             (None, 2, 2, 1024)   263168      ['activation_29[0][0]']          \n","                                                                                                  \n"," batch_normalization_33 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_33[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_9 (Add)                    (None, 2, 2, 1024)   0           ['batch_normalization_33[0][0]', \n","                                                                  'activation_27[0][0]']          \n","                                                                                                  \n"," activation_30 (Activation)     (None, 2, 2, 1024)   0           ['add_9[0][0]']                  \n","                                                                                                  \n"," conv2d_34 (Conv2D)             (None, 2, 2, 256)    262400      ['activation_30[0][0]']          \n","                                                                                                  \n"," batch_normalization_34 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_34[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_31 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_34[0][0]'] \n","                                                                                                  \n"," conv2d_35 (Conv2D)             (None, 2, 2, 256)    590080      ['activation_31[0][0]']          \n","                                                                                                  \n"," batch_normalization_35 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_35[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_32 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_35[0][0]'] \n","                                                                                                  \n"," conv2d_36 (Conv2D)             (None, 2, 2, 1024)   263168      ['activation_32[0][0]']          \n","                                                                                                  \n"," batch_normalization_36 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_36[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_10 (Add)                   (None, 2, 2, 1024)   0           ['batch_normalization_36[0][0]', \n","                                                                  'activation_30[0][0]']          \n","                                                                                                  \n"," activation_33 (Activation)     (None, 2, 2, 1024)   0           ['add_10[0][0]']                 \n","                                                                                                  \n"," conv2d_37 (Conv2D)             (None, 2, 2, 256)    262400      ['activation_33[0][0]']          \n","                                                                                                  \n"," batch_normalization_37 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_37[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_34 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_37[0][0]'] \n","                                                                                                  \n"," conv2d_38 (Conv2D)             (None, 2, 2, 256)    590080      ['activation_34[0][0]']          \n","                                                                                                  \n"," batch_normalization_38 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_38[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_35 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_38[0][0]'] \n","                                                                                                  \n"," conv2d_39 (Conv2D)             (None, 2, 2, 1024)   263168      ['activation_35[0][0]']          \n","                                                                                                  \n"," batch_normalization_39 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_39[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_11 (Add)                   (None, 2, 2, 1024)   0           ['batch_normalization_39[0][0]', \n","                                                                  'activation_33[0][0]']          \n","                                                                                                  \n"," activation_36 (Activation)     (None, 2, 2, 1024)   0           ['add_11[0][0]']                 \n","                                                                                                  \n"," conv2d_40 (Conv2D)             (None, 2, 2, 256)    262400      ['activation_36[0][0]']          \n","                                                                                                  \n"," batch_normalization_40 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_40[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_37 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_40[0][0]'] \n","                                                                                                  \n"," conv2d_41 (Conv2D)             (None, 2, 2, 256)    590080      ['activation_37[0][0]']          \n","                                                                                                  \n"," batch_normalization_41 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_41[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_38 (Activation)     (None, 2, 2, 256)    0           ['batch_normalization_41[0][0]'] \n","                                                                                                  \n"," conv2d_42 (Conv2D)             (None, 2, 2, 1024)   263168      ['activation_38[0][0]']          \n","                                                                                                  \n"," batch_normalization_42 (BatchN  (None, 2, 2, 1024)  4096        ['conv2d_42[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_12 (Add)                   (None, 2, 2, 1024)   0           ['batch_normalization_42[0][0]', \n","                                                                  'activation_36[0][0]']          \n","                                                                                                  \n"," activation_39 (Activation)     (None, 2, 2, 1024)   0           ['add_12[0][0]']                 \n","                                                                                                  \n"," conv2d_43 (Conv2D)             (None, 1, 1, 512)    524800      ['activation_39[0][0]']          \n","                                                                                                  \n"," batch_normalization_43 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_43[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_40 (Activation)     (None, 1, 1, 512)    0           ['batch_normalization_43[0][0]'] \n","                                                                                                  \n"," conv2d_44 (Conv2D)             (None, 1, 1, 512)    2359808     ['activation_40[0][0]']          \n","                                                                                                  \n"," batch_normalization_44 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_44[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_41 (Activation)     (None, 1, 1, 512)    0           ['batch_normalization_44[0][0]'] \n","                                                                                                  \n"," conv2d_45 (Conv2D)             (None, 1, 1, 2048)   1050624     ['activation_41[0][0]']          \n","                                                                                                  \n"," conv2d_46 (Conv2D)             (None, 1, 1, 2048)   2099200     ['activation_39[0][0]']          \n","                                                                                                  \n"," batch_normalization_45 (BatchN  (None, 1, 1, 2048)  8192        ['conv2d_45[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_46 (BatchN  (None, 1, 1, 2048)  8192        ['conv2d_46[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_13 (Add)                   (None, 1, 1, 2048)   0           ['batch_normalization_45[0][0]', \n","                                                                  'batch_normalization_46[0][0]'] \n","                                                                                                  \n"," activation_42 (Activation)     (None, 1, 1, 2048)   0           ['add_13[0][0]']                 \n","                                                                                                  \n"," conv2d_47 (Conv2D)             (None, 1, 1, 512)    1049088     ['activation_42[0][0]']          \n","                                                                                                  \n"," batch_normalization_47 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_47[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_43 (Activation)     (None, 1, 1, 512)    0           ['batch_normalization_47[0][0]'] \n","                                                                                                  \n"," conv2d_48 (Conv2D)             (None, 1, 1, 512)    2359808     ['activation_43[0][0]']          \n","                                                                                                  \n"," batch_normalization_48 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_48[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_44 (Activation)     (None, 1, 1, 512)    0           ['batch_normalization_48[0][0]'] \n","                                                                                                  \n"," conv2d_49 (Conv2D)             (None, 1, 1, 2048)   1050624     ['activation_44[0][0]']          \n","                                                                                                  \n"," batch_normalization_49 (BatchN  (None, 1, 1, 2048)  8192        ['conv2d_49[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_14 (Add)                   (None, 1, 1, 2048)   0           ['batch_normalization_49[0][0]', \n","                                                                  'activation_42[0][0]']          \n","                                                                                                  \n"," activation_45 (Activation)     (None, 1, 1, 2048)   0           ['add_14[0][0]']                 \n","                                                                                                  \n"," conv2d_50 (Conv2D)             (None, 1, 1, 512)    1049088     ['activation_45[0][0]']          \n","                                                                                                  \n"," batch_normalization_50 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_50[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_46 (Activation)     (None, 1, 1, 512)    0           ['batch_normalization_50[0][0]'] \n","                                                                                                  \n"," conv2d_51 (Conv2D)             (None, 1, 1, 512)    2359808     ['activation_46[0][0]']          \n","                                                                                                  \n"," batch_normalization_51 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_51[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_47 (Activation)     (None, 1, 1, 512)    0           ['batch_normalization_51[0][0]'] \n","                                                                                                  \n"," conv2d_52 (Conv2D)             (None, 1, 1, 2048)   1050624     ['activation_47[0][0]']          \n","                                                                                                  \n"," batch_normalization_52 (BatchN  (None, 1, 1, 2048)  8192        ['conv2d_52[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_15 (Add)                   (None, 1, 1, 2048)   0           ['batch_normalization_52[0][0]', \n","                                                                  'activation_45[0][0]']          \n","                                                                                                  \n"," activation_48 (Activation)     (None, 1, 1, 2048)   0           ['add_15[0][0]']                 \n","                                                                                                  \n"," average_pooling2d (AveragePool  (None, 1, 1, 2048)  0           ['activation_48[0][0]']          \n"," ing2D)                                                                                           \n","                                                                                                  \n"," flatten (Flatten)              (None, 2048)         0           ['average_pooling2d[0][0]']      \n","                                                                                                  \n"," dense (Dense)                  (None, 2)            4098        ['flatten[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 23,591,810\n","Trainable params: 23,538,690\n","Non-trainable params: 53,120\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)\n","csv_logger = CSVLogger(METRICS_PATH+MODEL_NAME+\".csv\")\n","model_chekpoint = ModelCheckpoint(\"ResNet50_F1score_DA_aug.hdf5\",monitor = 'val_loss',verbose = 1,save_best_only=True)\n"],"metadata":{"id":"Se5-8T6iM03A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","batch_size = 16\n","data_augmentation = False\n","epochs = 100"],"metadata":{"id":"pDP7SmY6FdVo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","start = time.time()\n","\n","if data_augmentation:\n","    print(\"-------------Using Data augmentation------------\")\n","    # This will do preprocessing and realtime data augmentation:\n","    datagen = ImageDataGenerator(\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        horizontal_flip=True,\n","        vertical_flip=True)  # randomly flip images\n","\n","    datagen.fit(x_train)\n","    history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n","                        steps_per_epoch=x_train.shape[0] // batch_size,\n","                        epochs=epochs, verbose=1, validation_data=(x_test, y_test),\n","                        callbacks=[lr_reducer,  csv_logger, model_chekpoint])\n","\n","else:\n","    print(\"-----Not Using Data augmentation---------------\")\n","    history = model.fit(x_train, y_train,\n","              batch_size=batch_size * 4,\n","              epochs=epochs,\n","              validation_data=(x_test, y_test),\n","              shuffle=True, callbacks=[lr_reducer,  csv_logger, model_chekpoint])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZkzCryBGFkyy","executionInfo":{"status":"ok","timestamp":1683352055718,"user_tz":-330,"elapsed":1776522,"user":{"displayName":"pythoder 179","userId":"04425462557897485101"}},"outputId":"8a58a6c5-6b31-4433-eda4-205209cedaa8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-----Not Using Data augmentation---------------\n","Epoch 1/100\n","345/345 [==============================] - ETA: 0s - loss: 0.6674 - accuracy: 0.6984 - f1: 0.6985 - sensitivity: 0.6985 - specificity: 0.6985\n","Epoch 1: val_loss improved from inf to 0.72020, saving model to ResNet50_F1score_DA_aug.hdf5\n","345/345 [==============================] - 76s 60ms/step - loss: 0.6674 - accuracy: 0.6984 - f1: 0.6985 - sensitivity: 0.6985 - specificity: 0.6985 - val_loss: 0.7202 - val_accuracy: 0.6606 - val_f1: 0.6589 - val_sensitivity: 0.6589 - val_specificity: 0.6589 - lr: 0.0010\n","Epoch 2/100\n","345/345 [==============================] - ETA: 0s - loss: 0.4132 - accuracy: 0.8216 - f1: 0.8218 - sensitivity: 0.8218 - specificity: 0.8218\n","Epoch 2: val_loss improved from 0.72020 to 0.45404, saving model to ResNet50_F1score_DA_aug.hdf5\n","345/345 [==============================] - 18s 53ms/step - loss: 0.4132 - accuracy: 0.8216 - f1: 0.8218 - sensitivity: 0.8218 - specificity: 0.8218 - val_loss: 0.4540 - val_accuracy: 0.8059 - val_f1: 0.8041 - val_sensitivity: 0.8041 - val_specificity: 0.8041 - lr: 0.0010\n","Epoch 3/100\n","344/345 [============================>.] - ETA: 0s - loss: 0.2728 - accuracy: 0.8845 - f1: 0.8845 - sensitivity: 0.8845 - specificity: 0.8845\n","Epoch 3: val_loss did not improve from 0.45404\n","345/345 [==============================] - 16s 46ms/step - loss: 0.2727 - accuracy: 0.8846 - f1: 0.8846 - sensitivity: 0.8846 - specificity: 0.8846 - val_loss: 0.5488 - val_accuracy: 0.7983 - val_f1: 0.7978 - val_sensitivity: 0.7978 - val_specificity: 0.7978 - lr: 0.0010\n","Epoch 4/100\n","344/345 [============================>.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9218 - f1: 0.9218 - sensitivity: 0.9218 - specificity: 0.9218\n","Epoch 4: val_loss did not improve from 0.45404\n","345/345 [==============================] - 16s 47ms/step - loss: 0.1949 - accuracy: 0.9218 - f1: 0.9218 - sensitivity: 0.9218 - specificity: 0.9218 - val_loss: 0.6347 - val_accuracy: 0.7709 - val_f1: 0.7694 - val_sensitivity: 0.7694 - val_specificity: 0.7694 - lr: 0.0010\n","Epoch 5/100\n","344/345 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9547 - f1: 0.9547 - sensitivity: 0.9547 - specificity: 0.9547\n","Epoch 5: val_loss improved from 0.45404 to 0.19532, saving model to ResNet50_F1score_DA_aug.hdf5\n","345/345 [==============================] - 19s 56ms/step - loss: 0.1203 - accuracy: 0.9547 - f1: 0.9547 - sensitivity: 0.9547 - specificity: 0.9547 - val_loss: 0.1953 - val_accuracy: 0.9291 - val_f1: 0.9273 - val_sensitivity: 0.9273 - val_specificity: 0.9273 - lr: 3.1623e-04\n","Epoch 6/100\n","345/345 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9648 - f1: 0.9647 - sensitivity: 0.9647 - specificity: 0.9647\n","Epoch 6: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 0.0935 - accuracy: 0.9648 - f1: 0.9647 - sensitivity: 0.9647 - specificity: 0.9647 - val_loss: 0.2371 - val_accuracy: 0.9126 - val_f1: 0.9122 - val_sensitivity: 0.9122 - val_specificity: 0.9122 - lr: 3.1623e-04\n","Epoch 7/100\n","344/345 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9711 - f1: 0.9711 - sensitivity: 0.9711 - specificity: 0.9711\n","Epoch 7: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 0.0780 - accuracy: 0.9711 - f1: 0.9712 - sensitivity: 0.9712 - specificity: 0.9712 - val_loss: 0.2602 - val_accuracy: 0.9182 - val_f1: 0.9152 - val_sensitivity: 0.9152 - val_specificity: 0.9152 - lr: 3.1623e-04\n","Epoch 8/100\n","345/345 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9887 - f1: 0.9887 - sensitivity: 0.9887 - specificity: 0.9887\n","Epoch 8: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 47ms/step - loss: 0.0334 - accuracy: 0.9887 - f1: 0.9887 - sensitivity: 0.9887 - specificity: 0.9887 - val_loss: 0.2592 - val_accuracy: 0.9303 - val_f1: 0.9298 - val_sensitivity: 0.9298 - val_specificity: 0.9298 - lr: 1.0000e-04\n","Epoch 9/100\n","344/345 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9957 - f1: 0.9957 - sensitivity: 0.9957 - specificity: 0.9957\n","Epoch 9: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 48ms/step - loss: 0.0127 - accuracy: 0.9957 - f1: 0.9957 - sensitivity: 0.9957 - specificity: 0.9957 - val_loss: 0.3525 - val_accuracy: 0.9300 - val_f1: 0.9294 - val_sensitivity: 0.9294 - val_specificity: 0.9294 - lr: 1.0000e-04\n","Epoch 10/100\n","345/345 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 0.9981 - f1: 0.9981 - sensitivity: 0.9981 - specificity: 0.9981\n","Epoch 10: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 0.0055 - accuracy: 0.9981 - f1: 0.9981 - sensitivity: 0.9981 - specificity: 0.9981 - val_loss: 0.3264 - val_accuracy: 0.9334 - val_f1: 0.9341 - val_sensitivity: 0.9341 - val_specificity: 0.9341 - lr: 3.1623e-05\n","Epoch 11/100\n","344/345 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9994 - f1: 0.9994 - sensitivity: 0.9994 - specificity: 0.9994\n","Epoch 11: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 48ms/step - loss: 0.0025 - accuracy: 0.9994 - f1: 0.9994 - sensitivity: 0.9994 - specificity: 0.9994 - val_loss: 0.3417 - val_accuracy: 0.9349 - val_f1: 0.9355 - val_sensitivity: 0.9355 - val_specificity: 0.9355 - lr: 3.1623e-05\n","Epoch 12/100\n","345/345 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 12: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 0.0011 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.3512 - val_accuracy: 0.9372 - val_f1: 0.9379 - val_sensitivity: 0.9379 - val_specificity: 0.9379 - lr: 1.0000e-05\n","Epoch 13/100\n","345/345 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998 - f1: 0.9998 - sensitivity: 0.9998 - specificity: 0.9998\n","Epoch 13: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 48ms/step - loss: 0.0010 - accuracy: 0.9998 - f1: 0.9998 - sensitivity: 0.9998 - specificity: 0.9998 - val_loss: 0.3609 - val_accuracy: 0.9352 - val_f1: 0.9359 - val_sensitivity: 0.9359 - val_specificity: 0.9359 - lr: 1.0000e-05\n","Epoch 14/100\n","344/345 [============================>.] - ETA: 0s - loss: 8.2176e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999\n","Epoch 14: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 8.2115e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.3665 - val_accuracy: 0.9358 - val_f1: 0.9364 - val_sensitivity: 0.9364 - val_specificity: 0.9364 - lr: 3.1623e-06\n","Epoch 15/100\n","344/345 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998 - f1: 0.9998 - sensitivity: 0.9998 - specificity: 0.9998\n","Epoch 15: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 48ms/step - loss: 0.0011 - accuracy: 0.9998 - f1: 0.9998 - sensitivity: 0.9998 - specificity: 0.9998 - val_loss: 0.3689 - val_accuracy: 0.9343 - val_f1: 0.9350 - val_sensitivity: 0.9350 - val_specificity: 0.9350 - lr: 3.1623e-06\n","Epoch 16/100\n","344/345 [============================>.] - ETA: 0s - loss: 5.8834e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999\n","Epoch 16: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 48ms/step - loss: 5.8900e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.3715 - val_accuracy: 0.9343 - val_f1: 0.9350 - val_sensitivity: 0.9350 - val_specificity: 0.9350 - lr: 1.0000e-06\n","Epoch 17/100\n","345/345 [==============================] - ETA: 0s - loss: 5.8876e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 17: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 48ms/step - loss: 5.8876e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.3709 - val_accuracy: 0.9345 - val_f1: 0.9352 - val_sensitivity: 0.9352 - val_specificity: 0.9352 - lr: 1.0000e-06\n","Epoch 18/100\n","345/345 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998 - f1: 0.9998 - sensitivity: 0.9998 - specificity: 0.9998\n","Epoch 18: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 0.0011 - accuracy: 0.9998 - f1: 0.9998 - sensitivity: 0.9998 - specificity: 0.9998 - val_loss: 0.3694 - val_accuracy: 0.9347 - val_f1: 0.9353 - val_sensitivity: 0.9353 - val_specificity: 0.9353 - lr: 5.0000e-07\n","Epoch 19/100\n","344/345 [============================>.] - ETA: 0s - loss: 7.3380e-04 - accuracy: 0.9998 - f1: 0.9998 - sensitivity: 0.9998 - specificity: 0.9998\n","Epoch 19: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 48ms/step - loss: 7.3295e-04 - accuracy: 0.9998 - f1: 0.9998 - sensitivity: 0.9998 - specificity: 0.9998 - val_loss: 0.3709 - val_accuracy: 0.9341 - val_f1: 0.9348 - val_sensitivity: 0.9348 - val_specificity: 0.9348 - lr: 5.0000e-07\n","Epoch 20/100\n","344/345 [============================>.] - ETA: 0s - loss: 7.9488e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 20: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 47ms/step - loss: 7.9402e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.3719 - val_accuracy: 0.9349 - val_f1: 0.9355 - val_sensitivity: 0.9355 - val_specificity: 0.9355 - lr: 5.0000e-07\n","Epoch 21/100\n","344/345 [============================>.] - ETA: 0s - loss: 5.8317e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999\n","Epoch 21: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 47ms/step - loss: 5.8258e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.3725 - val_accuracy: 0.9345 - val_f1: 0.9352 - val_sensitivity: 0.9352 - val_specificity: 0.9352 - lr: 5.0000e-07\n","Epoch 22/100\n","344/345 [============================>.] - ETA: 0s - loss: 4.7106e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 22: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 47ms/step - loss: 5.4960e-04 - accuracy: 1.0000 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.3744 - val_accuracy: 0.9352 - val_f1: 0.9359 - val_sensitivity: 0.9359 - val_specificity: 0.9359 - lr: 5.0000e-07\n","Epoch 23/100\n","345/345 [==============================] - ETA: 0s - loss: 4.8744e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 23: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 4.8744e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.3759 - val_accuracy: 0.9341 - val_f1: 0.9348 - val_sensitivity: 0.9348 - val_specificity: 0.9348 - lr: 5.0000e-07\n","Epoch 24/100\n","344/345 [============================>.] - ETA: 0s - loss: 4.9435e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 24: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 48ms/step - loss: 4.9380e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.3752 - val_accuracy: 0.9341 - val_f1: 0.9348 - val_sensitivity: 0.9348 - val_specificity: 0.9348 - lr: 5.0000e-07\n","Epoch 25/100\n","344/345 [============================>.] - ETA: 0s - loss: 5.4995e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999\n","Epoch 25: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 47ms/step - loss: 5.5009e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.3784 - val_accuracy: 0.9347 - val_f1: 0.9353 - val_sensitivity: 0.9353 - val_specificity: 0.9353 - lr: 5.0000e-07\n","Epoch 26/100\n","345/345 [==============================] - ETA: 0s - loss: 5.4192e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 26: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 47ms/step - loss: 5.4192e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.3829 - val_accuracy: 0.9341 - val_f1: 0.9348 - val_sensitivity: 0.9348 - val_specificity: 0.9348 - lr: 5.0000e-07\n","Epoch 27/100\n","345/345 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997 - f1: 0.9997 - sensitivity: 0.9997 - specificity: 0.9997\n","Epoch 27: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 0.0011 - accuracy: 0.9997 - f1: 0.9997 - sensitivity: 0.9997 - specificity: 0.9997 - val_loss: 0.3789 - val_accuracy: 0.9349 - val_f1: 0.9355 - val_sensitivity: 0.9355 - val_specificity: 0.9355 - lr: 5.0000e-07\n","Epoch 28/100\n","345/345 [==============================] - ETA: 0s - loss: 4.0085e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 28: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 4.0085e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.3805 - val_accuracy: 0.9352 - val_f1: 0.9359 - val_sensitivity: 0.9359 - val_specificity: 0.9359 - lr: 5.0000e-07\n","Epoch 29/100\n","344/345 [============================>.] - ETA: 0s - loss: 3.9590e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 29: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 48ms/step - loss: 3.9672e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.3835 - val_accuracy: 0.9356 - val_f1: 0.9362 - val_sensitivity: 0.9362 - val_specificity: 0.9362 - lr: 5.0000e-07\n","Epoch 30/100\n","345/345 [==============================] - ETA: 0s - loss: 3.5288e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 30: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 48ms/step - loss: 3.5288e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.3869 - val_accuracy: 0.9354 - val_f1: 0.9361 - val_sensitivity: 0.9361 - val_specificity: 0.9361 - lr: 5.0000e-07\n","Epoch 31/100\n","345/345 [==============================] - ETA: 0s - loss: 3.6851e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 31: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 3.6851e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.3872 - val_accuracy: 0.9354 - val_f1: 0.9361 - val_sensitivity: 0.9361 - val_specificity: 0.9361 - lr: 5.0000e-07\n","Epoch 32/100\n","344/345 [============================>.] - ETA: 0s - loss: 4.7468e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999\n","Epoch 32: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 48ms/step - loss: 4.7485e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.3905 - val_accuracy: 0.9356 - val_f1: 0.9362 - val_sensitivity: 0.9362 - val_specificity: 0.9362 - lr: 5.0000e-07\n","Epoch 33/100\n","345/345 [==============================] - ETA: 0s - loss: 4.3857e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999\n","Epoch 33: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 48ms/step - loss: 4.3857e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.3918 - val_accuracy: 0.9356 - val_f1: 0.9362 - val_sensitivity: 0.9362 - val_specificity: 0.9362 - lr: 5.0000e-07\n","Epoch 34/100\n","345/345 [==============================] - ETA: 0s - loss: 4.0961e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999\n","Epoch 34: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 4.0961e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.3938 - val_accuracy: 0.9360 - val_f1: 0.9366 - val_sensitivity: 0.9366 - val_specificity: 0.9366 - lr: 5.0000e-07\n","Epoch 35/100\n","344/345 [============================>.] - ETA: 0s - loss: 5.6371e-04 - accuracy: 0.9998 - f1: 0.9998 - sensitivity: 0.9998 - specificity: 0.9998\n","Epoch 35: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 5.6373e-04 - accuracy: 0.9998 - f1: 0.9998 - sensitivity: 0.9998 - specificity: 0.9998 - val_loss: 0.3931 - val_accuracy: 0.9356 - val_f1: 0.9362 - val_sensitivity: 0.9362 - val_specificity: 0.9362 - lr: 5.0000e-07\n","Epoch 36/100\n","345/345 [==============================] - ETA: 0s - loss: 3.4535e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999\n","Epoch 36: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 51ms/step - loss: 3.4535e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.3973 - val_accuracy: 0.9349 - val_f1: 0.9355 - val_sensitivity: 0.9355 - val_specificity: 0.9355 - lr: 5.0000e-07\n","Epoch 37/100\n","344/345 [============================>.] - ETA: 0s - loss: 4.5303e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 37: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 48ms/step - loss: 4.5289e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.3994 - val_accuracy: 0.9347 - val_f1: 0.9353 - val_sensitivity: 0.9353 - val_specificity: 0.9353 - lr: 5.0000e-07\n","Epoch 38/100\n","344/345 [============================>.] - ETA: 0s - loss: 3.1956e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 38: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 47ms/step - loss: 3.1988e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.3998 - val_accuracy: 0.9354 - val_f1: 0.9361 - val_sensitivity: 0.9361 - val_specificity: 0.9361 - lr: 5.0000e-07\n","Epoch 39/100\n","344/345 [============================>.] - ETA: 0s - loss: 2.2544e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 39: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 47ms/step - loss: 2.2529e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4019 - val_accuracy: 0.9354 - val_f1: 0.9361 - val_sensitivity: 0.9361 - val_specificity: 0.9361 - lr: 5.0000e-07\n","Epoch 40/100\n","344/345 [============================>.] - ETA: 0s - loss: 2.1147e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 40: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 2.1138e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4057 - val_accuracy: 0.9345 - val_f1: 0.9352 - val_sensitivity: 0.9352 - val_specificity: 0.9352 - lr: 5.0000e-07\n","Epoch 41/100\n","345/345 [==============================] - ETA: 0s - loss: 1.8998e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 41: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 1.8998e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4073 - val_accuracy: 0.9354 - val_f1: 0.9361 - val_sensitivity: 0.9361 - val_specificity: 0.9361 - lr: 5.0000e-07\n","Epoch 42/100\n","345/345 [==============================] - ETA: 0s - loss: 1.7897e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 42: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 48ms/step - loss: 1.7897e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4151 - val_accuracy: 0.9352 - val_f1: 0.9359 - val_sensitivity: 0.9359 - val_specificity: 0.9359 - lr: 5.0000e-07\n","Epoch 43/100\n","344/345 [============================>.] - ETA: 0s - loss: 1.9854e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 43: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 48ms/step - loss: 1.9842e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4185 - val_accuracy: 0.9351 - val_f1: 0.9357 - val_sensitivity: 0.9357 - val_specificity: 0.9357 - lr: 5.0000e-07\n","Epoch 44/100\n","345/345 [==============================] - ETA: 0s - loss: 2.2032e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 44: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 2.2032e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4238 - val_accuracy: 0.9356 - val_f1: 0.9362 - val_sensitivity: 0.9362 - val_specificity: 0.9362 - lr: 5.0000e-07\n","Epoch 45/100\n","344/345 [============================>.] - ETA: 0s - loss: 6.3552e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999\n","Epoch 45: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 6.3509e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.4254 - val_accuracy: 0.9336 - val_f1: 0.9343 - val_sensitivity: 0.9343 - val_specificity: 0.9343 - lr: 5.0000e-07\n","Epoch 46/100\n","345/345 [==============================] - ETA: 0s - loss: 2.6768e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 46: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 2.6768e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4271 - val_accuracy: 0.9341 - val_f1: 0.9348 - val_sensitivity: 0.9348 - val_specificity: 0.9348 - lr: 5.0000e-07\n","Epoch 47/100\n","344/345 [============================>.] - ETA: 0s - loss: 1.5729e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 47: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 1.5716e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4266 - val_accuracy: 0.9340 - val_f1: 0.9346 - val_sensitivity: 0.9346 - val_specificity: 0.9346 - lr: 5.0000e-07\n","Epoch 48/100\n","345/345 [==============================] - ETA: 0s - loss: 1.4011e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 48: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 1.4011e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4293 - val_accuracy: 0.9347 - val_f1: 0.9353 - val_sensitivity: 0.9353 - val_specificity: 0.9353 - lr: 5.0000e-07\n","Epoch 49/100\n","344/345 [============================>.] - ETA: 0s - loss: 9.2725e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 49: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 52ms/step - loss: 9.5908e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4303 - val_accuracy: 0.9341 - val_f1: 0.9348 - val_sensitivity: 0.9348 - val_specificity: 0.9348 - lr: 5.0000e-07\n","Epoch 50/100\n","344/345 [============================>.] - ETA: 0s - loss: 3.1806e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999\n","Epoch 50: val_loss did not improve from 0.19532\n","345/345 [==============================] - 16s 48ms/step - loss: 3.1775e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.4297 - val_accuracy: 0.9343 - val_f1: 0.9350 - val_sensitivity: 0.9350 - val_specificity: 0.9350 - lr: 5.0000e-07\n","Epoch 51/100\n","345/345 [==============================] - ETA: 0s - loss: 1.8126e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 51: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 1.8126e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4333 - val_accuracy: 0.9341 - val_f1: 0.9348 - val_sensitivity: 0.9348 - val_specificity: 0.9348 - lr: 5.0000e-07\n","Epoch 52/100\n","345/345 [==============================] - ETA: 0s - loss: 1.7502e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 52: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 1.7502e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4342 - val_accuracy: 0.9340 - val_f1: 0.9346 - val_sensitivity: 0.9346 - val_specificity: 0.9346 - lr: 5.0000e-07\n","Epoch 53/100\n","344/345 [============================>.] - ETA: 0s - loss: 1.4048e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 53: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 1.4031e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4382 - val_accuracy: 0.9343 - val_f1: 0.9350 - val_sensitivity: 0.9350 - val_specificity: 0.9350 - lr: 5.0000e-07\n","Epoch 54/100\n","345/345 [==============================] - ETA: 0s - loss: 1.1579e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 54: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 1.1579e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4393 - val_accuracy: 0.9347 - val_f1: 0.9353 - val_sensitivity: 0.9353 - val_specificity: 0.9353 - lr: 5.0000e-07\n","Epoch 55/100\n","344/345 [============================>.] - ETA: 0s - loss: 8.5556e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 55: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 51ms/step - loss: 8.5554e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4409 - val_accuracy: 0.9347 - val_f1: 0.9353 - val_sensitivity: 0.9353 - val_specificity: 0.9353 - lr: 5.0000e-07\n","Epoch 56/100\n","345/345 [==============================] - ETA: 0s - loss: 8.5897e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 56: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 53ms/step - loss: 8.5897e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4437 - val_accuracy: 0.9338 - val_f1: 0.9344 - val_sensitivity: 0.9344 - val_specificity: 0.9344 - lr: 5.0000e-07\n","Epoch 57/100\n","344/345 [============================>.] - ETA: 0s - loss: 3.1023e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999\n","Epoch 57: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 3.0983e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.4454 - val_accuracy: 0.9338 - val_f1: 0.9344 - val_sensitivity: 0.9344 - val_specificity: 0.9344 - lr: 5.0000e-07\n","Epoch 58/100\n","344/345 [============================>.] - ETA: 0s - loss: 9.2274e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 58: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 9.2324e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4487 - val_accuracy: 0.9340 - val_f1: 0.9346 - val_sensitivity: 0.9346 - val_specificity: 0.9346 - lr: 5.0000e-07\n","Epoch 59/100\n","344/345 [============================>.] - ETA: 0s - loss: 7.6387e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 59: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 51ms/step - loss: 7.6354e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4483 - val_accuracy: 0.9352 - val_f1: 0.9359 - val_sensitivity: 0.9359 - val_specificity: 0.9359 - lr: 5.0000e-07\n","Epoch 60/100\n","345/345 [==============================] - ETA: 0s - loss: 1.8561e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999\n","Epoch 60: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 52ms/step - loss: 1.8561e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.4493 - val_accuracy: 0.9347 - val_f1: 0.9353 - val_sensitivity: 0.9353 - val_specificity: 0.9353 - lr: 5.0000e-07\n","Epoch 61/100\n","344/345 [============================>.] - ETA: 0s - loss: 2.2639e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 61: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 2.2609e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4476 - val_accuracy: 0.9343 - val_f1: 0.9350 - val_sensitivity: 0.9350 - val_specificity: 0.9350 - lr: 5.0000e-07\n","Epoch 62/100\n","345/345 [==============================] - ETA: 0s - loss: 8.1499e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 62: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 8.1499e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4507 - val_accuracy: 0.9347 - val_f1: 0.9353 - val_sensitivity: 0.9353 - val_specificity: 0.9353 - lr: 5.0000e-07\n","Epoch 63/100\n","345/345 [==============================] - ETA: 0s - loss: 5.7359e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 63: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 5.7359e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4548 - val_accuracy: 0.9343 - val_f1: 0.9350 - val_sensitivity: 0.9350 - val_specificity: 0.9350 - lr: 5.0000e-07\n","Epoch 64/100\n","345/345 [==============================] - ETA: 0s - loss: 1.1991e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 64: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 53ms/step - loss: 1.1991e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4541 - val_accuracy: 0.9345 - val_f1: 0.9352 - val_sensitivity: 0.9352 - val_specificity: 0.9352 - lr: 5.0000e-07\n","Epoch 65/100\n","345/345 [==============================] - ETA: 0s - loss: 8.2861e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 65: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 8.2861e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4606 - val_accuracy: 0.9349 - val_f1: 0.9355 - val_sensitivity: 0.9355 - val_specificity: 0.9355 - lr: 5.0000e-07\n","Epoch 66/100\n","345/345 [==============================] - ETA: 0s - loss: 5.2006e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 66: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 51ms/step - loss: 5.2006e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4642 - val_accuracy: 0.9347 - val_f1: 0.9353 - val_sensitivity: 0.9353 - val_specificity: 0.9353 - lr: 5.0000e-07\n","Epoch 67/100\n","345/345 [==============================] - ETA: 0s - loss: 4.7596e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 67: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 52ms/step - loss: 4.7596e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4661 - val_accuracy: 0.9349 - val_f1: 0.9355 - val_sensitivity: 0.9355 - val_specificity: 0.9355 - lr: 5.0000e-07\n","Epoch 68/100\n","345/345 [==============================] - ETA: 0s - loss: 5.7177e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 68: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 53ms/step - loss: 5.7177e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4716 - val_accuracy: 0.9343 - val_f1: 0.9350 - val_sensitivity: 0.9350 - val_specificity: 0.9350 - lr: 5.0000e-07\n","Epoch 69/100\n","344/345 [============================>.] - ETA: 0s - loss: 2.7876e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999\n","Epoch 69: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 51ms/step - loss: 2.7843e-04 - accuracy: 0.9999 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.4746 - val_accuracy: 0.9345 - val_f1: 0.9352 - val_sensitivity: 0.9352 - val_specificity: 0.9352 - lr: 5.0000e-07\n","Epoch 70/100\n","345/345 [==============================] - ETA: 0s - loss: 4.7273e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 70: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 51ms/step - loss: 4.7273e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4731 - val_accuracy: 0.9347 - val_f1: 0.9353 - val_sensitivity: 0.9353 - val_specificity: 0.9353 - lr: 5.0000e-07\n","Epoch 71/100\n","345/345 [==============================] - ETA: 0s - loss: 3.8063e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 71: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 52ms/step - loss: 3.8063e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4779 - val_accuracy: 0.9345 - val_f1: 0.9352 - val_sensitivity: 0.9352 - val_specificity: 0.9352 - lr: 5.0000e-07\n","Epoch 72/100\n","345/345 [==============================] - ETA: 0s - loss: 4.0437e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 72: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 4.0437e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4807 - val_accuracy: 0.9345 - val_f1: 0.9352 - val_sensitivity: 0.9352 - val_specificity: 0.9352 - lr: 5.0000e-07\n","Epoch 73/100\n","344/345 [============================>.] - ETA: 0s - loss: 1.0029e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 73: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 1.0017e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.9349 - val_f1: 0.9355 - val_sensitivity: 0.9355 - val_specificity: 0.9355 - lr: 5.0000e-07\n","Epoch 74/100\n","345/345 [==============================] - ETA: 0s - loss: 4.9638e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 74: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 4.9638e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4895 - val_accuracy: 0.9351 - val_f1: 0.9357 - val_sensitivity: 0.9357 - val_specificity: 0.9357 - lr: 5.0000e-07\n","Epoch 75/100\n","345/345 [==============================] - ETA: 0s - loss: 1.2482e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 75: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 52ms/step - loss: 1.2482e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4903 - val_accuracy: 0.9340 - val_f1: 0.9346 - val_sensitivity: 0.9346 - val_specificity: 0.9346 - lr: 5.0000e-07\n","Epoch 76/100\n","345/345 [==============================] - ETA: 0s - loss: 1.4846e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 76: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 1.4846e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4943 - val_accuracy: 0.9349 - val_f1: 0.9355 - val_sensitivity: 0.9355 - val_specificity: 0.9355 - lr: 5.0000e-07\n","Epoch 77/100\n","345/345 [==============================] - ETA: 0s - loss: 2.6306e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 77: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 2.6306e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4956 - val_accuracy: 0.9349 - val_f1: 0.9355 - val_sensitivity: 0.9355 - val_specificity: 0.9355 - lr: 5.0000e-07\n","Epoch 78/100\n","345/345 [==============================] - ETA: 0s - loss: 4.8478e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 78: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 4.8478e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4962 - val_accuracy: 0.9343 - val_f1: 0.9350 - val_sensitivity: 0.9350 - val_specificity: 0.9350 - lr: 5.0000e-07\n","Epoch 79/100\n","345/345 [==============================] - ETA: 0s - loss: 1.8937e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 79: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 52ms/step - loss: 1.8937e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4988 - val_accuracy: 0.9345 - val_f1: 0.9352 - val_sensitivity: 0.9352 - val_specificity: 0.9352 - lr: 5.0000e-07\n","Epoch 80/100\n","344/345 [============================>.] - ETA: 0s - loss: 4.8744e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 80: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 4.8689e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4982 - val_accuracy: 0.9341 - val_f1: 0.9348 - val_sensitivity: 0.9348 - val_specificity: 0.9348 - lr: 5.0000e-07\n","Epoch 81/100\n","344/345 [============================>.] - ETA: 0s - loss: 8.2339e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 81: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 8.2235e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4955 - val_accuracy: 0.9334 - val_f1: 0.9341 - val_sensitivity: 0.9341 - val_specificity: 0.9341 - lr: 5.0000e-07\n","Epoch 82/100\n","345/345 [==============================] - ETA: 0s - loss: 5.0200e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 82: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 5.0200e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4991 - val_accuracy: 0.9334 - val_f1: 0.9341 - val_sensitivity: 0.9341 - val_specificity: 0.9341 - lr: 5.0000e-07\n","Epoch 83/100\n","345/345 [==============================] - ETA: 0s - loss: 4.5074e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 83: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 51ms/step - loss: 4.5074e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5005 - val_accuracy: 0.9338 - val_f1: 0.9344 - val_sensitivity: 0.9344 - val_specificity: 0.9344 - lr: 5.0000e-07\n","Epoch 84/100\n","345/345 [==============================] - ETA: 0s - loss: 1.0445e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 84: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 1.0445e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4979 - val_accuracy: 0.9340 - val_f1: 0.9346 - val_sensitivity: 0.9346 - val_specificity: 0.9346 - lr: 5.0000e-07\n","Epoch 85/100\n","344/345 [============================>.] - ETA: 0s - loss: 3.0859e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 85: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 48ms/step - loss: 3.0863e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.4978 - val_accuracy: 0.9343 - val_f1: 0.9350 - val_sensitivity: 0.9350 - val_specificity: 0.9350 - lr: 5.0000e-07\n","Epoch 86/100\n","344/345 [============================>.] - ETA: 0s - loss: 3.2636e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 86: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 48ms/step - loss: 3.2637e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5018 - val_accuracy: 0.9332 - val_f1: 0.9339 - val_sensitivity: 0.9339 - val_specificity: 0.9339 - lr: 5.0000e-07\n","Epoch 87/100\n","345/345 [==============================] - ETA: 0s - loss: 1.6569e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 87: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 1.6569e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5098 - val_accuracy: 0.9334 - val_f1: 0.9341 - val_sensitivity: 0.9341 - val_specificity: 0.9341 - lr: 5.0000e-07\n","Epoch 88/100\n","344/345 [============================>.] - ETA: 0s - loss: 1.3501e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 88: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 51ms/step - loss: 1.3484e-04 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5055 - val_accuracy: 0.9340 - val_f1: 0.9346 - val_sensitivity: 0.9346 - val_specificity: 0.9346 - lr: 5.0000e-07\n","Epoch 89/100\n","345/345 [==============================] - ETA: 0s - loss: 8.0111e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 89: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 8.0111e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5015 - val_accuracy: 0.9336 - val_f1: 0.9343 - val_sensitivity: 0.9343 - val_specificity: 0.9343 - lr: 5.0000e-07\n","Epoch 90/100\n","345/345 [==============================] - ETA: 0s - loss: 1.8786e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 90: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 1.8786e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5113 - val_accuracy: 0.9338 - val_f1: 0.9344 - val_sensitivity: 0.9344 - val_specificity: 0.9344 - lr: 5.0000e-07\n","Epoch 91/100\n","345/345 [==============================] - ETA: 0s - loss: 1.8441e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 91: val_loss did not improve from 0.19532\n","345/345 [==============================] - 18s 52ms/step - loss: 1.8441e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5128 - val_accuracy: 0.9338 - val_f1: 0.9344 - val_sensitivity: 0.9344 - val_specificity: 0.9344 - lr: 5.0000e-07\n","Epoch 92/100\n","344/345 [============================>.] - ETA: 0s - loss: 1.6944e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 92: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 1.7030e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5154 - val_accuracy: 0.9336 - val_f1: 0.9343 - val_sensitivity: 0.9343 - val_specificity: 0.9343 - lr: 5.0000e-07\n","Epoch 93/100\n","344/345 [============================>.] - ETA: 0s - loss: 1.3970e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 93: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 7.1599e-05 - accuracy: 1.0000 - f1: 0.9999 - sensitivity: 0.9999 - specificity: 0.9999 - val_loss: 0.5159 - val_accuracy: 0.9338 - val_f1: 0.9344 - val_sensitivity: 0.9344 - val_specificity: 0.9344 - lr: 5.0000e-07\n","Epoch 94/100\n","345/345 [==============================] - ETA: 0s - loss: 2.4189e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 94: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 2.4189e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5179 - val_accuracy: 0.9340 - val_f1: 0.9346 - val_sensitivity: 0.9346 - val_specificity: 0.9346 - lr: 5.0000e-07\n","Epoch 95/100\n","345/345 [==============================] - ETA: 0s - loss: 1.6514e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 95: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 1.6514e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5163 - val_accuracy: 0.9345 - val_f1: 0.9352 - val_sensitivity: 0.9352 - val_specificity: 0.9352 - lr: 5.0000e-07\n","Epoch 96/100\n","344/345 [============================>.] - ETA: 0s - loss: 3.2115e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 96: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 3.2076e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5250 - val_accuracy: 0.9338 - val_f1: 0.9344 - val_sensitivity: 0.9344 - val_specificity: 0.9344 - lr: 5.0000e-07\n","Epoch 97/100\n","344/345 [============================>.] - ETA: 0s - loss: 1.2907e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 97: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 49ms/step - loss: 1.2920e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5260 - val_accuracy: 0.9334 - val_f1: 0.9341 - val_sensitivity: 0.9341 - val_specificity: 0.9341 - lr: 5.0000e-07\n","Epoch 98/100\n","344/345 [============================>.] - ETA: 0s - loss: 1.3886e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 98: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 1.3892e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5302 - val_accuracy: 0.9340 - val_f1: 0.9346 - val_sensitivity: 0.9346 - val_specificity: 0.9346 - lr: 5.0000e-07\n","Epoch 99/100\n","345/345 [==============================] - ETA: 0s - loss: 1.4624e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 99: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 1.4624e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5349 - val_accuracy: 0.9338 - val_f1: 0.9344 - val_sensitivity: 0.9344 - val_specificity: 0.9344 - lr: 5.0000e-07\n","Epoch 100/100\n","345/345 [==============================] - ETA: 0s - loss: 1.2143e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000\n","Epoch 100: val_loss did not improve from 0.19532\n","345/345 [==============================] - 17s 50ms/step - loss: 1.2143e-05 - accuracy: 1.0000 - f1: 1.0000 - sensitivity: 1.0000 - specificity: 1.0000 - val_loss: 0.5364 - val_accuracy: 0.9329 - val_f1: 0.9335 - val_sensitivity: 0.9335 - val_specificity: 0.9335 - lr: 5.0000e-07\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"EMIwaVENpaBC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","print('------------Training time is seconds:%s',time.time()-start)\n","scores = model.evaluate(x_test, y_test, verbose=1)\n","\n","#history keys and values\n","print(history.history.keys())\n","print(history.history.values())\n","\n","#Metrics for testing\n","print(scores)\n","print('Test loss:', scores[0])\n","print('Test accuracy:', scores[1])\n","print('Test f1:',scores[2])\n","print('Test sensitivity:',scores[3])\n","print('Test specificity:',scores[4])\n","print(\"Max Test accuracy\", max(history.history['val_accuracy']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1zyGGAAFwJx","executionInfo":{"status":"ok","timestamp":1683352832522,"user_tz":-330,"elapsed":3588,"user":{"displayName":"pythoder 179","userId":"04425462557897485101"}},"outputId":"b3954115-901c-474e-8f0f-d96387295dc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["------------Training time is seconds:%s 2550.7592375278473\n","173/173 [==============================] - 3s 13ms/step - loss: 0.5364 - accuracy: 0.9329 - f1: 0.9332 - sensitivity: 0.9332 - specificity: 0.9332\n","dict_keys(['loss', 'accuracy', 'f1', 'sensitivity', 'specificity', 'val_loss', 'val_accuracy', 'val_f1', 'val_sensitivity', 'val_specificity', 'lr'])\n","dict_values([[0.6673594117164612, 0.4131864905357361, 0.2727060616016388, 0.19491863250732422, 0.12028965353965759, 0.09350138902664185, 0.07799872010946274, 0.033424459397792816, 0.012670572847127914, 0.005522019229829311, 0.0024824868887662888, 0.0010692080250009894, 0.0010071537690237164, 0.0008211518288590014, 0.0010722022270783782, 0.0005889998865313828, 0.0005887591396458447, 0.001066755736246705, 0.0007329543586820364, 0.0007940191426314414, 0.0005825813859701157, 0.0005495977238751948, 0.0004874409642070532, 0.0004937995108775795, 0.0005500852130353451, 0.0005419188528321683, 0.0010679648257791996, 0.0004008512187283486, 0.00039671853301115334, 0.0003528794622980058, 0.00036850705510005355, 0.0004748547216877341, 0.0004385712672956288, 0.00040960541809909046, 0.0005637252470478415, 0.00034534838050603867, 0.0004528907302301377, 0.0003198751073796302, 0.00022528806584887207, 0.00021137842850293964, 0.00018998443556483835, 0.00017897470388561487, 0.0001984174596145749, 0.00022032037668395787, 0.000635088246781379, 0.00026767668896354735, 0.00015716039342805743, 0.00014011426537763327, 9.590763511369005e-05, 0.00031774971284903586, 0.00018126046052202582, 0.00017501898400951177, 0.00014031205500941724, 0.00011579493002500385, 8.555442764190957e-05, 8.589671779191121e-05, 0.00030982898897491395, 9.232397860614583e-05, 7.635363726876676e-05, 0.0001856100861914456, 0.00022608766448684037, 8.149944915203378e-05, 5.735857121180743e-05, 0.0001199084726977162, 8.28609918244183e-05, 5.2005532779730856e-05, 4.759585135616362e-05, 5.717677777283825e-05, 0.0002784343669191003, 4.727336636278778e-05, 3.806337554124184e-05, 4.0437040297547355e-05, 0.00010017116437666118, 4.9637928896117955e-05, 0.0001248195767402649, 0.000148455350426957, 2.6305531719117425e-05, 4.8477642849320546e-05, 1.8937003915198147e-05, 4.868914402322844e-05, 8.223533222917467e-05, 5.019965101382695e-05, 4.507413905230351e-05, 0.00010445105726830661, 3.086342621827498e-05, 3.2637097319820896e-05, 1.6568592400290072e-05, 0.0001348366349702701, 8.011099271243438e-05, 1.8785913198371418e-05, 1.8441382053424604e-05, 1.7029804439516738e-05, 7.159912638599053e-05, 2.418898839096073e-05, 1.6514026356162503e-05, 3.207645931979641e-05, 1.2920359949930571e-05, 1.3891778507968411e-05, 1.462400450691348e-05, 1.2143226740590762e-05], [0.6984033584594727, 0.8216003179550171, 0.8845595717430115, 0.921799898147583, 0.9547309875488281, 0.9648462533950806, 0.9711058735847473, 0.9886600971221924, 0.9956908226013184, 0.998094916343689, 0.999364972114563, 0.9999546408653259, 0.9998185634613037, 0.9999092817306519, 0.9997732043266296, 0.9999092817306519, 1.0, 0.9998185634613037, 0.9997732043266296, 0.9999546408653259, 0.9998639225959778, 0.9999546408653259, 0.9999546408653259, 0.9999546408653259, 0.9998639225959778, 0.9999546408653259, 0.9996824860572815, 1.0, 0.9999546408653259, 1.0, 1.0, 0.9999092817306519, 0.9999092817306519, 0.9999092817306519, 0.9997732043266296, 0.9999092817306519, 0.9999546408653259, 0.9999546408653259, 1.0, 1.0, 1.0, 1.0, 0.9999546408653259, 1.0, 0.9999092817306519, 0.9999546408653259, 1.0, 1.0, 1.0, 0.9999092817306519, 0.9999546408653259, 0.9999546408653259, 1.0, 1.0, 1.0, 1.0, 0.9999092817306519, 1.0, 1.0, 0.9999092817306519, 0.9999546408653259, 1.0, 1.0, 0.9999546408653259, 1.0, 1.0, 1.0, 1.0, 0.9999092817306519, 1.0, 1.0, 1.0, 0.9999546408653259, 1.0, 0.9999546408653259, 0.9999546408653259, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999546408653259, 0.9999546408653259, 1.0, 1.0, 1.0, 0.9999546408653259, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.6985083818435669, 0.8217722177505493, 0.8845832347869873, 0.9217662215232849, 0.954749345779419, 0.9646950364112854, 0.9711503386497498, 0.9886775612831116, 0.9956974387168884, 0.998097836971283, 0.9993659257888794, 0.9999547004699707, 0.9998188614845276, 0.9999094009399414, 0.9997735619544983, 0.9999094009399414, 1.0, 0.9998188614845276, 0.9997735619544983, 0.9999547004699707, 0.9998641014099121, 0.9999033808708191, 0.9999547004699707, 0.9999547004699707, 0.9998641014099121, 0.9999547004699707, 0.9996829628944397, 1.0, 0.9999547004699707, 1.0, 1.0, 0.9999094009399414, 0.9999094009399414, 0.9999094009399414, 0.9997735619544983, 0.9999094009399414, 0.9999547004699707, 0.9999547004699707, 1.0, 1.0, 1.0, 1.0, 0.9999547004699707, 1.0, 0.9999094009399414, 0.9999547004699707, 1.0, 1.0, 1.0, 0.9999094009399414, 0.9999547004699707, 0.9999547004699707, 1.0, 1.0, 1.0, 1.0, 0.9999094009399414, 1.0, 1.0, 0.9999094009399414, 0.9999547004699707, 1.0, 1.0, 0.9999547004699707, 1.0, 1.0, 1.0, 1.0, 0.9999094009399414, 1.0, 1.0, 1.0, 0.9999547004699707, 1.0, 0.9999547004699707, 0.9999547004699707, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999547004699707, 0.9999547004699707, 1.0, 1.0, 1.0, 0.9999033808708191, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.6985084414482117, 0.8217722773551941, 0.8845832943916321, 0.9217662811279297, 0.9547494053840637, 0.9646950364112854, 0.9711503386497498, 0.9886775612831116, 0.9956974387168884, 0.998097836971283, 0.9993659257888794, 0.9999547004699707, 0.9998188614845276, 0.9999094009399414, 0.9997735619544983, 0.9999094009399414, 1.0, 0.9998188614845276, 0.9997735619544983, 0.9999547004699707, 0.9998641014099121, 0.9999033808708191, 0.9999547004699707, 0.9999547004699707, 0.9998641014099121, 0.9999547004699707, 0.9996829628944397, 1.0, 0.9999547004699707, 1.0, 1.0, 0.9999094009399414, 0.9999094009399414, 0.9999094009399414, 0.9997735619544983, 0.9999094009399414, 0.9999547004699707, 0.9999547004699707, 1.0, 1.0, 1.0, 1.0, 0.9999547004699707, 1.0, 0.9999094009399414, 0.9999547004699707, 1.0, 1.0, 1.0, 0.9999094009399414, 0.9999547004699707, 0.9999547004699707, 1.0, 1.0, 1.0, 1.0, 0.9999094009399414, 1.0, 1.0, 0.9999094009399414, 0.9999547004699707, 1.0, 1.0, 0.9999547004699707, 1.0, 1.0, 1.0, 1.0, 0.9999094009399414, 1.0, 1.0, 1.0, 0.9999547004699707, 1.0, 0.9999547004699707, 0.9999547004699707, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999547004699707, 0.9999547004699707, 1.0, 1.0, 1.0, 0.9999033808708191, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.6985084414482117, 0.8217722773551941, 0.8845832943916321, 0.9217662811279297, 0.9547494053840637, 0.9646950364112854, 0.9711503386497498, 0.9886775612831116, 0.9956974387168884, 0.998097836971283, 0.9993659257888794, 0.9999547004699707, 0.9998188614845276, 0.9999094009399414, 0.9997735619544983, 0.9999094009399414, 1.0, 0.9998188614845276, 0.9997735619544983, 0.9999547004699707, 0.9998641014099121, 0.9999033808708191, 0.9999547004699707, 0.9999547004699707, 0.9998641014099121, 0.9999547004699707, 0.9996829628944397, 1.0, 0.9999547004699707, 1.0, 1.0, 0.9999094009399414, 0.9999094009399414, 0.9999094009399414, 0.9997735619544983, 0.9999094009399414, 0.9999547004699707, 0.9999547004699707, 1.0, 1.0, 1.0, 1.0, 0.9999547004699707, 1.0, 0.9999094009399414, 0.9999547004699707, 1.0, 1.0, 1.0, 0.9999094009399414, 0.9999547004699707, 0.9999547004699707, 1.0, 1.0, 1.0, 1.0, 0.9999094009399414, 1.0, 1.0, 0.9999094009399414, 0.9999547004699707, 1.0, 1.0, 0.9999547004699707, 1.0, 1.0, 1.0, 1.0, 0.9999094009399414, 1.0, 1.0, 1.0, 0.9999547004699707, 1.0, 0.9999547004699707, 0.9999547004699707, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999547004699707, 0.9999547004699707, 1.0, 1.0, 1.0, 0.9999033808708191, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.7202032804489136, 0.4540365934371948, 0.5487759709358215, 0.6347259879112244, 0.1953182816505432, 0.23708772659301758, 0.2601661682128906, 0.2591973543167114, 0.3524976968765259, 0.3263750970363617, 0.34170767664909363, 0.3512069284915924, 0.36090588569641113, 0.3665259778499603, 0.36891013383865356, 0.3715497851371765, 0.3709318935871124, 0.36940187215805054, 0.37086206674575806, 0.3718821108341217, 0.37250640988349915, 0.3744049370288849, 0.37586724758148193, 0.3752012252807617, 0.3783789575099945, 0.38293373584747314, 0.3788633346557617, 0.3805217444896698, 0.38353514671325684, 0.38687261939048767, 0.3871889114379883, 0.39047980308532715, 0.3917591869831085, 0.3937808573246002, 0.39312615990638733, 0.39727872610092163, 0.3994266092777252, 0.39978864789009094, 0.4019011855125427, 0.40565788745880127, 0.40729936957359314, 0.4151243567466736, 0.418474406003952, 0.42384034395217896, 0.4253942668437958, 0.42711392045021057, 0.4266226887702942, 0.42928940057754517, 0.43025293946266174, 0.42965179681777954, 0.4332921504974365, 0.43416810035705566, 0.4381628930568695, 0.4392743706703186, 0.4408693015575409, 0.44368159770965576, 0.44542455673217773, 0.4487050473690033, 0.4482709765434265, 0.44934892654418945, 0.4475723206996918, 0.45072141289711, 0.45477595925331116, 0.45410218834877014, 0.46055400371551514, 0.46415087580680847, 0.4660607874393463, 0.4715650975704193, 0.47464826703071594, 0.473113477230072, 0.4779092073440552, 0.48074060678482056, 0.4846975803375244, 0.48947688937187195, 0.49032485485076904, 0.49427661299705505, 0.4955931603908539, 0.49621984362602234, 0.49877071380615234, 0.4982258975505829, 0.49548184871673584, 0.499083548784256, 0.5005088448524475, 0.49789682030677795, 0.49780941009521484, 0.501803457736969, 0.5097506046295166, 0.5055255889892578, 0.5014767646789551, 0.5113089680671692, 0.5128289461135864, 0.515376091003418, 0.515948474407196, 0.5179256200790405, 0.5162977576255798, 0.5250058174133301, 0.5260198712348938, 0.5302097201347351, 0.5349089503288269, 0.5363672375679016], [0.6605587601661682, 0.8058781027793884, 0.798258364200592, 0.7708635926246643, 0.9290638566017151, 0.9125544428825378, 0.9181784987449646, 0.9303337931632996, 0.9299709796905518, 0.9334179759025574, 0.9348693490028381, 0.9372278451919556, 0.9352322220802307, 0.9357764720916748, 0.934325098991394, 0.934325098991394, 0.9345065355300903, 0.9346879720687866, 0.9341436624526978, 0.9348693490028381, 0.9345065355300903, 0.9352322220802307, 0.9341436624526978, 0.9341436624526978, 0.9346879720687866, 0.9341436624526978, 0.9348693490028381, 0.9352322220802307, 0.9355950355529785, 0.935413658618927, 0.935413658618927, 0.9355950355529785, 0.9355950355529785, 0.9359579086303711, 0.9355950355529785, 0.9348693490028381, 0.9346879720687866, 0.935413658618927, 0.935413658618927, 0.9345065355300903, 0.935413658618927, 0.9352322220802307, 0.9350507855415344, 0.9355950355529785, 0.9335994124412537, 0.9341436624526978, 0.9339622855186462, 0.9346879720687866, 0.9341436624526978, 0.934325098991394, 0.9341436624526978, 0.9339622855186462, 0.934325098991394, 0.9346879720687866, 0.9346879720687866, 0.93378084897995, 0.93378084897995, 0.9339622855186462, 0.9352322220802307, 0.9346879720687866, 0.934325098991394, 0.9346879720687866, 0.934325098991394, 0.9345065355300903, 0.9348693490028381, 0.9346879720687866, 0.9348693490028381, 0.934325098991394, 0.9345065355300903, 0.9346879720687866, 0.9345065355300903, 0.9345065355300903, 0.9348693490028381, 0.9350507855415344, 0.9339622855186462, 0.9348693490028381, 0.9348693490028381, 0.934325098991394, 0.9345065355300903, 0.9341436624526978, 0.9334179759025574, 0.9334179759025574, 0.93378084897995, 0.9339622855186462, 0.934325098991394, 0.9332365989685059, 0.9334179759025574, 0.9339622855186462, 0.9335994124412537, 0.93378084897995, 0.93378084897995, 0.9335994124412537, 0.93378084897995, 0.9339622855186462, 0.9345065355300903, 0.93378084897995, 0.9334179759025574, 0.9339622855186462, 0.93378084897995, 0.9328737258911133], [0.6589439511299133, 0.804058849811554, 0.7977728843688965, 0.7693964838981628, 0.9272628426551819, 0.9121766090393066, 0.9152297973632812, 0.929777204990387, 0.9294180274009705, 0.9340875744819641, 0.9355243444442749, 0.9378591179847717, 0.9358835220336914, 0.9364223480224609, 0.9349855184555054, 0.9349855184555054, 0.9351651668548584, 0.9353447556495667, 0.9348059296607971, 0.9355243444442749, 0.9351651668548584, 0.9358835220336914, 0.9348059296607971, 0.9348059296607971, 0.9353447556495667, 0.9348059296607971, 0.9355243444442749, 0.9358835220336914, 0.9362426996231079, 0.9360631108283997, 0.9360631108283997, 0.9362426996231079, 0.9362426996231079, 0.9366019368171692, 0.9362426996231079, 0.9355243444442749, 0.9353447556495667, 0.9360631108283997, 0.9360631108283997, 0.9351651668548584, 0.9360631108283997, 0.9358835220336914, 0.9357039332389832, 0.9362426996231079, 0.9342671632766724, 0.9348059296607971, 0.9346263408660889, 0.9353447556495667, 0.9348059296607971, 0.9349855184555054, 0.9348059296607971, 0.9346263408660889, 0.9349855184555054, 0.9353447556495667, 0.9353447556495667, 0.9344467520713806, 0.9344467520713806, 0.9346263408660889, 0.9358835220336914, 0.9353447556495667, 0.9349855184555054, 0.9353447556495667, 0.9349855184555054, 0.9351651668548584, 0.9355243444442749, 0.9353447556495667, 0.9355243444442749, 0.9349855184555054, 0.9351651668548584, 0.9353447556495667, 0.9351651668548584, 0.9351651668548584, 0.9355243444442749, 0.9357039332389832, 0.9346263408660889, 0.9355243444442749, 0.9355243444442749, 0.9349855184555054, 0.9351651668548584, 0.9348059296607971, 0.9340875744819641, 0.9340875744819641, 0.9344467520713806, 0.9346263408660889, 0.9349855184555054, 0.9339079856872559, 0.9340875744819641, 0.9346263408660889, 0.9342671632766724, 0.9344467520713806, 0.9344467520713806, 0.9342671632766724, 0.9344467520713806, 0.9346263408660889, 0.9351651668548584, 0.9344467520713806, 0.9340875744819641, 0.9346263408660889, 0.9344467520713806, 0.9335487484931946], [0.6589439511299133, 0.8040589094161987, 0.797773003578186, 0.7693965435028076, 0.9272629022598267, 0.9121767282485962, 0.915229856967926, 0.9297773241996765, 0.9294180870056152, 0.9340876340866089, 0.9355244040489197, 0.9378591775894165, 0.935883641242981, 0.9364224076271057, 0.9349856376647949, 0.9349856376647949, 0.9351652264595032, 0.9353448152542114, 0.9348060488700867, 0.9355244040489197, 0.9351652264595032, 0.935883641242981, 0.9348060488700867, 0.9348060488700867, 0.9353448152542114, 0.9348060488700867, 0.9355244040489197, 0.935883641242981, 0.9362428188323975, 0.9360632300376892, 0.9360632300376892, 0.9362428188323975, 0.9362428188323975, 0.936601996421814, 0.9362428188323975, 0.9355244040489197, 0.9353448152542114, 0.9360632300376892, 0.9360632300376892, 0.9351652264595032, 0.9360632300376892, 0.935883641242981, 0.9357040524482727, 0.9362428188323975, 0.9342672228813171, 0.9348060488700867, 0.9346264600753784, 0.9353448152542114, 0.9348060488700867, 0.9349856376647949, 0.9348060488700867, 0.9346264600753784, 0.9349856376647949, 0.9353448152542114, 0.9353448152542114, 0.9344468116760254, 0.9344468116760254, 0.9346264600753784, 0.935883641242981, 0.9353448152542114, 0.9349856376647949, 0.9353448152542114, 0.9349856376647949, 0.9351652264595032, 0.9355244040489197, 0.9353448152542114, 0.9355244040489197, 0.9349856376647949, 0.9351652264595032, 0.9353448152542114, 0.9351652264595032, 0.9351652264595032, 0.9355244040489197, 0.9357040524482727, 0.9346264600753784, 0.9355244040489197, 0.9355244040489197, 0.9349856376647949, 0.9351652264595032, 0.9348060488700867, 0.9340876340866089, 0.9340876340866089, 0.9344468116760254, 0.9346264600753784, 0.9349856376647949, 0.9339080452919006, 0.9340876340866089, 0.9346264600753784, 0.9342672228813171, 0.9344468116760254, 0.9344468116760254, 0.9342672228813171, 0.9344468116760254, 0.9346264600753784, 0.9351652264595032, 0.9344468116760254, 0.9340876340866089, 0.9346264600753784, 0.9344468116760254, 0.9335488677024841], [0.6589439511299133, 0.8040589094161987, 0.797773003578186, 0.7693965435028076, 0.9272629022598267, 0.9121767282485962, 0.915229856967926, 0.9297773241996765, 0.9294180870056152, 0.9340876340866089, 0.9355244040489197, 0.9378591775894165, 0.935883641242981, 0.9364224076271057, 0.9349856376647949, 0.9349856376647949, 0.9351652264595032, 0.9353448152542114, 0.9348060488700867, 0.9355244040489197, 0.9351652264595032, 0.935883641242981, 0.9348060488700867, 0.9348060488700867, 0.9353448152542114, 0.9348060488700867, 0.9355244040489197, 0.935883641242981, 0.9362428188323975, 0.9360632300376892, 0.9360632300376892, 0.9362428188323975, 0.9362428188323975, 0.936601996421814, 0.9362428188323975, 0.9355244040489197, 0.9353448152542114, 0.9360632300376892, 0.9360632300376892, 0.9351652264595032, 0.9360632300376892, 0.935883641242981, 0.9357040524482727, 0.9362428188323975, 0.9342672228813171, 0.9348060488700867, 0.9346264600753784, 0.9353448152542114, 0.9348060488700867, 0.9349856376647949, 0.9348060488700867, 0.9346264600753784, 0.9349856376647949, 0.9353448152542114, 0.9353448152542114, 0.9344468116760254, 0.9344468116760254, 0.9346264600753784, 0.935883641242981, 0.9353448152542114, 0.9349856376647949, 0.9353448152542114, 0.9349856376647949, 0.9351652264595032, 0.9355244040489197, 0.9353448152542114, 0.9355244040489197, 0.9349856376647949, 0.9351652264595032, 0.9353448152542114, 0.9351652264595032, 0.9351652264595032, 0.9355244040489197, 0.9357040524482727, 0.9346264600753784, 0.9355244040489197, 0.9355244040489197, 0.9349856376647949, 0.9351652264595032, 0.9348060488700867, 0.9340876340866089, 0.9340876340866089, 0.9344468116760254, 0.9346264600753784, 0.9349856376647949, 0.9339080452919006, 0.9340876340866089, 0.9346264600753784, 0.9342672228813171, 0.9344468116760254, 0.9344468116760254, 0.9342672228813171, 0.9344468116760254, 0.9346264600753784, 0.9351652264595032, 0.9344468116760254, 0.9340876340866089, 0.9346264600753784, 0.9344468116760254, 0.9335488677024841], [0.001, 0.001, 0.001, 0.001, 0.0003162278, 0.0003162278, 0.0003162278, 0.000100000005, 0.000100000005, 3.1622778e-05, 3.1622778e-05, 1.0000001e-05, 1.0000001e-05, 3.1622778e-06, 3.1622778e-06, 1.0000001e-06, 1.0000001e-06, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07]])\n","[0.5363672971725464, 0.9328737258911133, 0.9331647157669067, 0.9331647157669067, 0.9331647157669067]\n","Test loss: 0.5363672971725464\n","Test accuracy: 0.9328737258911133\n","Test f1: 0.9331647157669067\n","Test sensitivity: 0.9331647157669067\n","Test specificity: 0.9331647157669067\n","Max Test accuracy 0.9372278451919556\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","# summarize history for accuracy\n","\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model Accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.savefig(METRICS_PATH+MODEL_NAME+\"_acc.png\")\n","plt.show()\n"],"metadata":{"id":"reepHdo4LBXt","executionInfo":{"status":"ok","timestamp":1683352838492,"user_tz":-330,"elapsed":710,"user":{"displayName":"pythoder 179","userId":"04425462557897485101"}},"colab":{"base_uri":"https://localhost:8080/","height":472},"outputId":"963c24a7-5ef6-475c-a964-b9c22c1b802e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# summarize history for loss\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.savefig(METRICS_PATH+MODEL_NAME+\"_loss.png\")\n","plt.show()\n"],"metadata":{"id":"2w5C3a5RLBes","executionInfo":{"status":"ok","timestamp":1683352847383,"user_tz":-330,"elapsed":1291,"user":{"displayName":"pythoder 179","userId":"04425462557897485101"}},"colab":{"base_uri":"https://localhost:8080/","height":472},"outputId":"aaf61cdf-e203-4ec6-de70-daea8c486eb2"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABeZUlEQVR4nO3de3xT9eH/8VeS5tJS2lIKLZdCUZCLcpMKVtx0s4qT6XRuotMJTPE3J5vauQle8E7ZpgydTNSJ+N1NvOvE4aWCF4agXFSQiyA3gbaUS+/X5Pz+OE3aQG+0aU8T3s/HsqQnn5N8clrpu5+rzTAMAxEREZEIYbe6AiIiIiKhpHAjIiIiEUXhRkRERCKKwo2IiIhEFIUbERERiSgKNyIiIhJRFG5EREQkoijciIiISERRuBEREZGIonAjIp3ezp07sdlsLFq06LjPXb58OTabjeXLlzdZbtGiRdhsNnbu3NmqOopI56FwIyIiIhFF4UZEREQiisKNiIiIRBSFGxFp1r333ovNZmPr1q1cc801xMfH06NHD+6++24Mw2DPnj386Ec/Ii4ujpSUFB555JFjXiM/P5/rrruO5ORkPB4PI0eO5Lnnnjum3JEjR5gyZQrx8fEkJCQwefJkjhw50mC9Nm/ezE9+8hMSExPxeDykp6fzxhtvhPSz//Wvf+XUU0/F7XbTu3dvbrrppmPq8/XXX3P55ZeTkpKCx+Ohb9++XHnllRQWFgbKvPvuu5x99tkkJCQQGxvL4MGDueOOO0JaVxExRVldAREJH5MmTWLo0KHMmTOHJUuW8OCDD5KYmMiTTz7J97//ff7whz/wz3/+k9tuu40zzjiD7373uwCUl5dz7rnnsm3bNqZPn86AAQN48cUXmTJlCkeOHOHmm28GwDAMfvSjH/Hxxx/zy1/+kqFDh/Lqq68yefLkY+qyceNGxo8fT58+fZgxYwZdunThhRde4NJLL+Xll1/msssua/Pnvffee7nvvvvIzMzkxhtvZMuWLTzxxBN8+umnrFixAqfTSVVVFRMmTKCyspJf//rXpKSksHfvXt58802OHDlCfHw8Gzdu5Ic//CEjRozg/vvvx+12s23bNlasWNHmOopIAwwRkWbcc889BmDccMMNgWM1NTVG3759DZvNZsyZMydw/PDhw0Z0dLQxefLkwLF58+YZgPGPf/wjcKyqqsrIyMgwYmNjjaKiIsMwDOO1114zAOOPf/xj0Pt85zvfMQDj2WefDRw/77zzjOHDhxsVFRWBYz6fzzjrrLOMQYMGBY4tW7bMAIxly5Y1+RmfffZZAzB27NhhGIZh5OfnGy6Xy7jgggsMr9cbKPf4448bgLFw4ULDMAxj3bp1BmC8+OKLjb72n//8ZwMwDhw40GQdRCQ01C0lIi12/fXXBx47HA7S09MxDIPrrrsucDwhIYHBgwfzzTffBI699dZbpKSkcNVVVwWOOZ1OfvOb31BSUsIHH3wQKBcVFcWNN94Y9D6//vWvg+px6NAh3n//fa644gqKi4spKCigoKCAgwcPMmHCBL7++mv27t3bps/63nvvUVVVxS233ILdXvdP5bRp04iLi2PJkiUAxMfHA/D2229TVlbW4GslJCQA8Prrr+Pz+dpULxFpnsKNiLRYv379gr6Oj4/H4/GQlJR0zPHDhw8Hvt61axeDBg0KCgkAQ4cODTzvv+/VqxexsbFB5QYPHhz09bZt2zAMg7vvvpsePXoE3e655x7AHOPTFv46Hf3eLpeLk046KfD8gAEDyMrK4m9/+xtJSUlMmDCB+fPnB423mTRpEuPHj+f6668nOTmZK6+8khdeeEFBR6SdaMyNiLSYw+Fo0TEwx8+0F38ouO2225gwYUKDZQYOHNhu73+0Rx55hClTpvD666/zzjvv8Jvf/Ibs7Gw++eQT+vbtS3R0NB9++CHLli1jyZIlLF26lMWLF/P973+fd955p9FrKCKto5YbEWl3/fv35+uvvz6mpWLz5s2B5/33+/fvp6SkJKjcli1bgr4+6aSTALNrKzMzs8Fb165d21znht67qqqKHTt2BJ73Gz58OHfddRcffvghH330EXv37mXBggWB5+12O+eddx5z587lq6++4qGHHuL9999n2bJlbaqniBxL4UZE2t1FF11Ebm4uixcvDhyrqanhL3/5C7GxsZxzzjmBcjU1NTzxxBOBcl6vl7/85S9Br9ezZ0/OPfdcnnzySfbv33/M+x04cKDNdc7MzMTlcvHYY48FtUI988wzFBYWMnHiRACKioqoqakJOnf48OHY7XYqKysBc4zQ0UaNGgUQKCMioaNuKRFpdzfccANPPvkkU6ZMYc2aNaSlpfHSSy+xYsUK5s2bF2hlufjiixk/fjwzZsxg586dDBs2jFdeeSVo/Irf/PnzOfvssxk+fDjTpk3jpJNOIi8vj5UrV/Ltt9/y+eeft6nOPXr0YObMmdx3331ceOGFXHLJJWzZsoW//vWvnHHGGVxzzTUAvP/++0yfPp2f/vSnnHLKKdTU1PD3v/8dh8PB5ZdfDsD999/Phx9+yMSJE+nfvz/5+fn89a9/pW/fvpx99tltqqeIHEvhRkTaXXR0NMuXL2fGjBk899xzFBUVMXjwYJ599lmmTJkSKGe323njjTe45ZZb+Mc//oHNZuOSSy7hkUceYfTo0UGvOWzYMD777DPuu+8+Fi1axMGDB+nZsyejR49m1qxZIan3vffeS48ePXj88ce59dZbSUxM5IYbbmD27Nk4nU4ARo4cyYQJE/jPf/7D3r17iYmJYeTIkfz3v//lzDPPBOCSSy5h586dLFy4kIKCApKSkjjnnHO47777ArOtRCR0bEZ7jvoTERER6WAacyMiIiIRReFGREREIorCjYiIiEQUhRsRERGJKAo3IiIiElEUbkRERCSinHDr3Ph8Pvbt20fXrl2x2WxWV0dERERawDAMiouL6d279zGb8B7thAs3+/btIzU11epqiIiISCvs2bOHvn37NlnmhAs3/mXe9+zZQ1xcnMW1ERERkZYoKioiNTW1RZvinnDhxt8VFRcXp3AjIiISZloypEQDikVERCSiKNyIiIhIRFG4ERERkYhywo25aSmv10t1dbXV1QhLTqcTh8NhdTVEROQEpXBzFMMwyM3N5ciRI1ZXJawlJCSQkpKitYRERKTDKdwcxR9sevbsSUxMjH45HyfDMCgrKyM/Px+AXr16WVwjERE50Sjc1OP1egPBpnv37lZXJ2xFR0cDkJ+fT8+ePdVFJSIiHUoDiuvxj7GJiYmxuCbhz38NNW5JREQ6msJNA9QV1Xa6hiIiYhWFGxEREYkoCjdyjLS0NObNm2d1NURERFpFA4ojxLnnnsuoUaNCEko+/fRTunTp0vZKiYiIWEDhJlQMH3hrAAOi3FbX5hiGYeD1eomKav5b3qNHjw6okYiISPtQt1SoVJVB/kY4uL3D33rKlCl88MEHPProo9hsNmw2G4sWLcJms/Hf//6XMWPG4Ha7+fjjj9m+fTs/+tGPSE5OJjY2ljPOOIP33nsv6PWO7pay2Wz87W9/47LLLiMmJoZBgwbxxhtvdPCnFBERaRmFm2YYhkFZVU3zt2qfeWtJ2RbeDMNoUR0fffRRMjIymDZtGvv372f//v2kpqYCMGPGDObMmcOmTZsYMWIEJSUlXHTRReTk5LBu3TouvPBCLr74Ynbv3t3ke9x3331cccUVfPHFF1x00UVcffXVHDp0qM3XV0REJNTULdWM8movw2a9fZxn7QnJe391/wRiXM1/i+Lj43G5XMTExJCSkgLA5s2bAbj//vs5//zzA2UTExMZOXJk4OsHHniAV199lTfeeIPp06c3+h5TpkzhqquuAmD27Nk89thjrF69mgsvvLBVn01ERKS9qOUmwqWnpwd9XVJSwm233cbQoUNJSEggNjaWTZs2NdtyM2LEiMDjLl26EBcXF9hiQUREpDNRy00zop0Ovrp/QvMFvdWQ/5X5OGUEhGARu2hn27ctOHrW02233ca7777Lww8/zMCBA4mOjuYnP/kJVVVVTb6O0+kM+tpms+Hz+dpcPxERkVBTuGmGzWZrUdcQPhs4axvCnA6wd2yjmMvlwuv1NltuxYoVTJkyhcsuuwwwW3J27tzZzrUTERHpOJ2iW2r+/PmkpaXh8XgYN24cq1evbrTsueeeG5gRVP82ceLEDqxxA4Jaajq+RSMtLY1Vq1axc+dOCgoKGm1VGTRoEK+88grr16/n888/52c/+5laYEREJKJYHm4WL15MVlYW99xzD2vXrmXkyJFMmDCh0fEcr7zySmBG0P79+9mwYQMOh4Of/vSnHVzzo9jqXUqj48PCbbfdhsPhYNiwYfTo0aPRMTRz586lW7dunHXWWVx88cVMmDCB008/vYNrKyIi0n5sRkvnG7eTcePGccYZZ/D4448D4PP5SE1N5de//jUzZsxo9vx58+Yxa9Ys9u/f36JVdYuKioiPj6ewsJC4uLig5yoqKtixYwcDBgzA4/Ec/4fZ/7kZbHoOhahWnB9B2nwtRURE6mnq9/fRLG25qaqqYs2aNWRmZgaO2e12MjMzWblyZYte45lnnuHKK69sNNhUVlZSVFQUdGs3/tYba/OiiIjICc3ScFNQUIDX6yU5OTnoeHJyMrm5uc2ev3r1ajZs2MD111/faJns7Gzi4+MDN//idu0iEG40hkVERMQqlo+5aYtnnnmG4cOHM3bs2EbLzJw5k8LCwsBtz57QLLDXIIUbERERy1k6FTwpKQmHw0FeXl7Q8by8vMBKu40pLS3l+eef5/7772+ynNvtxu3uoI0s/TOmFG5EREQsY2nLjcvlYsyYMeTk5ASO+Xw+cnJyyMjIaPLcF198kcrKSq655pr2rmbLacyNiIiI5SxfxC8rK4vJkyeTnp7O2LFjmTdvHqWlpUydOhWAa6+9lj59+pCdnR103jPPPMOll15K9+7drah2I9QtJSIiYjXLw82kSZM4cOAAs2bNIjc3l1GjRrF06dLAIOPdu3djP2q13y1btvDxxx/zzjvvWFHlxmnMjYiIiOUsDzcA06dPb3RH6uXLlx9zbPDgwVi8PE/DFG5EREQsF9azpTodDSgWERGxnMJNKGlAsYiIiOUUbkLJwm6pc889l1tuuSVkrzdlyhQuvfTSkL2eiIhIR1G4CSWNuREREbGcwk0oWTTmZsqUKXzwwQc8+uij2Gw2bDYbO3fuZMOGDfzgBz8gNjaW5ORkfv7zn1NQUBA476WXXmL48OFER0fTvXt3MjMzKS0t5d577+W5557j9ddfD7xeQwO7RUREOqNOMVuqUzMMqC5rWdnqCqguB0cpVJW2/b2dMXWBqQmPPvooW7du5bTTTgus2Ox0Ohk7dizXX389f/7znykvL+f222/niiuu4P3332f//v1cddVV/PGPf+Syyy6juLiYjz76CMMwuO2229i0aRNFRUU8++yzACQmJrb984iIiHQAhZvmVJfB7N7WvPcd+8DV8G7n9cXHx+NyuYiJiQlsW/Hggw8yevRoZs+eHSi3cOFCUlNT2bp1KyUlJdTU1PDjH/+Y/v37AzB8+PBA2ejoaCorK5vdBkNERKSzUbiJUJ9//jnLli0jNjb2mOe2b9/OBRdcwHnnncfw4cOZMGECF1xwAT/5yU/o1q2bBbUVEREJHYWb5jhjzBaUZpRW1nCwIJ9+9gPg7AJJA0Pz3q1UUlLCxRdfzB/+8IdjnuvVqxcOh4N3332X//3vf7zzzjv85S9/4c4772TVqlUMGDCgLbUWERGxlMJNc2y2FnUN2anB64wFewk43S06J5RcLhderzfw9emnn87LL79MWloaUVENf5ttNhvjx49n/PjxzJo1i/79+/Pqq6+SlZV1zOuJiIiEC82WChG7zYYP61YoTktLY9WqVezcuZOCggJuuukmDh06xFVXXcWnn37K9u3befvtt5k6dSper5dVq1Yxe/ZsPvvsM3bv3s0rr7zCgQMHGDp0aOD1vvjiC7Zs2UJBQQHV1dUd/plERERaQ+EmRGw2G0Yg3HT8CsW33XYbDoeDYcOG0aNHD6qqqlixYgVer5cLLriA4cOHc8stt5CQkIDdbicuLo4PP/yQiy66iFNOOYW77rqLRx55hB/84AcATJs2jcGDB5Oenk6PHj1YsWJFh38mERGR1rAZnXIHyvZTVFREfHw8hYWFxMXFBT1XUVHBjh07GDBgAB6P57het8br45v9BZxi34thj8KWMrz5kyJYW66liIjI0Zr6/X00tdyEiNXdUiIiImJSuAkRmw0M6m2/cGI1iImIiHQaCjchYrPZAqsJm/+vcCMiImIFhZtQsjnqHqtrSkRExBIKNw1o7Rhrm81W1xt1gndLnWDj1EVEpBNRuKnH6XQCUFbWwo0yj2LToOIA/zX0X1MREZGOohWK63E4HCQkJJCfnw9ATEyMOZamhYyaKsq9EGUzoLwcnCde64VhGJSVlZGfn09CQgIOh6P5k0REREJI4eYo/l2w/QHneBworqTaW0AUXih0QJQr1NULGwkJCdpRXERELKFwcxSbzUavXr3o2bPncW858MRLnzN930Pm5pmXPQV9BrdTLTs3p9OpFhsREbGMwk0jHA7Hcf+CLvc5qC45iMe+B4wyaMnKvD4fHN4BiScFppKLiIhI62lAcQhFOx1UUNsVVV3espNWPwV/OR3WLGq3eomIiJxIFG5CKNrpoNzwh5uKlp209zPz/tD29qmUiIjICUbhJoSiXfVabmpa2HJTuNe899a0T6VEREROMAo3IeR22o+/W6roW/PeW9U+lRIRETnBKNyEkDnmxm1+0ZJw4/NB0X7zscKNiIhISCjchFC000GFUbsib0vCTWk++Gqnm3uPb9q5iIiINEzhJoSiXQ7K/S03LRlz4x9vA3UhR0RERNpE4SaEPEFTwVswW8o/3gbULSUiIhIiCjchZHZL+cNNCzbfrN9yo24pERGRkFC4CaGglpualrTcKNyIiIiEmsJNCB33CsWF6pYSEREJNYWbEIp22eutUNyCcKOWGxERkZBTuAmh4+6Wqt9yo9lSIiIiIWF5uJk/fz5paWl4PB7GjRvH6tWrmyx/5MgRbrrpJnr16oXb7eaUU07hrbfe6qDaNi24W6qZAcXeaijOrfe1uqVERERCIcrKN1+8eDFZWVksWLCAcePGMW/ePCZMmMCWLVvo2bPnMeWrqqo4//zz6dmzJy+99BJ9+vRh165dJCQkdHzlGxC0zk1zU8GL9wNG3dfqlhIREQkJS8PN3LlzmTZtGlOnTgVgwYIFLFmyhIULFzJjxoxjyi9cuJBDhw7xv//9D6fTXAk4LS2tI6vcJE+Ug8raFYqN6jJsTRWuPw0cFG5ERERCxLJuqaqqKtasWUNmZmZdZex2MjMzWblyZYPnvPHGG2RkZHDTTTeRnJzMaaedxuzZs/F6vY2+T2VlJUVFRUG39lK/5cZoruXGP5jYGWPeK9yIiIiEhGXhpqCgAK/XS3JyctDx5ORkcnNzGzznm2++4aWXXsLr9fLWW29x991388gjj/Dggw82+j7Z2dnEx8cHbqmpqSH9HPW5o+rtCt7c9gv+wcTd0sx7jbkREREJCcsHFB8Pn89Hz549eeqppxgzZgyTJk3izjvvZMGCBY2eM3PmTAoLCwO3PXv2tFv9bDYbRHnMx81NBfe33HQbYN4r3IiIiISEZWNukpKScDgc5OXlBR3Py8sjJSWlwXN69eqF0+nE4XAEjg0dOpTc3FyqqqpwuVzHnON2u3G73aGtfFOcMeADm68avDXgaOQS+8fc+FtufDUdUj0REZFIZ1nLjcvlYsyYMeTk5ASO+Xw+cnJyyMjIaPCc8ePHs23bNnw+X+DY1q1b6dWrV4PBxgo2Z3TdF011Tfk3zUxUy42IiEgoWdotlZWVxdNPP81zzz3Hpk2buPHGGyktLQ3Mnrr22muZOXNmoPyNN97IoUOHuPnmm9m6dStLlixh9uzZ3HTTTVZ9hGPYnPVaiZoaVFyobikREZH2YOlU8EmTJnHgwAFmzZpFbm4uo0aNYunSpYFBxrt378Zur8tfqampvP3229x6662MGDGCPn36cPPNN3P77bdb9RGO4XE5qTCceGzVjbfcVFdAWYH52N9yY/jA5wW7o+FzREREpEUsDTcA06dPZ/r06Q0+t3z58mOOZWRk8Mknn7RzrVrPv0qxh+rG95eqPw28S4+6495qhRsREZE2CqvZUuEgeJXiZsJNXB+IqteNpa4pERGRNlO4CTGP00FF7SrFjW6e6R9vE98H7M6645oxJSIi0mYKNyFmdkv5W24a2TzTv4BfXF+w28FW2xWllhsREZE2U7gJMY+z3irFjc2W8k8Dj+9j3jtqyyvciIiItJnCTYhFOx1UGP5w01jLTb0xNwCO2q4p7S8lIiLSZgo3IeZxOSgP7C/VWMtNvTE3oHAjIiISQgo3IeafCg40Plsq0HLT17xXt5SIiEjIKNyEWLPhprIYKgvNx/6WG7tabkREREJF4SbEPPXH3DTULeVvtXHHg7ur+djfLeVTuBEREWkrhZsQC265aWBAcWCmVN+6Y+qWEhERCRmFmxALGlDc0FTwwqMGEwM4anfBULgRERFpM4WbEAuaCt7QxplFR00Dh3otN1qhWEREpK0UbkKs2QHFDbbcqFtKREQkVBRuQizaZW9648yielsv+NnVLSUiIhIqCjch5o5yUEnt7KfjbbnRxpkiIiJtpnATYtEuB+VGbcvN0VPBDaOZMTdquREREWkrhZsQa3LMTfnhuunhQeHGv4ifwo2IiEhbKdyEWP1wYxy9zk3xfvM+pjs4PXXHA+FG3VIiIiJtpXATYma3lD/cHNUtVVQbbrr2Dj6ubikREZGQUbgJMXeUvfmWm64pwcfVLSUiIhIyCjchZrPZ8EXVdjkd3XLTWLjRxpkiIiIhE2V1BSKRLSoaDLAdvUKxP9zENdItpY0zRUQkXFWVwaHtcHAbREXD4Astq4rCTTuwOT1QBXZvJfh8YK9tICvONe/VLSUiIuGuogjWPAvbcuDg9rpFagH6naVwE2kMVwz4c0pNBbhizMdF+8z7rr2CT3CoW0pERMJE2SFYtcC8VRQGP+dJgKRB0HuUFTULULhpBw5ndN0X1eV14SbQcnN0uPHPllK4ERGRTqpwrxloPlsIVSXmse6DYNz/g14joftAiEm0to61FG7agdvlotKIwm2rqdsZ3FsDpfnm40ZbbtQtJSIinYhhwO5PzFCz6T9geM3jycPhu7+FoZeA3WFtHRugcNMOPE4HlbhwU1M3Y6o0Hwwf2BzQpUfwCZotJSIiVqssNltnivebt6K98NUbkPtFXZm070DGdDhlAths1tW1GQo37cBTu0pxHGV12y3UnwZuP2oGvmZLiYiIVQwDls+BD/9U1zJTX5QHRlwBY/8fpJzW8fVrBYWbdhBYpdhG3eaZRY2scQPqlhIREWt4q+HNW2DdP8yvPfHm0An/LflUGPWzTjOWpqUUbtpBtNN+7OaZgZabXseeoNlSIiLS0apK4cUp8PU7YLPDxLmQPtXqWoWEwk07aHBn8CbDjWZLiYhICHmr4cBm2LcO9q2HI7ugWxr0GGLeYpPh1f8H+9aaC+79ZCEMucjqWoeMwk078LjqhRv/bCn/NPC4plpu1C0lIiItdHgn7PgQcr8015upKILKIvPxwW11wyKaEp0IP1sMqWPbvbodSeGmHXiiHFQYx9Fyo9lSIiLSlKpSM7Dkb4KdH5uh5siups9xx5mL6fUeDd0GmGHowGbzNY7sgqRT4Mp/mYvuRRiFm3YQ7XJQjtv8wh9umhxQ7O+WUsuNiIjU2vCyOdC34Gso3HPs8/Yo6JMO/cZBTBJ44sxA44mDhDRIPOnY2bl+1RUQ5e7U07nbQuGmHZhjbmpbY/zNgoGWm97HnuDvltJUcBGRyPPZQvj4zzDgHDjzRnMGUnNWPw1v3RZ8LCbJbGXpM8Z8rf4Z4O7aujo5Pa07L0w0Euk61vz580lLS8Pj8TBu3DhWr17daNlFixZhs9mCbh5P5/omRTvrd0uVma03FUfMr5tsuVG4ERGJGDVV8J9b4M1b4chuWPd3eOIsWPRD2LwEfA2sKQPw6TN1weaM6+EX78Dvd8Dvt8MvlsKEh+CUC1ofbE4AlrfcLF68mKysLBYsWMC4ceOYN28eEyZMYMuWLfTs2bPBc+Li4tiyZUvga1sna1bzuBwcDHRLVdS12kRFm2sIHE0DikVEIkvJAXjhWtj9P8AG42+GwzvMLQx2fmTeEvpD+i9g9M+hS3fzvM+ehSVZ5uOM6XDBgxHbddSeLA83c+fOZdq0aUydas6tX7BgAUuWLGHhwoXMmDGjwXNsNhspKQ20gHQS0bXbLwBmq039mVIN/ZAGwk1Nx1RQRETaz771sPgac5yMOw4u/5u5XQHAkT3w6dOw5jlzUO9798Cy2XDaj80xMsseMsudeZOCTRtYGm6qqqpYs2YNM2fODByz2+1kZmaycuXKRs8rKSmhf//++Hw+Tj/9dGbPns2pp7agD7ODeJx2c4ViMKeCF+0zHzc0Uwo0oFhEpDMwDCg7CAe3mzOTDm2HyhJzwO6Ac6BLUuPn+ryw9W347BnYlgMY5i7ZV/4bepxSVy4hFc6/H86ZYQ4Y/vRp2P85fP7vujLjbjS7nhRsWs3ScFNQUIDX6yU5OTnoeHJyMps3b27wnMGDB7Nw4UJGjBhBYWEhDz/8MGeddRYbN26kb9++x5SvrKyksrIy8HVRUVFoP0QDghfxq6hruWks3NjVLSUi0iaGAflfwTcfmNOkS/LMMSnurrUziOLNKdEnfx9iewSft389rP83bHzV3OT4aKufNO9TRsBJ55otLDa7ebM7zNaYtf8HRd/WnTPsR3DxYxCd0HB9XTFw+s9h9DWwdy18+jfY/CaMmQznP6Bg00aWd0sdr4yMDDIyMgJfn3XWWQwdOpQnn3ySBx544Jjy2dnZ3HfffR1ZxcDGmYA5oLi4iWngUG+2lLqlREQaVFNldvMc3mGOZ6k4Yi5WV37E/Dd21wooPdCy1+o1CgZmgqsLfLHYXPslwAbxfc0A0/1kcLjN8TF5G8zdsevvkH206EQzrKRPNc9vCZsN+o4xbzzRsnOkWZaGm6SkJBwOB3l5eUHH8/LyWjymxul0Mnr0aLZt29bg8zNnziQrKyvwdVFREampqa2vdAuY69z4u6Uqml7AD9QtJSJytPLD8Pli2PIWHNphtooYvqbPccZAvww46RxzgbrKEnPF3soiMxDtrF3Nd/968+YX5YEhE2HkVZB2Njijj33tkvzaVqEPzLoZPvPm85rrxQy9xGytifAp1uHC0nDjcrkYM2YMOTk5XHrppQD4fD5ycnKYPn16i17D6/Xy5ZdfctFFDe+J4Xa7cbvdoapyixwzFbyy2Hzc0NYLoNlSIiJgdhHtXWOuC7Phlbrta/yios39keJ6gSfB7GryxJs7VvdJh77pZtBoSnEebH8ftr1ntvwMvRhOvbThmaz1xfaEET81b9LpWd4tlZWVxeTJk0lPT2fs2LHMmzeP0tLSwOypa6+9lj59+pCdnQ3A/fffz5lnnsnAgQM5cuQIf/rTn9i1axfXX3+9lR8jSP0xN0Z1ObbSAvOJ5lpu/H8F2B0dUEsREQvtW2cOwC3JN7uTSg9A0V5zPRi/nsPg9MnmFgLdBpgBo61jUbomw6irzJtELMvDzaRJkzhw4ACzZs0iNzeXUaNGsXTp0sAg4927d2Ovt3z04cOHmTZtGrm5uXTr1o0xY8bwv//9j2HDhln1EY5Rf8yNr6ocR3MDiv0tN2Au5KdwIyKRas9q+OCPsO3dhp93uOHUy8z1X1LHamCttIrNMAzD6kp0pKKiIuLj4yksLCQuLq5d3sPnM7jyzkd4wf0Avpgk7GW1LTd35jbcl1tdAQ/VzhibscfcF0REJFIYhjng98M/wTfLzWM2h9kl1GMwdOlhTrPu0sPcmiC6m6XVlc7peH5/W95yE4nsdhu+KHNQWSDYeBIaDjYQ3HKjGVMiEikqCuGLF8xVd/M3msfsUebA3e9ktXxGkchxUrhpJ0ZUNNRvE4trYMNMP7vD/CvG8GpQsYiEN2+12fX0xfPw5UvmpAowBwOPugrOvhUS+llbR4l4CjftxRkN9XNKY2vc+DmcUKNwIyJhqCTfnH209W3YvgwqC+ue6zHEHD8zYlLjC9qJhJjCTXs5Jtw0MpjYz+Ey18TRzuAiEi4ObIWPHoYvXwxegyY60dxL6fTJ0O9MDQqWDqdw005sR4+vaTbc+Ne6UbgRkU6gpsrcxmDPJxCbDEmDzIXxuvYyV/T98E/mWjT+/vdeI2HQBBh0AfQ5XbM+xVIKN+3E7ooJPtBct5T2lxIRq1WWwNfvmHscff2uubLv0VyxUFVS9/XgiXDO78x9m0Q6CYWbduJyuakx7ETZaptqmxpQDPW2YFDLjYh0sJoqc+PGD/5g7tnkF5sCA88z928q2GJug+APNsN+BN/9HaQMt6LGIk1SuGkn/oX8YqkwD7RkQDGAT+FGRDqIYcDmJfDu3XDoG/NYtzQzuAy5GPqMgXqLqFJTZZZzdYGE9t2jT6QtFG7aiX/zzLpw04IBxaBuKRFpXzWVkL8J9n9urkGz62PzeJeecN7dMOrqxsfLRLmg55COq6tIKynctBNPlJ1K/87gNrv5D0dTHLXfCoUbEQklwzDXnfnyRdizygw29VuIozyQMR3OvgXcXS2rpkgoKdy0k2iXg3LDDTbMYONo5lIHWm60QrGIhMDhXfDFYvj833VdTn7R3czZTb1HQ/p16mKSiKNw007MncFrx9HENdMlBeqWEpHQKD0IS283W2r8nF1g2CUw+CJzh+34VK09IxFN4aad1N8ZvNnxNmDutwIKNyLSehtfgyW/hbICwAYDvgujfgZDfgjuWKtrJ9JhFG7aSaBbCpqfKQV1LTfaOFNEmlKcBwc2gTvO7F6K7mb+UfTWbfDV62aZnsPgR/PNxfRETkAKN+0k2umoG1DctZk1bkDdUiLSOMOAnR+ba9FsfrPxP4LsUXB2Fnz3Nohyd2wdRToRhZt24nHaWe87mfMc67D3G9f8CZotJSJHK8k3u5o+e8bc8sAv8SRzSnf54bpdt1OGm601vUZaUlWRzkThpp14nA7mey9lU+qVLBzw3eZP0ArFIpHBMGD/etj9ibk4p7OLueidKwaqy6FoHxTtNe9LCyChHySfBimnQfKpUFkMm/4DX70Bu1cS2LvJ2QVGTjJnN6WcVvd+NZXmOTHdNUhYpJbCTTuJdpqLYB32eVp2gsKNSPgyDMj9Aja+at4O7wzda/c+HUZead488cc+H+VWF5TIURRu2km0yww35VXelp2g2VIi4cHng4NfQ94GyPsK8jaawaZob12ZqGg46Ryz5aaqFKrKzPsoN8T3gbg+5n5z0YlwaHvta2yAom8BG/TLgKEXmzetQSNy3BRu2om/5aaiuoXhRi03Ip1X+WHY/r65U/bX79ZOtT5KlAcGXQCnXganTDC7olrzPoYBMYltr7PICUzhpp14asNN+fGGG22cKWIdnw+K95mDdw9sMe/zvoJ9a8Hw1ZVzxpjTrZNPNcfLJA+DXqPavpZMdLe2nS8igMJNuwmEm5Z2S2m2lEjHKthmruJ76BuzS6nwW3OQb2N/YPQYCoMyzdaZ1DPNTSRFpFNSuGkn/jE3FdW+ZkrWUreUSPvzVsPmJebU6h0fNlzGHgXdB0KPwZA02LxPHWvOahKRsKBw0078Y26qvD68PgOHvZkpmgo3IqFTUwXfrjZbYkryoTQfSg6Y42ZKcs0yNrvZCtP/LHOAb3xf875rr+Y3uhWRTk3/BbcTf7gBc1BxF3czl1qzpUTaxr++zPp/wZcvQfmhhst16QmnXwtjpmgmkkiEUrhpJ+4oe+BxeUvCjVpuRI5VdsjcduDQdnNPpZJcKM41ZxW5upjrvrjjwN0V9q6B/K/qzu3S0+xSik2G2J7QpYf59aALzCnaIhKxFG7aid1uw+O0U1Hta9mgYs2WkhOJtxoKvjZnIxm+2hV8Y83ZRlWl8M1yswtp71oCK/S2hMMNQybCqKvhpHPVvSRygtJ/+e3I43RQUe1r2Vo3/r8k1S0lkcRbbc5GOrAFCrbWTa0u2NryIN9jiLlfUtcUiE0x72MSzYXxKgqhssi875oCQy+B6IR2/Ugi0vkp3LSjaKeDI1S3bK2bQLhRy42EOZ/PnGL9v8fMMNPYDtbuODO4OD1QWQJVJea9rXaF3pO/Byd9z1zRV0TkOCjctKO6VYpbMB08MOZGLTcSxr5ZDu/cbW5H4OeKhaRBtdOqT4Gep5qL3sWnaqNHEWkXCjft6LhWKVbLjYQrwzDHxiyfDdveM4+54+DsW2HEJHMPJYUYEelACjft6Lg2z7Qr3EiYyfsKNrxs3g7vMI/ZoyD9Ojjn99Alydr6icgJS+GmHR3X5pnqlpJwseMj+O/vg6ddR0WbO1ifOwO6n2xd3UREULhpVx6nudbNcXVLaSq4dFaGAasWwNt3guE1A/nA8+G0H8MpF7Z900gRkRBRuGlHMS7z8pZWNjJbpD6NuZHOrLoc3rwVPv+3+fXwK+CiP2oXaxHplBRu2lFCjBlYCstbEFjULSWd1eGd8MJkc2sDmwMueADO/JUGCYtIp2Vvvkj7mz9/PmlpaXg8HsaNG8fq1atbdN7zzz+PzWbj0ksvbd8KtlJCtBlujpS1JNz4W25a0Moj0l6Kc2HNc/Df2+G5S+DhU+DRkWawiU6En78KGTcp2IhIp2Z5y83ixYvJyspiwYIFjBs3jnnz5jFhwgS2bNlCz549Gz1v586d3HbbbXznO9/pwNoen/gYszXmSEtabuxaoVgs4q2GrW/Dur/D1++a42mOlnom/Pgp6Na/4+snInKcLA83c+fOZdq0aUydOhWABQsWsGTJEhYuXMiMGTMaPMfr9XL11Vdz33338dFHH3HkyJEOrHHL1bXctCCwqFtKOophwJHd8O2nsGcVbHwVSg/UPd/3DEgdZ64e3HOoudmku6t19RUROU6WhpuqqirWrFnDzJkzA8fsdjuZmZmsXLmy0fPuv/9+evbsyXXXXcdHH33U5HtUVlZSWVkZ+LqoqKjtFW+h+OjjGXPjny2lbilpo5oqyPvSXFiv7BBUl0FNhXlfkm/unl0/zIC5g/aoq2DUNeYqwiIiYczScFNQUIDX6yU5OTnoeHJyMps3b27wnI8//phnnnmG9evXt+g9srOzue+++9pa1VY5vgHF6paSVjIM2PU/2PIWfPuZOT6mpqLpc+xOSBkOfdPN/ZsGnV/3MygiEuYs75Y6HsXFxfz85z/n6aefJimpZaufzpw5k6ysrMDXRUVFpKamtlcVg/jDTcsGFKtbSo5TVZm5QeXqpyBvQ/Bz0d2gTzrE9wVnjLk5pTMa3PHQexSkjDCPiYhEIEvDTVJSEg6Hg7y8vKDjeXl5pKSkHFN++/bt7Ny5k4svvjhwzOczN6WMiopiy5YtnHxy8Oqobrcbt9vdDrVvXny0GViKKqrx+gwc9iZmmPjDjeEDnxfsjg6ooXRaRfvMbqV9a2u7lwrAkwDRCea9zQ5fvQ4VR8zyUdHmYnpp3zHHzHQ/WTOaROSEZWm4cblcjBkzhpycnMB0bp/PR05ODtOnTz+m/JAhQ/jyyy+Djt11110UFxfz6KOPdliLTEv5x9wYBhRXVJNQO3uqQfZ63wpvtcLNicAwIH+TuYP24Z1weJd5f2g7lOQ1d7YpoT+MnQajroaYxPasrYhI2LC8WyorK4vJkyeTnp7O2LFjmTdvHqWlpYHZU9deey19+vQhOzsbj8fDaaedFnR+QkICwDHHOwNXlJ0Yl4OyKi9HypoJN456z3mr1GUQyfI3w8ZXzFlKBVsbLmOzQ4+h0Gc09D4d4lOhsgjKD0P5Eagqhn4ZMOgCBWERkaNYHm4mTZrEgQMHmDVrFrm5uYwaNYqlS5cGBhnv3r0bu71TrDXYKgnRTsqqvM0PKq4/mFNbMIQ3w4DcL81uo6K9tTOVKsz7or3BgcbhMqddd0sLvvUcCq4u1tRfRCTMWR5uAKZPn95gNxTA8uXLmzx30aJFoa9QCMXHuNhXWNH8Qn52h7m0veHV5pnhyt8is+FlOLit8XJ2Jww8D079MQy+EDzxHVdHEZETQKcIN5Hs+Bbyc0KNVzOmrOLzwuY3YcMr0H0gjJkCCU2M4/L5YO9nsHmJOQ07qEXGDadcYM5YckZDlMe8uWOh/3hzYLCIiLQLhZt2dtybZ9ZUqFuqo9VUmrtdr3jMHMzr9/FcOOUHMPZ6GHCuOcg3/ys4sBlyN8C296A0v6683Qknfx9OuxwG/wA8cR39SUREhFaGm+eee46kpCQmTpwIwO9//3ueeuophg0bxr///W/699f+M37xrdo8U+Gm3RmGGVI2vgZrFkFJrnnckwCjrzFnMO34ELYsMW9RnoYXxnPHwcBMGDLRvFeLjIiI5VoVbmbPns0TTzwBwMqVK5k/fz5//vOfefPNN7n11lt55ZVXQlrJcBZ/PC032jyz7QzDXCPm4DbzVl1uLmgXnWDe2+zm5pBfvQ4Hv647r2tvOGs6nD7Z7DoCOLAFPv0brP+3OTvJZofEk6HnEHMmU78zzXVlopqYBSciIh2uVeFmz549DBw4EIDXXnuNyy+/nBtuuIHx48dz7rnnhrJ+YS+hdiG/41ulWC03LVJZbHYP7f/cbGnJ/RIObofq0pad73CZ3Uin/hhOvezYkNJjMFz0JzjvHnOWU0J/TdEXEQkDrQo3sbGxHDx4kH79+vHOO+8EtjfweDyUl5eHtILhrm7MTQsHFENkzJYqP2x26xTuNceeeBLMWUGeeOjWv/EZQoYBxfvN84r3QXGu2RJTkm+uxltRaK7zUnHEPI5x7GvYHOZ06qRB4Io1y/rPqSo191Madqm5RkxLxsW4Y82gIyIiYaFV4eb888/n+uuvZ/To0WzdupWLLroIgI0bN5KWlhbK+oW9hFaNuQnDbqnqCnPm0PZl8M0y2LfO3EqiMbHJkHSKGUCiu8GhHWY30cFvWt7yAhDXx9wnqddI6DUCkgab4UmbQIqInLBaFW7mz5/PXXfdxZ49e3j55Zfp3r07AGvWrOGqq64KaQXDnX9AccTtDF5dDntWwc4VsGuFuRu1tzK4TNJgSB4GlSV1rS5lh8x9kkryzNvOj459bZsD4npD117QNcV8HJtshiBPfO3+SvEQ3w9ie3TEpxURkTDSqnCTkJDA448/fszx++67r80VijT+AcXNLuIH4THmxueFdX+HnAfMkFJflx4w4BxzHMtJ50J8n4Zfo6LIbKUp+NpcG6b8MHQbYLbidB9odimp5UVERFqpVeFm6dKlxMbGcvbZZwNmS87TTz/NsGHDmD9/Pt26dQtpJcOZfz+pwrJqDMPA1tROzfZOPhV85wpYers5cBcgNgUGfMdclC7tbDOYtGQnak8c9Blj3kREREKsVZs2/e53v6OoqAiAL7/8kt/+9rdcdNFF7NixIzC4WEz+MTdVXh/l1d6mC3fWbqniPHhhMiy6yAw2nniYkA23boDL/wbpU81Wl5YEGxERkXbWqpabHTt2MGzYMABefvllfvjDHzJ79mzWrl0bGFwsphiXA6fDRrXX4EhZNTGuJi55Z+yWKtoHi35ortxrs8OYqfC9O6FLd6trJiIi0qBWtdy4XC7KysoAeO+997jgggsASExMDLToiMlms7V8UHFnmwpeP9jE94P/9xH8cK6CjYiIdGqtark5++yzycrKYvz48axevZrFixcDsHXrVvr27RvSCkaC+GgnBSVVzU8H70zdUoV74bkfwqFvIKEfTH7TnGItIiLSybWq5ebxxx8nKiqKl156iSeeeII+fcxZMf/973+58MILQ1rBSBAYVNzcQn6h7pbK3QCrnzZ3rz4ehd/Coom1waY/TFmiYCMiImGjVS03/fr148033zzm+J///Oc2VygStXghv1CHm7dug90rwRltbgbZEkd2w3MXw+Gd5pTsyW9CQmpo6iMiItIBWhVuALxeL6+99hqbNm0C4NRTT+WSSy7B4XCErHKRIrAzeHNjbuy1345QdUsd2mHer/9Xy8LNoR1msCncYwabKUsgXt2MIiISXloVbrZt28ZFF13E3r17GTzY3HMnOzub1NRUlixZwsknnxzSSoa7Fu8MHsqWG28NlOabj3etMINL4oDGyxdsM4NN8T5zvZrJ/zFXBhYREQkzrRpz85vf/IaTTz6ZPXv2sHbtWtauXcvu3bsZMGAAv/nNb0Jdx7DX4p3B/eEmFLOlSvKC93b6/PnGy+ZvNtewKd4HPYbAlLcUbEREJGy1quXmgw8+4JNPPiExMTFwrHv37syZM4fx48eHrHKRosU7gztC2C1VvD/468//BefcDvaj8mzeV2aLTVkBJJ8G174OXZLa/v4iIiIWaVXLjdvtpri4+JjjJSUluFyuNlcq0vjDTYcOKPaHm+TTwB1nDhTetSK4TE0lvDjZDDa9RppdUQo2IiIS5loVbn74wx9yww03sGrVKgzDwDAMPvnkE375y19yySWXhLqOYS+uxYv4+cNNCFpuimrDTeIAOPUy8/Hn/w4u89Fcc+PKLj3h569BTCIiIiLhrlXh5rHHHuPkk08mIyMDj8eDx+PhrLPOYuDAgcybNy/EVQx/LZ4KHpgtFYqWm33mfddeMOpq8/HG16CyxHycvwk+esR8/IM/KNiIiEjEaNWYm4SEBF5//XW2bdsWmAo+dOhQBg4cGNLKRYq6Rfw6sFvK33LTtRekjoXEk81tFDa9ASOuhP/cbA5cPuXCupYdERGRCNDicNPcbt/Lli0LPJ47d27raxSB/C03JZU1VHt9OB2NNJiFslvK33IT19vcrXvUVfD+g+aaN1WlsGcVuGJh4iPazVtERCJKi8PNunXrWlTOpl+Ux/CPuQGz9SYp1t1wQf9sqVBMBS/ONe+79jLvR1wJ7z8EOz+CvWvNY+fN0iJ9IiIScVocbuq3zMjxcdhtdPVEUVxR00y4aYduKf96NQmpcNI58M1yqC6FPulwxvVtfx8REZFOplUDiuX4tWg6eKi6pSqLoap2qn7XlLrjI39m3tuj4JLHwK6tMkREJPK0em8pOT4J0S72UN70Qn6hmi3lb7VxdQV317rjp/0Y8r6EXqMg+dS2vYeIiEgnpXDTQY6v5aaN4ca/gF9cr6Ne3wkXPNi21xYREenk1C3VQeJbstZNqLqliutNAxcRETnBKNx0kPiWrFIcqtlSRfUW8BMRETnBKNx0kLrNMy3slhIRETkBKNx0kIRoM7gcKWuiyylU3VKBlpvebXsdERGRMKRw00Hi/QOKm2y5qV3sr80tN7UL+KnlRkRETkAKNx0koSVjbuyhCjf+AcVquRERkRNPpwg38+fPJy0tDY/Hw7hx41i9enWjZV955RXS09NJSEigS5cujBo1ir///e8dWNvWCQwobu/ZUj5vva0XUpouKyIiEoEsDzeLFy8mKyuLe+65h7Vr1zJy5EgmTJhAfn5+g+UTExO58847WblyJV988QVTp05l6tSpvP322x1c8+Pj3xm83bulSg+A4QWbHWKTW/86IiIiYcrycDN37lymTZvG1KlTGTZsGAsWLCAmJoaFCxc2WP7cc8/lsssuY+jQoZx88sncfPPNjBgxgo8//riDa3586hbxq8LnMxou5A83bZkK7u+S6tKzbmq5iIjICcTScFNVVcWaNWvIzMwMHLPb7WRmZrJy5cpmzzcMg5ycHLZs2cJ3v/vdBstUVlZSVFQUdLOCv1vKZ0BJVU3DhULRLVWkaeAiInJiszTcFBQU4PV6SU4O7j5JTk4mNze30fMKCwuJjY3F5XIxceJE/vKXv3D++ec3WDY7O5v4+PjALTU1NaSfoaU8Tgcep3m5Gx134w83hs8cO9MaxZoGLiIiJzbLu6Vao2vXrqxfv55PP/2Uhx56iKysLJYvX95g2ZkzZ1JYWBi47dmzp2MrW0+zqxTb63UjtXbcjb/lRoOJRUTkBGXpoIykpCQcDgd5eXlBx/Py8khJafyXs91uZ+DAgQCMGjWKTZs2kZ2dzbnnnntMWbfbjdvtDmm9Wysh2kVeUWXj+0v5W27A7Jpyeo7/TbQ6sYiInOAsbblxuVyMGTOGnJycwDGfz0dOTg4ZGRktfh2fz0dlZWV7VDGk6hbya2RMjX9AMbS+5UZr3IiIyAnO8uk0WVlZTJ48mfT0dMaOHcu8efMoLS1l6tSpAFx77bX06dOH7OxswBxDk56ezsknn0xlZSVvvfUWf//733niiSes/BgtktDczuB2hzmF2/C1fsaUBhSLiMgJzvJwM2nSJA4cOMCsWbPIzc1l1KhRLF26NDDIePfu3djtdQ1MpaWl/OpXv+Lbb78lOjqaIUOG8I9//INJkyZZ9RFarGU7g7ugpqL1M6Y0oFhERE5wlocbgOnTpzN9+vQGnzt6oPCDDz7Igw8+2AG1Cr0W7wxeU9G6bqmqMqgoNB9rQLGIiJygwnK2VLgKrFLc1M7g/hlTrWm58Y+3ccaAJ/74zxcREYkACjcdKL65MTdQbyG/VrTcBAYT9wKb7fjPFxERiQAKNx0osAVDc91S0LpwExhMrPE2IiJy4lK46UD+lpuiJsNNW7ql/IOJNd5GREROXAo3HSgh2j/mpgUtN62ZCl5Ur1tKRETkBKVw04H83VKHy6owjGZ2Bm/LgGJ1S4mIyAlM4aYD9Ywzt4GorPFxqLSxVYpDNKBYRETkBKVw04HcUQ6SawPOt4fLGy5k97fcaECxiIhIayjcdLC+3WKAJsJNa7ulfL56LTcaUCwiIicuhZsO1rdbNADfHi5ruEBru6XKD9UNQo5VuBERkRNXp9h+4URSF26aablpbrbU3rWw8yPo0tNsqaksMo936QFRrhDVVkREJPwo3HSw1EC3VGMtNy3olqouh39cbrbWHE2DiUVE5ASncNPBmh9z04JuqS9fMoNNTBKkDIfiXHO8TWURDL04xDUWEREJLwo3Hax+t5RhGNiO3gPK3kzLjWHA6ifNx+N/A+NvrnvO5wW7I8Q1FhERCS8aUNzBeiV4sNmgvNrLwYbWunE0MxV8zyrI/RKiPDD658HPKdiIiIgo3HQ0d5SD5K4eoJGuqea6pVY/Zd4P/ynEJLZDDUVERMKbwo0FmpwO3tSA4qL98NXr5uOxN7RT7URERMKbwo0FmpwO3tRU8DWLwFcD/TKg14j2q6CIiEgYU7ixQN+mpoM31i1VUwVrnjUfj53WjrUTEREJbwo3FkhNbKLlprHZUpvegJI8c/XhIZruLSIi0hiFGws0udZNY7Ol/AOJ06dqBWIREZEmKNxYoP6AYsMwgp9sqFtq33pzCrjdCWOmdkwlRUREwpTCjQV6xUdjs0FFtY+CkqO6nxqaLbXpDfN+yETomtwxlRQREQlTCjcWcEXZSYnzr3Vz1KDihmZL7f7EvB94XgfUTkREJLwp3Fik0engR3dL1VTB3jXm434ZHVQ7ERGR8KVwY5FGBxUHwk1tt9T+z6GmAmK6Q/eBHVhDERGR8KRwY5HUxlYpttfuZepvudm9svaEM+HoTTZFRETkGAo3Fmm+5cYfbmrH2/Q7s4NqJiIiEt4UbizS6P5S9bulDAP2+MONxtuIiIi0hMKNReq33AStdeOo1y11cBuUHYQoD/QaaUEtRUREwo/CjUVS4j3YbVBZ4+NASWXdE/6WG1913XibPulalVhERKSFFG4sErzWTb1xN/W7pTTeRkRE5Lgp3FiowUHF9np7S/lbbjTeRkREpMUUbizU4KBi/wrFpQfg0DeADVLP6PjKiYiIhCmFGwv1TWyg5cbfLVVTYd4nnwqe+A6umYiISPjqFOFm/vz5pKWl4fF4GDduHKtXr2607NNPP813vvMdunXrRrdu3cjMzGyyfGfW4BYM/pYbP423EREROS6Wh5vFixeTlZXFPffcw9q1axk5ciQTJkwgPz+/wfLLly/nqquuYtmyZaxcuZLU1FQuuOAC9u7d28E1b7smu6X8NN5GRETkuFgebubOncu0adOYOnUqw4YNY8GCBcTExLBw4cIGy//zn//kV7/6FaNGjWLIkCH87W9/w+fzkZOT08E1b7vUegOKfb7atW4cR035VsuNiIjIcbE03FRVVbFmzRoyMzMDx+x2O5mZmaxcubJFr1FWVkZ1dTWJiYntVc1241/rpqrGR4F/rRt7vZab+FSI72tN5URERMKUpeGmoKAAr9dLcnJy0PHk5GRyc3Nb9Bq33347vXv3DgpI9VVWVlJUVBR06yycDju94s2uqT3+cTf1u6VSx1lQKxERkfBmebdUW8yZM4fnn3+eV199FY/H02CZ7Oxs4uPjA7fU1NQOrmXT+hw97qZ+t5S6pERERI6bpeEmKSkJh8NBXl5e0PG8vDxSUlKaPPfhhx9mzpw5vPPOO4wYMaLRcjNnzqSwsDBw27NnT0jqHirHzJiq33KjwcQiIiLHzdJw43K5GDNmTNBgYP/g4IyMxn+x//GPf+SBBx5g6dKlpKenN/kebrebuLi4oFtnUjeouLblxu6AM2+C0ddAz2EW1kxERCQ8RVldgaysLCZPnkx6ejpjx45l3rx5lJaWMnXqVACuvfZa+vTpQ3Z2NgB/+MMfmDVrFv/6179IS0sLjM2JjY0lNjbWss/RWqm1C/ntPlRvOviFsy2qjYiISPizPNxMmjSJAwcOMGvWLHJzcxk1ahRLly4NDDLevXs3dntdA9MTTzxBVVUVP/nJT4Je55577uHee+/tyKqHRL/acLPnUHkzJUVERKQlbIZhGFZXoiMVFRURHx9PYWFhp+ii2l9YTkb2+zjsNrY8cCFRjrAe4y0iItIujuf3t36TWiy5qweXw47XZ7C/sMLq6oiIiIQ9hRuL2e22wIypPfXH3YiIiEirKNx0Av5BxXsOK9yIiIi0lcJNJ5CaaLbc7FbLjYiISJsp3HQCmjElIiISOgo3nYB/IT+13IiIiLSdwk0n4B9z863G3IiIiLSZwk0n4A83BSVVlFbWWFwbERGR8KZw0wnERzuJjzY3zAxsoCkiIiKtonDTSWjGlIiISGgo3HQSdTOmFG5ERETaQuGmk9CMKRERkdBQuOkkNGNKREQkNBRuOgl/uFHLjYiISNso3HQS9VcpNgzD4tqIiIiEL4WbTqJ3ggebDcqrvRwsrbK6OiIiImFL4aaTcEc56BXnAdQ1JSIi0hYKN51IX00HFxERaTOFm05Ea92IiIi0ncJNJ+Jf62bPIW3BICIi0loKN51Iv+7agkFERKStFG46kUDLjRbyExERaTWFm07EP+Zm35Fyqr0+i2sjIiISnhRuOpEeXd24o+z4DNh/pMLq6oiIiIQlhZtOxGazaRsGERGRNlK46WRSu5mDijXuRkREpHUUbjqZfmq5ERERaROFm04mVQv5iYiItInCTSfTt5vCjYiISFso3HQygS0YDmuVYhERkdZQuOlkUhPNAcWHSqs4XFplcW1ERETCj8JNJ9PV42RISlcA3tqw3+LaiIiIhB+Fm07ox6f3AeDVtXstromIiEj4UbjphH40qg92G3y26zC7DpZaXR0REZGwonDTCSXHeRg/MAmAV9ep9UZEROR4KNx0UoGuqXV7MQzD4tqIiIiED8vDzfz580lLS8Pj8TBu3DhWr17daNmNGzdy+eWXk5aWhs1mY968eR1X0Q424dQUYlwOdh0sY+3uI1ZXR0REJGxYGm4WL15MVlYW99xzD2vXrmXkyJFMmDCB/Pz8BsuXlZVx0kknMWfOHFJSUjq4th0rxhXFhaean/HVdd9aXBsREZHwYWm4mTt3LtOmTWPq1KkMGzaMBQsWEBMTw8KFCxssf8YZZ/CnP/2JK6+8Erfb3cG17XiX1XZN/efz/VTWeC2ujYiISHiwLNxUVVWxZs0aMjMz6ypjt5OZmcnKlStD9j6VlZUUFRUF3cLFWScnkRznprC8mmWbD1hdHRERkbBgWbgpKCjA6/WSnJwcdDw5OZnc3NyQvU92djbx8fGBW2pqasheu7057DYuHeUfWKyuKRERkZawfEBxe5s5cyaFhYWB2549e6yu0nHxd029vzlf2zGIiIi0gGXhJikpCYfDQV5eXtDxvLy8kA4WdrvdxMXFBd3CyZCUOIb1iqPaa/DmF/usro6IiEinZ1m4cblcjBkzhpycnMAxn89HTk4OGRkZVlWrU7p8TF8AHl+2jaKKaotrIyIi0rlZ2i2VlZXF008/zXPPPcemTZu48cYbKS0tZerUqQBce+21zJw5M1C+qqqK9evXs379eqqqqti7dy/r169n27ZtVn2EDnH1uH6kdY8hr6iSh9/eYnV1REREOjVLw82kSZN4+OGHmTVrFqNGjWL9+vUsXbo0MMh49+7d7N9ftzP2vn37GD16NKNHj2b//v08/PDDjB49muuvv96qj9AhPE4Hsy8bDsDfP9nF2t2HLa6RiIhI52UzTrC1/YuKioiPj6ewsDDsxt/89oXPeXnttwxJ6cp/fn02TkfEjwcXEREBju/3t347hpE7Jw6lW4yTzbnF/O2jHVZXR0REpFNSuAkjiV1c3DVxGADz3tvKroOlFtdIRESk81G4CTM/Pr0P4wd2p7LGx12vbdCO4SIiIkdRuAkzNpuNhy4djivKzkdfF7Bhb/hsJyEiItIRFG7CUFpSF8af3B2ANbsOWVwbERGRzkXhJkyN7tcNgHV7jlhbERERkU5G4SZMje6XAMC63UcsrYeIiEhno3ATpkamJmCzwe5DZRSUVFpdHRERkU5D4SZMxXmcDOwRC8B6td6IiIgEKNyEsUDX1B5txyAiIuKncBPG/IOK1+46Ym1FREREOhGFmzDmb7n5/NsjeH1azE9ERAQUbsLaoJ5diXVHUVblZWtesdXVERER6RQUbsKYw25jZGo8oCnhIiIifgo3YW50au1ifrs1qFhERAQUbsJe3YypI5bWQ0REpLNQuAlzo1ITANiWX0JhebW1lREREekEFG7CXPdYN/27xwDwuVpvREREFG4iweja1hsNKhYREVG4iQh1O4RrULGIiIjCTQSov0O4YWgxPxERObEp3ESAISlxuKPsFJZX801BqdXVERERsZTCTQRwRdkZ3keL+YmIiIDCTcTwd02t1WJ+IiJyglO4iRDpaYkAvLMxj4pqr8W1ERERsY7CTYT43uCe9I73UFBSyUtrvrW6OiIiIpZRuIkQrig70757EgBPffgNNV6fxTUSERGxhsJNBJl0RirdYpzsPlTGki/3W10dERERSyjcRJAYVxRTxw8A4Inl27XmjYiInJAUbiLMtRn9iXE52JxbzPKtB6yujoiISIdTuIkwCTEufja2HwBPLNtucW1EREQ6nsJNBLr+OyfhdNhYvfMQn+08ZHV1REREOpTCTQRKiffw49F9AXPsjYiIyIlE4SZC/b9zTsJmg5zN+bz42R6rqyMiItJhFG4i1Ek9YrliTCoAv3vpC25dvJ6SyhqLayUiItL+FG4i2OwfD+e2C07BYbfx6rq9/PCxj9iwt9DqaomIiLSrThFu5s+fT1paGh6Ph3HjxrF69eomy7/44osMGTIEj8fD8OHDeeuttzqopuHFYbcx/fuDWHzDmfSO97DzYBmX/XUFs9/axLrdh/H5tA6OiIhEHsvDzeLFi8nKyuKee+5h7dq1jBw5kgkTJpCfn99g+f/9739cddVVXHfddaxbt45LL72USy+9lA0bNnRwzcNHeloib938HSacmky11+CpD7/hsr/+jzOzc7jj1S/J2ZTHnkNleBV2REQkAtgMi5exHTduHGeccQaPP/44AD6fj9TUVH79618zY8aMY8pPmjSJ0tJS3nzzzcCxM888k1GjRrFgwYJm36+oqIj4+HgKCwuJi4sL3QcJA4Zh8PbGPN78Yh/Ltxw4ZgyOK8pOv8QY0rp3IbGLE7vNhs1mw24Du82Gw27D6bAR5bDjtNswgCqvj6oa81bjNYh2OYhxOejijiLWHYU7yo7NVr8OUF7tpbSyhpJKL2VVNVTV+HBH2fE4HbidDtxRduw2GwYGhmHW2zAw39dhw+mw43TYcdRGcxs2av+H3WbDbjfvAWz137z2GgD4al+zoTxnGAYV1V6KK2soqaihtLKGKq9BfLST+GgnCTHmvctx7N8GNlvtDVvw5673wOsz8BoGPp+BzwCj9ln/Obba16He1wbmeTU+gxqvjxqfQZS97lq4omzYbTZ8hoHPR+D1bbXftyi7DbvdhqPe9XHYze9t/Wvkf+Q/Vv/qHXUpA9/Poz+j/xobQEW1l/IqL2VV5n21z0e004HH6SDa6cDtNOvvr4/DZgtcQ/81aey6VnvNn7lqr48qrw8Ms7Wy/s3GsZUOvPZRr+f/uu5x8Pch+HMbVNUY1Ph85vvXGNhs1H4/bIF76r2e/3r5DMP8GfAZga+N2tc0as+Istux2817h71eHet9b/w/w/7zgr9/dfX2n2Pj2O/h0f/61/2c+aiuMajy+vD6DBx2Gy6HHWeU+dn836eg63rUtTao/fk2jNr/zupq6f+5s9V+jvrl6v67r3st8/tp/txG2c1/U/zP1336Y/+783/GwGsS/PNZd17T1yjoe2T4fx7N/+bsDfz33hj/9968N+vlP9duq7s2dpv/WPDPT2P816v+NQd/HVtev8a05lxXlJ2eXT2tf9MGHM/v76iQvvNxqqqqYs2aNcycOTNwzG63k5mZycqVKxs8Z+XKlWRlZQUdmzBhAq+99lqD5SsrK6msrAx8XVRU1PaKhymbzcaFp6Vw4WkpVNZ4Wbn9IO9+lccn3xxk96Eyqmp8bMsvYVt+idVVFRGRMHZ6vwRe+dV4y97f0nBTUFCA1+slOTk56HhycjKbN29u8Jzc3NwGy+fm5jZYPjs7m/vuuy80FY4g7igH5w7uybmDewJQ4/Wx70gFOw6WsrOgNNCq429d8BoGXp//L2XzMZjp3Gw5MO/Lq7yUVpmtHaWVXiprvMe8t8fpINYdRYwrili3A6fDTmWNj4pqLxU1Xiqqzdeu3xIDUF37V7r/L2XjqL946/9lFvhryzj2r47gv4ga+tseYlxmHWM9ZguUw26jqLyGwvJqCsurKCyvptob/Gev+cdpXV18hhH03v53MltQqG05sdU7t+4vTP+x+g2rUQ47UfVaJbw+//Woa83xv6a99vUNA2pq/1Ks8db91ei/Nl7DOOYv4KP/auaoevjrdnQrT/3r67/3RJkteR6neR/lsFNR7aWy2kd5tZeKai81vrqWDH/d/PWouw61Nay9rkC9VivzuthsBF7H/5oNOfoveeq9ft3zwT9XR/O30rhqW2iialvxqr0+qmt8VNd+b4I+h2FgD3x/6loi6v917m+R8NV+Bp9hBH7O6rfQUPtzbbfbglpz6pcLatWp91mDr4VxTCudvbZF0FX7uaLsNmp89f/b8x1zbev/PNTnb4Gw1Wvpqd8aa2C2WPivS/3PUr/V0jCMoJ+TwPch8P2wHfP9qt8qUr9lxt/Ke/R5TV2jhj6Hr17Lm6+Bf2fqzq+rj9lyGtxK6b8OvtrW1vrfu/otu82pf/38VWnq59h/ffyObmGv/+/B0VrSkuOKsnbUi6XhpiPMnDkzqKWnqKiI1NRUC2vUOUU57PTrHkO/7jGcc0oPq6sjIiLSapaGm6SkJBwOB3l5eUHH8/LySElJafCclJSU4yrvdrtxu92hqbCIiIh0epa2G7lcLsaMGUNOTk7gmM/nIycnh4yMjAbPycjICCoP8O677zZaXkRERE4slndLZWVlMXnyZNLT0xk7dizz5s2jtLSUqVOnAnDttdfSp08fsrOzAbj55ps555xzeOSRR5g4cSLPP/88n332GU899ZSVH0NEREQ6CcvDzaRJkzhw4ACzZs0iNzeXUaNGsXTp0sCg4d27d2O31zUwnXXWWfzrX//irrvu4o477mDQoEG89tprnHbaaVZ9BBEREelELF/npqOdyOvciIiIhKvj+f1t+QrFIiIiIqGkcCMiIiIRReFGREREIorCjYiIiEQUhRsRERGJKAo3IiIiElEUbkRERCSiKNyIiIhIRFG4ERERkYhi+fYLHc2/IHNRUZHFNREREZGW8v/ebsnGCidcuCkuLgYgNTXV4pqIiIjI8SouLiY+Pr7JMifc3lI+n499+/bRtWtXbDZbSF+7qKiI1NRU9uzZo32r2pmudcfRte44utYdR9e644TqWhuGQXFxMb179w7aULshJ1zLjd1up2/fvu36HnFxcfqPpYPoWnccXeuOo2vdcXStO04ornVzLTZ+GlAsIiIiEUXhRkRERCKKwk0Iud1u7rnnHtxut9VViXi61h1H17rj6Fp3HF3rjmPFtT7hBhSLiIhIZFPLjYiIiEQUhRsRERGJKAo3IiIiElEUbkRERCSiKNyEyPz580lLS8Pj8TBu3DhWr15tdZXCXnZ2NmeccQZdu3alZ8+eXHrppWzZsiWoTEVFBTfddBPdu3cnNjaWyy+/nLy8PItqHDnmzJmDzWbjlltuCRzTtQ6dvXv3cs0119C9e3eio6MZPnw4n332WeB5wzCYNWsWvXr1Ijo6mszMTL7++msLaxyevF4vd999NwMGDCA6OpqTTz6ZBx54IGhvIl3r1vvwww+5+OKL6d27Nzabjddeey3o+ZZc20OHDnH11VcTFxdHQkIC1113HSUlJW2vnCFt9vzzzxsul8tYuHChsXHjRmPatGlGQkKCkZeXZ3XVwtqECROMZ5991tiwYYOxfv1646KLLjL69etnlJSUBMr88pe/NFJTU42cnBzjs88+M84880zjrLPOsrDW4W/16tVGWlqaMWLECOPmm28OHNe1Do1Dhw4Z/fv3N6ZMmWKsWrXK+Oabb4y3337b2LZtW6DMnDlzjPj4eOO1114zPv/8c+OSSy4xBgwYYJSXl1tY8/Dz0EMPGd27dzfefPNNY8eOHcaLL75oxMbGGo8++migjK5167311lvGnXfeabzyyisGYLz66qtBz7fk2l544YXGyJEjjU8++cT46KOPjIEDBxpXXXVVm+umcBMCY8eONW666abA116v1+jdu7eRnZ1tYa0iT35+vgEYH3zwgWEYhnHkyBHD6XQaL774YqDMpk2bDMBYuXKlVdUMa8XFxcagQYOMd9991zjnnHMC4UbXOnRuv/124+yzz270eZ/PZ6SkpBh/+tOfAseOHDliuN1u49///ndHVDFiTJw40fjFL34RdOzHP/6xcfXVVxuGoWsdSkeHm5Zc26+++soAjE8//TRQ5r///a9hs9mMvXv3tqk+6pZqo6qqKtasWUNmZmbgmN1uJzMzk5UrV1pYs8hTWFgIQGJiIgBr1qyhuro66NoPGTKEfv366dq30k033cTEiRODrinoWofSG2+8QXp6Oj/96U/p2bMno0eP5umnnw48v2PHDnJzc4OudXx8POPGjdO1Pk5nnXUWOTk5bN26FYDPP/+cjz/+mB/84AeArnV7asm1XblyJQkJCaSnpwfKZGZmYrfbWbVqVZve/4TbODPUCgoK8Hq9JCcnBx1PTk5m8+bNFtUq8vh8Pm655RbGjx/PaaedBkBubi4ul4uEhISgssnJyeTm5lpQy/D2/PPPs3btWj799NNjntO1Dp1vvvmGJ554gqysLO644w4+/fRTfvOb3+ByuZg8eXLgejb0b4qu9fGZMWMGRUVFDBkyBIfDgdfr5aGHHuLqq68G0LVuRy25trm5ufTs2TPo+aioKBITE9t8/RVuJCzcdNNNbNiwgY8//tjqqkSkPXv2cPPNN/Puu+/i8Xisrk5E8/l8pKenM3v2bABGjx7Nhg0bWLBgAZMnT7a4dpHlhRde4J///Cf/+te/OPXUU1m/fj233HILvXv31rWOcOqWaqOkpCQcDscxs0by8vJISUmxqFaRZfr06bz55pssW7aMvn37Bo6npKRQVVXFkSNHgsrr2h+/NWvWkJ+fz+mnn05UVBRRUVF88MEHPPbYY0RFRZGcnKxrHSK9evVi2LBhQceGDh3K7t27AQLXU/+mtN3vfvc7ZsyYwZVXXsnw4cP5+c9/zq233kp2djaga92eWnJtU1JSyM/PD3q+pqaGQ4cOtfn6K9y0kcvlYsyYMeTk5ASO+Xw+cnJyyMjIsLBm4c8wDKZPn86rr77K+++/z4ABA4KeHzNmDE6nM+jab9myhd27d+vaH6fzzjuPL7/8kvXr1wdu6enpXH311YHHutahMX78+GOWNNi6dSv9+/cHYMCAAaSkpARd66KiIlatWqVrfZzKysqw24N/zTkcDnw+H6Br3Z5acm0zMjI4cuQIa9asCZR5//338fl8jBs3rm0VaNNwZDEMw5wK7na7jUWLFhlfffWVccMNNxgJCQlGbm6u1VULazfeeKMRHx9vLF++3Ni/f3/gVlZWFijzy1/+0ujXr5/x/vvvG5999pmRkZFhZGRkWFjryFF/tpRh6FqHyurVq42oqCjjoYceMr7++mvjn//8pxETE2P84x//CJSZM2eOkZCQYLz++uvGF198YfzoRz/S9ORWmDx5stGnT5/AVPBXXnnFSEpKMn7/+98Hyuhat15xcbGxbt06Y926dQZgzJ0711i3bp2xa9cuwzBadm0vvPBCY/To0caqVauMjz/+2Bg0aJCmgncmf/nLX4x+/foZLpfLGDt2rPHJJ59YXaWwBzR4e/bZZwNlysvLjV/96ldGt27djJiYGOOyyy4z9u/fb12lI8jR4UbXOnT+85//GKeddprhdruNIUOGGE899VTQ8z6fz7j77ruN5ORkw+12G+edd56xZcsWi2obvoqKioybb77Z6Nevn+HxeIyTTjrJuPPOO43KyspAGV3r1lu2bFmD/0ZPnjzZMIyWXduDBw8aV111lREbG2vExcUZU6dONYqLi9tcN5th1FuqUURERCTMacyNiIiIRBSFGxEREYkoCjciIiISURRuREREJKIo3IiIiEhEUbgRERGRiKJwIyIiIhFF4UZETnjLly/HZrMds3eWiIQnhRsRERGJKAo3IiIiElEUbkTEcj6fj+zsbAYMGEB0dDQjR47kpZdeAuq6jJYsWcKIESPweDyceeaZbNiwIeg1Xn75ZU499VTcbjdpaWk88sgjQc9XVlZy++23k5qaitvtZuDAgTzzzDNBZdasWUN6ejoxMTGcddZZx+zeLSLhQeFGRCyXnZ3N//3f/7FgwQI2btzIrbfeyjXXXMMHH3wQKPO73/2ORx55hE8//ZQePXpw8cUXU11dDZih5IorruDKK6/kyy+/5N577+Xuu+9m0aJFgfOvvfZa/v3vf/PYY4+xadMmnnzySWJjY4Pqceedd/LII4/w2WefERUVxS9+8YsO+fwiElraOFNELFVZWUliYiLvvfceGRkZgePXX389ZWVl3HDDDXzve9/j+eefZ9KkSQAcOnSIvn37smjRIq644gquvvpqDhw4wDvvvBM4//e//z1Llixh48aNbN26lcGDB/Puu++SmZl5TB2WL1/O9773Pd577z3OO+88AN566y0mTpxIeXk5Ho+nna+CiISSWm5ExFLbtm2jrKyM888/n9jY2MDt//7v/9i+fXugXP3gk5iYyODBg9m0aRMAmzZtYvz48UGvO378eL7++mu8Xi/r16/H4XBwzjnnNFmXESNGBB736tULgPz8/DZ/RhHpWFFWV0BETmwlJSUALFmyhD59+gQ953a7gwJOa0VHR7eonNPpDDy22WyAOR5IRMKLWm5ExFLDhg3D7Xaze/duBg4cGHRLTU0NlPvkk08Cjw8fPszWrVsZOnQoAEOHDmXFihVBr7tixQpOOeUUHA4Hw4cPx+fzBY3hEZHIpZYbEbFU165due2227j11lvx+XycffbZFBYWsmLFCuLi4ujfvz8A999/P927dyc5OZk777yTpKQkLr30UgB++9vfcsYZZ/DAAw8wadIkVq5cyeOPP85f//pXANLS0pg8eTK/+MUveOyxxxg5ciS7du0iPz+fK664wqqPLiLtROFGRCz3wAMP0KNHD7Kzs/nmm29ISEjg9NNP54477gh0C82ZM4ebb76Zr7/+mlGjRvGf//wHl8sFwOmnn84LL7zArFmzeOCBB+jVqxf3338/U6ZMCbzHE088wR133MGvfvUrDh48SL9+/bjjjjus+Lgi0s40W0pEOjX/TKbDhw+TkJBgdXVEJAxozI2IiIhEFIUbERERiSjqlhIREZGIopYbERERiSgKNyIiIhJRFG5EREQkoijciIiISERRuBEREZGIonAjIiIiEUXhRkRERCKKwo2IiIhEFIUbERERiSj/HwMuCXt4VlCAAAAAAElFTkSuQmCC\n"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"WSp3RqLpNUa1"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}